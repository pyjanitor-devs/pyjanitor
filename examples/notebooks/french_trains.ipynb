{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing French train data\n",
    "     \n",
    "## Background\n",
    "The SNCF (National Society of French Railways) is France's national state-owned railway company. Founded in 1938, it operates the country's national rail traffic along with Monaco, including the TGV, France's high-speed rail network. This dataset covers 2015-2018 with many different train stations. The dataset primarily covers aggregate trip times, delay times, cause for delay, etc., for each station there are 27 columns in total. A TGV route map can be seen [here](https://en.wikipedia.org/wiki/TGV#/media/File:France_TGV.png).\n",
    "\n",
    "## The data\n",
    "The source data set is available from the [SNCF](https://ressources.data.sncf.com/explore/dataset/regularite-mensuelle-tgv-aqst/information/). Check out this [visualization](https://twitter.com/noccaea/status/1095735292206739456) of it. This has been used in a [tidy tuesday](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-02-26) previously. The [full data set](https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-26/full_trains.csv) is available but we will work with a [subset](https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-26/small_trains.csv)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import janitor\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# allow plots to appear directly in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call chaining example\n",
    "First, we run all the methods using pyjanitor's preferred call chaining approach. This code updates the column names, removes any empty rows/columns, and drops some unneeded columns in a very readable manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chained_df = (\n",
    "    pd.read_csv(\n",
    "        \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-26/small_trains.csv\"\n",
    "    )  # ingest raw data\n",
    "    .clean_names()  # removes whitespace, punctuation/symbols, capitalization\n",
    "    .remove_empty()  # removes entirely empty rows / columns\n",
    "    .rename_column(\"num_late_at_departure\", \"num_departing_late\")  # renames 1 column\n",
    "    .drop(columns=[\"service\", \"delay_cause\", \"delayed_number\"])  # drops 3 unnecessary columns\n",
    "    # add 2 new columns with a calculation\n",
    "    .join_apply(\n",
    "        lambda df: df.num_departing_late / df.total_num_trips,\n",
    "        'prop_late_departures'\n",
    "    )\n",
    "    .join_apply(\n",
    "        lambda df: df.num_arriving_late / df.total_num_trips, \n",
    "        'prop_late_arrivals'\n",
    "    )\n",
    ")\n",
    "\n",
    "chained_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step by step through the methods\n",
    "Now, we will import the French data again and then use the methods from the call chain one at a time. Our subset of the train data has over 32000 rows and 13 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-02-26/small_trains.csv\"\n",
    ")\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning column names\n",
    "The clean_names method converts the column names to lowercase and replaces all spaces with underscores. For this data set, it actually does not modify any of the names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_columns = df.columns\n",
    "df = df.clean_names()\n",
    "new_columns = df.columns\n",
    "original_columns == new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming columns\n",
    "We rename the \"num_late_at_departure\" column for consistency purposes with the rename_column method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename_column(\"num_late_at_departure\", \"num_departing_late\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping empty columns and rows\n",
    "The remove_empty method looks for empty columns and rows and drops them if found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.remove_empty()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop unneeded columns\n",
    "We identify 3 columns that we decided are unnecessary for the analysis and can quickly drop them with the aptly named drop_columns method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"service\", \"delay_cause\", \"delayed_number\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gives us the top ten departure stations from that column\n",
    "Counter(df['departure_station']).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use seaborn to quickly visualize how quickly departure and arrivals times were late versus the total number of trips for each of the over 30000 routes in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(\n",
    "    df,\n",
    "    x_vars=['num_departing_late', 'num_arriving_late'],\n",
    "    y_vars='total_num_trips',\n",
    "    height=7,\n",
    "    aspect=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add additional statistics as new columns\n",
    "We can add columns containing additional statistics concerning the proportion of time each route is late either departing or arriving by using the add_columns method for each route. \n",
    "\n",
    "Note the difference between how we added the two columns below and the same code in the chained_df file creation at the top of the notebook. In order to operate on the df that was in the process of being created in the call chain, we had to use join_apply with a lambda function instead of the add_columns method. Alternatively, we could have split the chain into two separate chains with the df being created in the first chain and the add_columns method being used in the second chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop = (\n",
    "    df.add_columns(\n",
    "        prop_late_departures=df.num_departing_late / df.total_num_trips,\n",
    "        prop_late_arrivals=df.num_arriving_late / df.total_num_trips\n",
    "    )\n",
    ")\n",
    "\n",
    "df_prop.head(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
