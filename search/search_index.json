{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"pyjanitor","text":"<p><code>pyjanitor</code> is a Python implementation of the R package <code>janitor</code>, and provides a clean API for cleaning data.</p>"},{"location":"#quick-start","title":"Quick start","text":"<ul> <li>Installation: <code>conda install -c conda-forge pyjanitor</code>. Read more installation instructions here.</li> <li>Check out the collection of general functions.</li> </ul>"},{"location":"#why-janitor","title":"Why janitor?","text":"<p>Originally a port of the R package, <code>pyjanitor</code> has evolved from a set of convenient data cleaning routines into an experiment with the <code>method chaining</code> paradigm.</p> <p>Data preprocessing usually consists of a series of steps that involve transforming raw data into an understandable/usable format. These series of steps need to be run in a certain sequence to achieve success. We take a base data file as the starting point, and perform actions on it, such as removing null/empty rows, replacing them with other values, adding/renaming/removing columns of data, filtering rows and others. More formally, these steps along with their relationships and dependencies are commonly referred to as a Directed Acyclic Graph (DAG).</p> <p>The <code>pandas</code> API has been invaluable for the Python data science ecosystem, and implements method chaining of a subset of methods as part of the API. For example, resetting indexes (<code>.reset_index()</code>), dropping null values (<code>.dropna()</code>), and more, are accomplished via the appropriate <code>pd.DataFrame</code> method calls.</p> <p>Inspired by the ease-of-use and expressiveness of the <code>dplyr</code> package of the R statistical language ecosystem, we have evolved <code>pyjanitor</code> into a language for expressing the data processing DAG for <code>pandas</code> users.</p> <p>To accomplish this, actions for which we would need to invoke imperative-style statements, can be replaced with method chains that allow one to read off the logical order of actions taken. Let us see the annotated example below. First off, here is the textual description of a data cleaning pathway:</p> <ol> <li>Create a <code>DataFrame</code>.</li> <li>Delete one column.</li> <li>Drop rows with empty values in two particular columns.</li> <li>Rename another two columns.</li> <li>Add a new column.</li> </ol> <p>Let's import some libraries and begin with some sample data for this example:</p> <pre><code># Libraries\nimport numpy as np\nimport pandas as pd\nimport janitor\n\n# Sample Data curated for this example\ncompany_sales = {\n    'SalesMonth': ['Jan', 'Feb', 'Mar', 'April'],\n    'Company1': [150.0, 200.0, 300.0, 400.0],\n    'Company2': [180.0, 250.0, np.nan, 500.0],\n    'Company3': [400.0, 500.0, 600.0, 675.0]\n}\n</code></pre> <p>In <code>pandas</code> code, most users might type something like this:</p> <pre><code># The Pandas Way\n\n# 1. Create a pandas DataFrame from the company_sales dictionary\ndf = pd.DataFrame.from_dict(company_sales)\n\n# 2. Delete a column from the DataFrame. Say 'Company1'\ndel df['Company1']\n\n# 3. Drop rows that have empty values in columns 'Company2' and 'Company3'\ndf = df.dropna(subset=['Company2', 'Company3'])\n\n# 4. Rename 'Company2' to 'Amazon' and 'Company3' to 'Facebook'\ndf = df.rename(\n    {\n        'Company2': 'Amazon',\n        'Company3': 'Facebook',\n    },\n    axis=1,\n)\n\n# 5. Let's add some data for another company. Say 'Google'\ndf['Google'] = [450.0, 550.0, 800.0]\n\n# Output looks like this:\n# Out[15]:\n#   SalesMonth  Amazon  Facebook  Google\n# 0        Jan   180.0     400.0   450.0\n# 1        Feb   250.0     500.0   550.0\n# 3      April   500.0     675.0   800.0\n</code></pre> <p>Slightly more advanced users might take advantage of the functional API:</p> <pre><code>df = (\n    pd.DataFrame(company_sales)\n    .drop(columns=\"Company1\")\n    .dropna(subset=[\"Company2\", \"Company3\"])\n    .rename(columns={\"Company2\": \"Amazon\", \"Company3\": \"Facebook\"})\n    .assign(Google=[450.0, 550.0, 800.0])\n)\n\n# The output is the same as before, and looks like this:\n# Out[15]:\n#   SalesMonth  Amazon  Facebook  Google\n# 0        Jan   180.0     400.0   450.0\n# 1        Feb   250.0     500.0   550.0\n# 3      April   500.0     675.0   800.0\n</code></pre> <p>With <code>pyjanitor</code>, we enable method chaining with method names that are explicitly named verbs, which describe the action taken.</p> <pre><code>df = (\n    pd.DataFrame.from_dict(company_sales)\n    .remove_columns([\"Company1\"])\n    .dropna(subset=[\"Company2\", \"Company3\"])\n    .rename_column(\"Company2\", \"Amazon\")\n    .rename_column(\"Company3\", \"Facebook\")\n    .add_column(\"Google\", [450.0, 550.0, 800.0])\n)\n\n# Output looks like this:\n# Out[15]:\n#   SalesMonth  Amazon  Facebook  Google\n# 0        Jan   180.0     400.0   450.0\n# 1        Feb   250.0     500.0   550.0\n# 3      April   500.0     675.0   800.0\n</code></pre> <p>As such, <code>pyjanitor</code>'s etymology has a two-fold relationship to \"cleanliness\". Firstly, it's about extending Pandas with convenient data cleaning routines. Secondly, it's about providing a cleaner, method-chaining, verb-based API for common pandas routines.</p>"},{"location":"#installation","title":"Installation","text":"<p><code>pyjanitor</code> is currently installable from PyPI:</p> <pre><code>pip install pyjanitor\n</code></pre> <p><code>pyjanitor</code> also can be installed by the conda package manager:</p> <pre><code>conda install pyjanitor -c conda-forge\n</code></pre> <p><code>pyjanitor</code> can be installed by the pipenv environment manager too. This requires enabling prerelease dependencies:</p> <pre><code>pipenv install --pre pyjanitor\n</code></pre> <p><code>pyjanitor</code> requires Python 3.6+.</p>"},{"location":"#functionality","title":"Functionality","text":"<p>Current functionality includes:</p> <ul> <li>Cleaning columns name (multi-indexes are possible!)</li> <li>Removing empty rows and columns</li> <li>Identifying duplicate entries</li> <li>Encoding columns as categorical</li> <li>Splitting your data into features and targets (for machine learning)</li> <li>Adding, removing, and renaming columns</li> <li>Coalesce multiple columns into a single column</li> <li>Date conversions (from matlab, excel, unix) to Python datetime format</li> <li>Expand a single column that has delimited, categorical values   into dummy-encoded variables</li> <li>Concatenating and deconcatenating columns, based on a delimiter</li> <li>Syntactic sugar for filtering the dataframe based on queries on a column</li> <li>Experimental submodules for finance, biology, chemistry, engineering, and pyspark</li> </ul>"},{"location":"#api","title":"API","text":"<p>The idea behind the API is two-fold:</p> <ul> <li>Copy the R package function names,   but enable Pythonic use with method chaining or <code>pandas</code> piping.</li> <li>Add other utility functions   that make it easy to do data cleaning/preprocessing in <code>pandas</code>.</li> </ul> <p>Continuing with the company_sales dataframe previously used:</p> <pre><code>import pandas as pd\nimport numpy as np\ncompany_sales = {\n    'SalesMonth': ['Jan', 'Feb', 'Mar', 'April'],\n    'Company1': [150.0, 200.0, 300.0, 400.0],\n    'Company2': [180.0, 250.0, np.nan, 500.0],\n    'Company3': [400.0, 500.0, 600.0, 675.0]\n}\n</code></pre> <p>As such, there are three ways to use the API. The first, and most strongly recommended one, is to use <code>pyjanitor</code>'s functions as if they were native to pandas.</p> <pre><code>import janitor  # upon import, functions are registered as part of pandas.\n\n# This cleans the column names as well as removes any duplicate rows\ndf = pd.DataFrame.from_dict(company_sales).clean_names().remove_empty()\n</code></pre> <p>The second is the functional API.</p> <pre><code>from janitor import clean_names, remove_empty\n\ndf = pd.DataFrame.from_dict(company_sales)\ndf = clean_names(df)\ndf = remove_empty(df)\n</code></pre> <p>The final way is to use the <code>pipe()</code> method:</p> <pre><code>from janitor import clean_names, remove_empty\ndf = (\n    pd.DataFrame.from_dict(company_sales)\n    .pipe(clean_names)\n    .pipe(remove_empty)\n)\n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<p>Follow the development guide for a full description of the process of contributing to <code>pyjanitor</code>.</p>"},{"location":"#adding-new-functionality","title":"Adding new functionality","text":"<p>Keeping in mind the etymology of pyjanitor, contributing a new function to pyjanitor is a task that is not difficult at all.</p>"},{"location":"#define-a-function","title":"Define a function","text":"<p>First off, you will need to define the function that expresses the data processing/cleaning routine, such that it accepts a dataframe as the first argument, and returns a modified dataframe:</p> <pre><code>import pandas_flavor as pf\n\n@pf.register_dataframe_method\ndef my_data_cleaning_function(df, arg1, arg2, ...):\n    # Put data processing function here.\n    return df\n</code></pre> <p>We use <code>pandas_flavor</code> to register the function natively on a <code>pandas.DataFrame</code>.</p>"},{"location":"#add-a-test-case","title":"Add a test case","text":"<p>Secondly, we ask that you contribute a test case, to ensure that the function works as intended. Follow the contribution docs for further details.</p>"},{"location":"#feature-requests","title":"Feature requests","text":"<p>If you have a feature request, please post it as an issue on the GitHub repository issue tracker. Even better, put in a PR for it! We are more than happy to guide you through the codebase so that you can put in a contribution to the codebase.</p> <p>Because <code>pyjanitor</code> is currently maintained by volunteers and has no fiscal support, any feature requests will be prioritized according to what maintainers encounter as a need in our day-to-day jobs. Please temper expectations accordingly.</p>"},{"location":"#api-policy","title":"API Policy","text":"<p><code>pyjanitor</code> only extends or aliases the <code>pandas</code> API (and other dataframe APIs), but will never fix or replace them.</p> <p>Undesirable <code>pandas</code> behaviour should be reported upstream in the <code>pandas</code> issue tracker. We explicitly do not fix the <code>pandas</code> API. If at some point the <code>pandas</code> devs decide to take something from <code>pyjanitor</code> and internalize it as part of the official <code>pandas</code> API, then we will deprecate it from <code>pyjanitor</code>, while acknowledging the original contributors' contribution as part of the official deprecation record.</p>"},{"location":"#credits","title":"Credits","text":"<p>Test data for chemistry submodule can be found at Predictive Toxicology.</p>"},{"location":"AUTHORS/","title":"Contributors","text":"<p>Once you have added your contribution to <code>pyjanitor</code>, please add your name using this markdown template:</p> <pre><code>[@githubname](https://github.com/githubname) | [contributions](https://github.com/pyjanitor-devs/pyjanitor/issues?q=is%3Aclosed+mentions%3Agithubname)\n</code></pre> <p>You can copy/paste the template and replace <code>githubname</code> with your username.</p> <p>Contributions that did not leave a commit trace are indicated in bullet points below each user's username.</p>"},{"location":"AUTHORS/#leads","title":"Leads","text":"<ul> <li>@ericmjl | contributions</li> <li>@szuckerman | contributions</li> <li>@zbarry | contributions<ul> <li>Co-led sprint at SciPy 2019.</li> </ul> </li> <li>@hectormz | contributions</li> <li>@jk3587 | contributions<ul> <li>Tagged issues at SciPy 2019.</li> </ul> </li> <li>@sallyhong | contributions<ul> <li>Tagged issues at SciPy 2019.</li> </ul> </li> <li>@zjpoh | contributions<ul> <li>Started pyspark sub-module.</li> </ul> </li> <li>@anzelpwj | contributions</li> <li>@samukweku | contributions</li> <li>@loganthomas | contributions<ul> <li>Helped others with <code>git</code> issues at SciPy 2019.</li> </ul> </li> <li>@nvamsikrishna05 |  contributions</li> </ul>"},{"location":"AUTHORS/#contributors_1","title":"Contributors","text":"<ul> <li>@JoshuaC3 | contributions</li> <li>@cduvallet | contributions</li> <li>@shantanuo | contributions</li> <li>@jcvall | contributions</li> <li>@CWen001 | contributions</li> <li>@bhallaY | contributions</li> <li>@jekwatt | contributions<ul> <li>Helped other sprinters with <code>git</code> issues at SciPy 2019.</li> </ul> </li> <li>@kurtispinkney | contributions</li> <li>@lphk92 | contributions</li> <li>@jonnybazookatone | contributions</li> <li>@SorenFrohlich | contributions</li> <li>@dave-frazzetto | contributions</li> <li>@dsouzadaniel | contributions</li> <li>@Eidhagen | contributions</li> <li>@mdini | contributions</li> <li>@kimt33 | contributions</li> <li>@jack-kessler-88 | user no longer found</li> <li>@NapsterInBlue | contributions</li> <li>@ricky-lim | contributions</li> <li>@catherinedevlin | contributions</li> <li>@StephenSchroed | contributions</li> <li>@Rajat-181 | contributions</li> <li>@dendrondal | contributions</li> <li>@rahosbach | contributions</li> <li>@asearfos | contributions</li> <li>@emnemnemnem | contributions</li> <li>@rebeccawperry | contributions</li> <li>@TomMonks | contributions</li> <li>@benjaminjack | contributions</li> <li>@kulini | contributions</li> <li>@dwgoltra | contributions</li> <li>@shandou | contributions</li> <li>@samwalkow | contributions</li> <li>@portc13 | contributions</li> <li>@DSNortsev | contributions</li> <li>@qtson | contributions</li> <li>@keoghdata | contributions</li> <li>@cjmayers | contributions</li> <li>@gjlynx | contributions</li> <li>@aopisco | contributions</li> <li>@gaworecki5 | contributions</li> <li>@puruckertom | contributions</li> <li>@thomasjpfan | contributions</li> <li>@jiafengkevinchen | contributions</li> <li>@mralbu | contributions</li> <li>@Ram-N | contributions</li> <li>@eyaltrabelsi | contributions</li> <li>@gddcunh | contributions</li> <li>@DollofCuty | contributions</li> <li>@bdice | contributions</li> <li>@evan-anderson | contributions</li> <li>@smu095 | contributions</li> <li>@VPerrollaz | contributions</li> <li>@UGuntupalli | contributions</li> <li>@mphirke | contributions</li> <li>@sauln | contributions</li> <li>@richardqiu | contributions</li> <li>@MinchinWeb | contributions</li> <li>@BaritoneBeard | contributions</li> <li>@Sousa8697 | contributions</li> <li>@MollyCroke | contributions</li> <li>@ericclessantostv | contributions</li> <li>@fireddd |  contributions</li> <li>@Zeroto521 | contributions</li> <li>@thatlittleboy | contributions</li> <li>@robertmitchellv | contributions</li> <li>@Econundrums | contributions</li> <li>@ashenafiyb | contributions</li> <li>@gahjelle | contributions</li> <li>@ethompsy | contributions</li> <li>@apatao | contributions</li> <li>@OdinTech3 | contributions</li> <li>@asmirnov69 | contributions</li> <li>@xujiboy | contributions</li> <li>@joranbeasley | contributions -@kianmeng | contributions</li> <li>@lbeltrame | contributions</li> </ul>"},{"location":"CHANGELOG/","title":"Changelog","text":""},{"location":"CHANGELOG/#unreleased","title":"Unreleased","text":""},{"location":"CHANGELOG/#v0292-2024-09-28","title":"v0.29.2 - 2024-09-28","text":""},{"location":"CHANGELOG/#v0291-2024-09-23","title":"v0.29.1 - 2024-09-23","text":""},{"location":"CHANGELOG/#v0290-2024-09-15","title":"v0.29.0 - 2024-09-15","text":"<ul> <li>[DOC] Un-deprecate <code>join_apply</code> as no alternative currently exists - Issue #1399 @lbeltrame</li> </ul>"},{"location":"CHANGELOG/#v0281-2024-08-09","title":"v0.28.1 - 2024-08-09","text":""},{"location":"CHANGELOG/#v0280-2024-08-03","title":"v0.28.0 - 2024-08-03","text":"<ul> <li>[ENH] Added a <code>cartesian_product</code> function, as well as an <code>expand</code> method for pandas. - Issue #1293 @samukweku</li> <li>[ENH] Improve <code>pivot_longer</code> when <code>sort_by_appearance</code> is True. Added <code>pivot_longer_spec</code> for more control on how the dataframe should be unpivoted. -@samukweku #1361</li> <li>[ENH] Added <code>convert_excel_date</code> and <code>convert_matlab_date</code> methods for polars - Issue #1352</li> <li>[ENH] Added a <code>complete</code> method for polars. - Issue #1352 @samukweku</li> <li>[ENH] Added a <code>pivot_longer</code> method, and a <code>pivot_longer_spec</code> function for polars - Issue #1352 @samukweku</li> <li>[ENH] Added a <code>row_to_names</code> method for polars. Issue #1352 @samukweku</li> <li>[ENH] <code>read_commandline</code> function now supports polars - Issue #1352 @samukweku</li> <li>[ENH] <code>xlsx_cells</code> function now supports polars - Issue #1352 @samukweku</li> <li>[ENH] <code>xlsx_table</code> function now supports polars - Issue #1352 @samukweku</li> <li>[ENH] Added a <code>clean_names</code> method for polars - it can be used to clean the column names, or clean column values . Issue #1343 @samukweku</li> <li>[ENH] Improved performance for non-equi joins when using numba - @samukweku PR #1341</li> <li>[ENH] pandas Index,Series, DataFrame now supported in the <code>complete</code> method. - PR #1369 @samukweku</li> <li>[ENH] Improve performance for <code>first/last</code> in `conditional_join, when the join columns in the right dataframe are sorted. - PR #1382 @samukweku</li> </ul>"},{"location":"CHANGELOG/#v0270-2024-03-21","title":"v0.27.0 - 2024-03-21","text":"<ul> <li>[BUG] Fix logic for groupby in complete. Index support deprecated. Fix deprecation warning for fillna in <code>complete</code> PR #1289 @samukweku</li> <li>[ENH] <code>select</code> function now supports variable arguments - PR #1288 @samukweku</li> <li>[ENH] <code>conditional_join</code> now supports timedelta dtype. - PR #1297 @samukweku</li> <li>[ENH] <code>get_join_indices</code> function added - returns only join indices between two dataframes. Issue #1310 @samukweku</li> <li>[ENH] <code>explode_index</code> function added. - Issue #1283</li> <li>[ENH] <code>conditional_join</code> now supports timedelta dtype. - PR #1297</li> <li>[ENH] <code>change_index_dtype</code> added. - @samukweku Issue #1314</li> <li>[ENH] Add <code>glue</code> and <code>axis</code> parameters to <code>collapse_levels</code>. - Issue #211 @samukweku</li> <li>[ENH] <code>row_to_names</code> now supports multiple rows conversion to columns. - @samukweku Issue #1333</li> <li>[ENH] Fix warnings from Pandas. <code>truncate_datetime</code> now uses a vectorized option. -@samukweku #1337</li> </ul>"},{"location":"CHANGELOG/#v0260-2023-09-18","title":"v0.26.0 - 2023-09-18","text":"<ul> <li>[ENH] <code>clean_names</code> can now be applied to column values. Issue #995 @samukweku</li> <li>[BUG] Fix ImportError - Issue #1285 @samukweku</li> </ul>"},{"location":"CHANGELOG/#v0250-2023-07-27","title":"v0.25.0 - 2023-07-27","text":"<ul> <li>[INF] Replace <code>pytest.ini</code> file with <code>pyproject.toml</code> file. PR #1204 @Zeroto521</li> <li>[INF] Extract docstrings tests from all tests. PR #1205 @Zeroto521</li> <li>[BUG] Address the <code>TypeError</code> when importing v0.24.0 (issue #1201 @xujiboy and @joranbeasley)</li> <li>[INF] Fixed issue with missing PyPI README. PR #1216 @thatlittleboy</li> <li>[INF] Update some <code>mkdocs</code> compatibility code. PR #1231 @thatlittleboy</li> <li>[INF] Migrated docstring style from Sphinx to Google for better compatibility with <code>mkdocstrings</code>. PR #1235 @thatlittleboy</li> <li>[INF] Prevent selection of chevrons (<code>&gt;&gt;&gt;</code>) and outputs in Example code blocks. PR #1237 @thatlittleboy</li> <li>[DEPR] Add deprecation warnings for <code>process_text</code>, <code>rename_column</code>, <code>rename_columns</code>, <code>filter_on</code>, <code>remove_columns</code>, <code>fill_direction</code>. Issue #1045 @samukweku</li> <li>[ENH] <code>pivot_longer</code> now supports named groups where <code>names_pattern</code> is a regular expression. A dictionary can now be passed to <code>names_pattern</code>, and is internally evaluated as a list/tuple of regular expressions. Issue #1209 @samukweku</li> <li>[ENH] Improve selection in <code>conditional_join</code>. Issue #1223 @samukweku</li> <li>[ENH] Add <code>col</code> class for selecting columns within an expression. Currently limited to use within <code>conditional_join</code>. PR #1260 @samukweku.</li> <li>[ENH] Performance improvement for range joins in <code>conditional_join</code>, when <code>use_numba = False</code>. Performance improvement for equi-join and a range join, when <code>use_numba = True</code>, for many to many join with wide ranges. PR #1256, #1267 @samukweku</li> <li>[DEPR] Add deprecation warning for <code>pivot_wider</code>. Issue #1045 @samukweku</li> <li>[BUG] Fix string column selection on a MultiIndex. Issue #1265. @samukweku</li> </ul>"},{"location":"CHANGELOG/#v0240-2022-11-12","title":"v0.24.0 - 2022-11-12","text":"<ul> <li>[ENH] Add lazy imports to speed up the time taken to load pyjanitor (part 2)</li> <li>[DOC] Updated developer guide docs.</li> <li>[ENH] Allow column selection/renaming within conditional_join. Issue #1102. Also allow first or last match. Issue #1020 @samukweku.</li> <li>[ENH] New decorator <code>deprecated_kwargs</code> for breaking API. #1103 @Zeroto521</li> <li>[ENH] Extend select_columns to support non-string columns. Issue #1105 @samukweku</li> <li>[ENH] Performance improvement for groupby_topk. Issue #1093 @samukweku</li> <li>[ENH] <code>min_max_scale</code> drop <code>old_min</code> and <code>old_max</code> to fit sklearn's method API. Issue #1068 @Zeroto521</li> <li>[ENH] Add <code>jointly</code> option for <code>min_max_scale</code> support to transform each column values or entire values. Default transform each column, similar behavior to <code>sklearn.preprocessing.MinMaxScaler</code>. (Issue #1067, PR #1112, PR #1123) @Zeroto521</li> <li>[INF] Require pyspark minimal version is v3.2.0 to cut duplicates codes. Issue #1110 @Zeroto521</li> <li>[ENH] Add support for extension arrays in <code>expand_grid</code>. Issue #1121 @samukweku</li> <li>[ENH] Add <code>names_expand</code> and <code>index_expand</code> parameters to <code>pivot_wider</code> for exposing missing categoricals. Issue #1108 @samukweku</li> <li>[ENH] Add fix for slicing error when selecting columns in <code>pivot_wider</code>. Issue #1134 @samukweku</li> <li>[ENH] <code>dropna</code> parameter added to <code>pivot_longer</code>. Issue #1132 @samukweku</li> <li>[INF] Update <code>mkdocstrings</code> version and to fit its new coming features. PR #1138 @Zeroto521</li> <li>[BUG] Force <code>math.softmax</code> returning <code>Series</code>. PR #1139 @Zeroto521</li> <li>[INF] Set independent environment for building documentation. PR #1141 @Zeroto521</li> <li>[DOC] Add local documentation preview via github action artifact. PR #1149 @Zeroto521</li> <li>[ENH] Enable <code>encode_categorical</code> handle 2 (or more ) dimensions array. PR #1153 @Zeroto521</li> <li>[TST] Fix testcases failing on Window. Issue #1160 @Zeroto521, and @samukweku</li> <li>[INF] Cancel old workflow runs via Github Action <code>concurrency</code>. PR #1161 @Zeroto521</li> <li>[ENH] Faster computation for non-equi join, with a numba engine. Speed improvement for left/right joins when <code>sort_by_appearance</code> is False. Issue #1102 @samukweku</li> <li>[BUG] Avoid <code>change_type</code> mutating original <code>DataFrame</code>. PR #1162 @Zeroto521</li> <li>[ENH] The parameter <code>column_name</code> of <code>change_type</code> totally supports inputing multi-column now. #1163 @Zeroto521</li> <li>[ENH] Fix error when <code>sort_by_appearance=True</code> is combined with <code>dropna=True</code>. Issue #1168 @samukweku</li> <li>[ENH] Add explicit default parameter to <code>case_when</code> function. Issue #1159 @samukweku</li> <li>[BUG] pandas 1.5.x <code>_MergeOperation</code> doesn't have <code>copy</code> keyword anymore. Issue #1174 @Zeroto521</li> <li>[ENH] <code>select_rows</code> function added for flexible row selection. Generic <code>select</code> function added as well. Add support for MultiIndex selection via dictionary. Issue #1124 @samukweku</li> <li>[TST] Compat with macos and window, to fix <code>FailedHealthCheck</code> Issue #1181 @Zeroto521</li> <li>[INF] Merge two docs CIs (<code>docs-preview.yml</code> and <code>docs.yml</code>) to one. And add <code>documentation</code> pytest mark. PR #1183 @Zeroto521</li> <li>[INF] Merge <code>codecov.yml</code> (only works for the dev branch pushing event) into <code>tests.yml</code> (only works for PR event). PR #1185 @Zeroto521</li> <li>[TST] Fix failure for test/timeseries/test_fill_missing_timestamp. Issue #1184 @samukweku</li> <li>[BUG] Import <code>DataDescription</code> to fix: <code>AttributeError: 'DataFrame' object has no attribute 'data_description'</code>. PR #1191 @Zeroto521</li> </ul>"},{"location":"CHANGELOG/#v0231-2022-05-03","title":"v0.23.1 - 2022-05-03","text":"<ul> <li>[DOC] Updated <code>fill.py</code> and <code>update_where.py</code> documentation with working examples.</li> <li>[ENH] Deprecate <code>num_bins</code> from <code>bin_numeric</code> in favour of <code>bins</code>, and allow generic <code>**kwargs</code> to be passed into <code>pd.cut</code>. Issue #969. @thatlittleboy</li> <li>[ENH] Fix <code>concatenate_columns</code> not working on category inputs @zbarry</li> <li>[INF] Simplify CI system @ericmjl</li> <li>[ENH] Added \"read_commandline\" function to janitor.io @BaritoneBeard</li> <li>[BUG] Fix bug with the complement parameter of <code>filter_on</code>. Issue #988. @thatlittleboy</li> <li>[ENH] Add <code>xlsx_table</code>, for reading tables from an Excel sheet. @samukweku</li> <li>[ENH] minor improvements for conditional_join; equality only joins are no longer supported; there has to be at least one non-equi join present. @samukweku</li> <li>[BUG] <code>sort_column_value_order</code> no longer mutates original dataframe.</li> <li>[BUG] Extend <code>fill_empty</code>'s <code>column_names</code> type range. Issue #998. @Zeroto521</li> <li>[BUG] Removed/updated error-inducing default arguments in <code>row_to_names</code> (#1004) and <code>round_to_fraction</code> (#1005). @thatlittleboy</li> <li>[ENH] <code>patterns</code> deprecated in favour of importing <code>re.compile</code>. #1007 @samukweku</li> <li>[ENH] Changes to kwargs in <code>encode_categorical</code>, where the values can either be a string or a 1D array. #1021 @samukweku</li> <li>[ENH] Add <code>fill_value</code> and <code>explicit</code> parameters to the <code>complete</code> function. #1019 @samukweku</li> <li>[ENH] Performance improvement for <code>expand_grid</code>. @samukweku</li> <li>[BUG] Make <code>factorize_columns</code> (PR #1028) and <code>truncate_datetime_dataframe</code> (PR #1040) functions non-mutating. @thatlittleboy</li> <li>[BUG] Fix SettingWithCopyWarning and other minor bugs when using <code>truncate_datetime_dataframe</code>, along with further performance improvements (PR #1040). @thatlittleboy</li> <li>[ENH] Performance improvement for <code>conditional_join</code>. @samukweku</li> <li>[ENH] Multiple <code>.value</code> is now supported in <code>pivot_longer</code>. Multiple values_to is also supported, when names_pattern is a list or tuple. <code>names_transform</code> parameter added, for efficient dtype transformation of unpivoted columns. #1034, #1048, #1051 @samukweku</li> <li>[ENH] Add <code>xlsx_cells</code> for reading a spreadsheet as a table of individual cells. #929 @samukweku.</li> <li>[ENH] Let <code>filter_string</code> suit parameters of <code>Series.str.contains</code> Issue #1003 and #1047. @Zeroto521</li> <li>[ENH] <code>names_glue</code> in <code>pivot_wider</code> now takes a string form, using str.format_map under the hood. <code>levels_order</code> is also deprecated. @samukweku</li> <li>[BUG] Fixed bug in <code>transform_columns</code> which ignored the <code>column_names</code> specification when <code>new_column_names</code> dictionary was provided as an argument, issue #1063. @thatlittleboy</li> <li>[BUG] <code>count_cumulative_unique</code> no longer modifies the column being counted in the output when <code>case_sensitive</code> argument is set to False, issue #1065. @thatlittleboy</li> <li>[BUG] Fix for gcc missing error in dev container</li> <li>[DOC] Added a step in the dev guide to install <code>Remote Container</code> in VS Code. @ashenafiyb</li> <li>[DOC] Convert <code>expand_column</code> and <code>find_replace</code> code examples to doctests, issue #972. @gahjelle</li> <li>[DOC] Convert <code>expand_column</code> code examples to doctests, issue #972. @gahjelle</li> <li>[DOC] Convert <code>get_dupes</code> code examples to doctests, issue #972. @ethompsy</li> <li>[DOC] Convert <code>engineering</code> code examples to doctests, issue #972 @ashenafiyb</li> <li>[DOC] Convert <code>groupby_topk</code> code examples to doctests, issue #972. @ethompsy</li> <li>[DOC] Add doctests to <code>math</code>, issue #972. @gahjelle</li> <li>[DOC] Add doctests to <code>math</code> and <code>ml</code>, issue #972. @gahjelle</li> <li>[DOC] Add doctests to <code>math</code>, <code>ml</code>, and <code>xarray</code>, issue #972. @gahjelle</li> </ul>"},{"location":"CHANGELOG/#v0220-2021-11-21","title":"v0.22.0 - 2021-11-21","text":"<ul> <li>[BUG] Fix conditional join issue for multiple conditions, where pd.eval fails to evaluate if numexpr is installed. #898 @samukweku</li> <li>[ENH] Added <code>case_when</code> to handle multiple conditionals and replacement values. Issue #736. @robertmitchellv</li> <li>[ENH] Deprecate <code>new_column_names</code> and <code>merge_frame</code> from <code>process_text</code>. Only existing columns are supported. @samukweku</li> <li>[ENH] <code>complete</code> uses <code>pd.merge</code> internally, providing a simpler logic, with some speed improvements in certain cases over <code>pd.reindex</code>. @samukweku</li> <li>[ENH] <code>expand_grid</code> returns a MultiIndex DataFrame, allowing the user to decide how to manipulate the columns. @samukweku</li> <li>[INF] Simplify a bit linting, use pre-commit as the CI linting checker. @Zeroto521</li> <li>[ENH] Fix bug in <code>pivot_longer</code> for wrong output when <code>names_pattern</code> is a sequence with a single value. Issue #885 @samukweku</li> <li>[ENH] Deprecate <code>aggfunc</code> from <code>pivot_wider</code>; aggregation can be chained with pandas' <code>groupby</code>.</li> <li>[ENH] <code>As_Categorical</code> deprecated from <code>encode_categorical</code>; a tuple of <code>(categories, order)</code> suffices for **kwargs. @samukweku</li> <li>[ENH] Deprecate <code>names_sort</code> from <code>pivot_wider</code>.@samukweku</li> <li>[ENH] Add <code>softmax</code> to <code>math</code> module. Issue #902. @loganthomas</li> </ul>"},{"location":"CHANGELOG/#v0212-2021-09-01","title":"v0.21.2 - 2021-09-01","text":"<ul> <li>[ENH] Fix warning message in <code>coalesce</code>, from bfill/fill;<code>coalesce</code> now uses variable arguments. Issue #882 @samukweku</li> <li>[INF] Add SciPy as explicit dependency in <code>base.in</code>. Issue #895 @ericmjl</li> </ul>"},{"location":"CHANGELOG/#v0211-2021-08-29","title":"v0.21.1 - 2021-08-29","text":"<ul> <li>[DOC] Fix references and broken links in AUTHORS.rst. @loganthomas</li> <li>[DOC] Updated Broken links in the README and contributing docs. @nvamsikrishna05</li> <li>[INF] Update pre-commit hooks and remove mutable references. Issue #844. @loganthomas</li> <li>[INF] Add GitHub Release pointer to auto-release script. Issue #818. @loganthomas</li> <li>[INF] Updated black version in github actions code-checks to match pre-commit hooks. @nvamsikrishna05</li> <li>[ENH] Add reset_index flag to row_to_names function. @fireddd</li> <li>[ENH] Updated <code>label_encode</code> to use pandas factorize instead of scikit-learn LabelEncoder. @nvamsikrishna05</li> <li>[INF] Removed the scikit-learn package from the dependencies from environment-dev.yml and base.in files. @nvamsikrishna05</li> <li>[ENH] Add function to remove constant columns. @fireddd</li> <li>[ENH] Added <code>factorize_columns</code> method which will deprecate the <code>label_encode</code> method in future release. @nvamsikrishna05</li> <li>[DOC] Delete Read the Docs project and remove all readthedocs.io references from the repo. Issue #863. @loganthomas</li> <li>[DOC] Updated various documentation sources to reflect pyjanitor-dev ownership. @loganthomas</li> <li>[INF] Fix <code>isort</code> automatic checks. Issue #845. @loganthomas</li> <li>[ENH] <code>complete</code> function now uses variable args (*args) - @samukweku</li> <li>[ENH] Set <code>expand_column</code>'s <code>sep</code> default is <code>\"|\"</code>, same to <code>pandas.Series.str.get_dummies</code>. Issue #876. @Zeroto521</li> <li>[ENH] Deprecate <code>limit</code> from fill_direction. fill_direction now uses kwargs. @samukweku</li> <li>[ENH] Added <code>conditional_join</code> function that supports joins on non-equi operators. @samukweku</li> <li>[INF] Speed up pytest via <code>-n</code> (pytest-xdist) option. Issue #881. @Zeroto521</li> <li>[DOC] Add list mark to keep <code>select_columns</code>'s example same style. @Zeroto521</li> <li>[ENH] Updated <code>rename_columns</code> to take optional function argument for mapping. @nvamsikrishna05</li> </ul>"},{"location":"CHANGELOG/#v0210-2021-07-16","title":"v0.21.0 - 2021-07-16","text":"<ul> <li>[ENH] Drop <code>fill_value</code> parameter from <code>complete</code>. Users can use <code>fillna</code> instead. @samukweku</li> <li>[BUG] Fix bug in <code>pivot_longer</code> with single level columns. @samukweku</li> <li>[BUG] Disable exchange rates API until we can find another one to hit. @ericmjl</li> <li>[ENH] Change <code>coalesce</code> to return columns; also use <code>bfill</code>, <code>ffill</code>,     which is faster than <code>combine_first</code> @samukweku</li> <li>[ENH] Use <code>eval</code> for string conditions in <code>update_where</code>. @samukweku</li> <li>[ENH] Add clearer error messages for <code>pivot_longer</code>. h/t to @tdhock     for the observation. Issue #836 @samukweku</li> <li>[ENH] <code>select_columns</code> now uses variable arguments (*args),     to provide a simpler selection without the need for lists. - @samukweku</li> <li>[ENH] <code>encode_categoricals</code> refactored to use generic functions     via <code>functools.dispatch</code>. - @samukweku</li> <li>[ENH] Updated convert_excel_date to throw meaningful error when values contain non-numeric. @nvamsikrishna05</li> </ul>"},{"location":"CHANGELOG/#v02014-2021-03-25","title":"v0.20.14 - 2021-03-25","text":"<ul> <li>[ENH] Add <code>dropna</code> parameter to groupby_agg. @samukweku</li> <li>[ENH] <code>complete</code> adds a <code>by</code> parameter to expose explicit missing values per group, via groupby. @samukweku</li> <li>[ENH] Fix check_column to support single inputs - fixes <code>label_encode</code>. @zbarry</li> </ul>"},{"location":"CHANGELOG/#v02013-2021-02-25","title":"v0.20.13 - 2021-02-25","text":"<ul> <li>[ENH] Performance improvements to <code>expand_grid</code>. @samukweku</li> <li>[HOTFIX] Add <code>multipledispatch</code> to pip requirements. @ericmjl</li> </ul>"},{"location":"CHANGELOG/#v02012-2021-02-25","title":"v0.20.12 - 2021-02-25","text":"<ul> <li>[INF] Auto-release GitHub action maintenance. @loganthomas</li> </ul>"},{"location":"CHANGELOG/#v02011-2021-02-24","title":"v0.20.11 - 2021-02-24","text":"<ul> <li>[INF] Setup auto-release GitHub action. @loganthomas</li> <li>[INF] Deploy <code>darglint</code> package for docstring linting. Issue #745. @loganthomas</li> <li>[ENH] Added optional truncation to <code>clean_names</code> function. Issue #753. @richardqiu</li> <li>[ENH] Added <code>timeseries.flag_jumps()</code> function. Issue #711. @loganthomas</li> <li>[ENH] <code>pivot_longer</code> can handle multiple values in paired columns, and can reshape     using a list/tuple of regular expressions in <code>names_pattern</code>. @samukweku</li> <li>[ENH] Replaced default numeric conversion of dataframe with a <code>dtypes</code> parameter,     allowing the user to control the data types. - @samukweku</li> <li>[INF] Loosen dependency specifications. Switch to pip-tools for managing     dependencies. Issue #760. @MinchinWeb</li> <li>[DOC] added pipenv installation instructions @evan-anderson</li> <li>[ENH] Add <code>pivot_wider</code> function, which is the inverse of the <code>pivot_longer</code>     function. @samukweku</li> <li>[INF] Add <code>openpyxl</code> to <code>environment-dev.yml</code>. @samukweku</li> <li>[ENH] Reduce code by reusing existing functions for fill_direction. @samukweku</li> <li>[ENH] Improvements to <code>pivot_longer</code> function, with improved speed and cleaner code.     <code>dtypes</code> parameter dropped; user can change dtypes with pandas' <code>astype</code> method, or     pyjanitor's <code>change_type</code> method. @samukweku</li> <li>[ENH] Add kwargs to <code>encode_categorical</code> function, to create ordered categorical columns,     or categorical columns with explicit categories. @samukweku</li> <li>[ENH] Improvements to <code>complete</code> method. Use <code>pd.merge</code> to handle duplicates and     null values. @samukweku</li> <li>[ENH] Add <code>new_column_names</code> parameter to <code>process_text</code>, allowing a user to     create a new column name after processing a text column. Also added a <code>merge_frame</code>     parameter, allowing dataframe merging, if the result of the text processing is a     dataframe.@samukweku</li> <li>[ENH] Add <code>aggfunc</code> parameter to pivot_wider. @samukweku</li> <li>[ENH] Modified the <code>check</code> function in utils to verify if a value is a callable. @samukweku</li> <li>[ENH] Add a base <code>_select_column</code> function, using <code>functools.singledispatch</code>,     to allow for flexible columns selection. @samukweku</li> <li>[ENH] pivot_longer and pivot_wider now support janitor.select_columns syntax,     allowing for more flexible and dynamic column selection. @samukweku</li> </ul>"},{"location":"CHANGELOG/#v02010","title":"v0.20.10","text":"<ul> <li>[ENH] Added function <code>sort_timestamps_monotonically</code> to timeseries functions @UGuntupalli</li> <li>[ENH] Added the complete function for converting implicit missing values     to explicit ones. @samukweku</li> <li>[ENH] Further simplification of expand_grid. @samukweku</li> <li>[BUGFIX] Added copy() method to original dataframe, to avoid mutation. Issue #729. @samukweku</li> <li>[ENH] Added <code>also</code> method for running functions in chain with no return values.</li> <li>[DOC] Added a <code>timeseries</code> module section to website docs. Issue #742. @loganthomas</li> <li>[ENH] Added a <code>pivot_longer</code> function, a wrapper around <code>pd.melt</code> and similar to     tidyr's <code>pivot_longer</code> function. Also added an example notebook. @samukweku</li> <li>[ENH] Fixed code to returns error if <code>fill_value</code> is not a dictionary. @samukweku</li> <li>[INF] Welcome bot (.github/config.yml) for new users added. Issue #739. @samukweku</li> </ul>"},{"location":"CHANGELOG/#v0209","title":"v0.20.9","text":"<ul> <li>[ENH] Updated groupby_agg function to account for null entries in the <code>by</code> argument. @samukweku</li> <li>[ENH] Added function <code>groupby_topk</code> to janitor functions @mphirke</li> </ul>"},{"location":"CHANGELOG/#v0208","title":"v0.20.8","text":"<ul> <li>[ENH] Upgraded <code>update_where</code> function to use either the pandas query style,     or boolean indexing via the <code>loc</code> method. Also updated <code>find_replace</code> function to use the <code>loc</code>     method directly, instead of routing it through the <code>update_where</code> function. @samukweku</li> <li>[INF] Update <code>pandas</code> minimum version to 1.0.0. @hectormz</li> <li>[DOC] Updated the general functions API page to show all available functions. @samukweku</li> <li>[DOC] Fix the few lacking type annotations of functions. @VPerrollaz</li> <li>[DOC] Changed the signature from str to Optional[str] when initialized by None. @VPerrollaz</li> <li>[DOC] Add the Optional type for all signatures of the API. @VPerrollaz</li> <li>[TST] Updated test_expand_grid to account for int dtype difference in Windows OS @samukweku</li> <li>[TST] Make importing <code>pandas</code> testing functions follow uniform pattern. @hectormz</li> <li>[ENH] Added <code>process_text</code> wrapper function for all Pandas string methods. @samukweku</li> <li>[TST] Only skip tests for non-installed libraries on local machine. @hectormz</li> <li>[DOC] Fix minor issues in documentation. @hectormz</li> <li>[ENH] Added <code>fill_direction</code> function for forward/backward fills on missing values     for selected columns in a dataframe. @samukweku</li> <li>[ENH] Simpler logic and less lines of code for expand_grid function @samukweku</li> </ul>"},{"location":"CHANGELOG/#v0207","title":"v0.20.7","text":"<ul> <li>[TST] Add a test for transform_column to check for nonmutation. @VPerrollaz</li> <li>[ENH] Contributed <code>expand_grid</code> function by @samukweku</li> </ul>"},{"location":"CHANGELOG/#v0206","title":"v0.20.6","text":"<ul> <li>[DOC] Pep8 all examples. @VPerrollaz</li> <li>[TST] Add docstrings to tests @hectormz</li> <li>[INF] Add <code>debug-statements</code>, <code>requirements-txt-fixer</code>, and <code>interrogate</code> to <code>pre-commit</code>. @hectormz</li> <li>[ENH] Upgraded transform_column to use df.assign underneath the hood,     and also added option to transform column elementwise (via apply)     or columnwise (thus operating on a series). @ericmjl</li> </ul>"},{"location":"CHANGELOG/#v0205","title":"v0.20.5","text":"<ul> <li>[INF] Replace <code>pycodestyle</code> with <code>flake8</code> in order to add <code>pandas-vet</code> linter @hectormz</li> <li>[ENH] <code>select_columns()</code> now raises <code>NameError</code> if column label in     <code>search_columns_labels</code> is missing from <code>DataFrame</code> columns. @smu095</li> </ul>"},{"location":"CHANGELOG/#v0201","title":"v0.20.1","text":"<ul> <li>[DOC] Added an example for groupby_agg in general functions @samukweku</li> <li>[ENH] Contributed <code>sort_naturally()</code> function. @ericmjl</li> </ul>"},{"location":"CHANGELOG/#v0200","title":"v0.20.0","text":"<ul> <li>[DOC] Edited transform_column dest_column_name kwarg description to be clearer on defaults by @evan-anderson.</li> <li>[ENH] Replace <code>apply()</code> in favor of <code>pandas</code> functions in several functions. @hectormz</li> <li>[ENH] Add <code>ecdf()</code> Series function by @ericmjl.</li> <li>[DOC] Update API policy for clarity. @ericmjl</li> <li>[ENH] Enforce string conversion when cleaning names. @ericmjl</li> <li>[ENH] Change <code>find_replace</code> implementation to use keyword arguments to specify columns to perform find and replace on. @ericmjl</li> <li>[ENH] Add <code>jitter()</code> dataframe function by @rahosbach</li> </ul>"},{"location":"CHANGELOG/#v0190","title":"v0.19.0","text":"<ul> <li>[ENH] Add xarray support and clone_using / convert_datetime_to_number funcs by @zbarry.</li> </ul>"},{"location":"CHANGELOG/#v0183","title":"v0.18.3","text":"<ul> <li>[ENH] Series toset() functionality #570 @eyaltrabelsi</li> <li>[ENH] Added option to coalesce function to not delete coalesced columns. @gddcunh</li> <li>[ENH] Added functionality to deconcatenate tuple/list/collections in a column to deconcatenate_column @zbarry</li> <li>[ENH] Fix error message when length of new_column_names is wrong @DollofCutty</li> <li>[DOC] Fixed several examples of functional syntax in <code>functions.py</code>. @bdice</li> <li>[DOC] Fix #noqa comments showing up in docs by @hectormz</li> <li>[ENH] Add unionizing a group of dataframes' categoricals. @zbarry</li> <li>[DOC] Fix contributions hyperlinks in <code>AUTHORS.rst</code> and contributions by @hectormz</li> <li>[INF] Add <code>pre-commit</code> hooks to repository by @ericmjl</li> <li>[DOC] Fix formatting code in <code>CONTRIBUTING.rst</code> by @hectormz</li> <li>[DOC] Changed the typing for most \"column_name(s)\" to Hashable rather than enforcing strings, to more closely match Pandas API by @dendrondal</li> <li>[INF] Edited pycodestyle and Black parameters to avoid venvs by @dendrondal</li> </ul>"},{"location":"CHANGELOG/#v0182","title":"v0.18.2","text":"<ul> <li>[INF] Make requirements.txt smaller @eyaltrabelsi</li> <li>[ENH] Add a reset_index parameter to shuffle @eyaltrabelsi</li> <li>[DOC] Added contribution page link to readme @eyaltrabelsi</li> <li>[DOC] fix example for <code>update_where</code>, provide a bit more detail, and expand the bad_values example notebook to demonstrate its use by @anzelpwj.</li> <li>[INF] Fix pytest marks by @ericmjl (issue #520)</li> <li>[ENH] add example notebook with use of finance submodule methods by @rahosbach</li> <li>[DOC] added a couple of admonitions for Windows users. h/t @anzelpwj for debugging     help when a few tests failed for <code>win32</code> @Ram-N</li> <li>[ENH] Pyjanitor for PySpark @zjpoh</li> <li>[ENH] Add pyspark clean_names @zjpoh</li> <li>[ENH] Convert asserts to raise exceptions by @hectormz</li> <li>[ENH] Add decorator functions for missing and error handling @jiafengkevinchen</li> <li>[DOC] Update README with functional <code>pandas</code> API example. @ericmjl</li> <li>[INF] Move <code>get_features_targets()</code> to new <code>ml.py</code> module by @hectormz</li> <li>[ENH] Add chirality to morgan fingerprints in janitor.chemistry submodule by @Clayton-Springer</li> <li>[INF] <code>import_message</code> suggests python dist. appropriate installs by @hectormz</li> <li>[ENH] Add count_cumulative_unique() method to janitor.functions submodule by @rahosbach</li> <li>[ENH] Add <code>update_where()</code> method to <code>janitor.spark.functions</code> submodule by @zjpoh</li> </ul>"},{"location":"CHANGELOG/#v0181","title":"v0.18.1","text":"<ul> <li>[ENH] extend find_replace functionality to allow both exact match and     regular-expression-based fuzzy match by @shandou</li> <li>[ENH] add preserve_position kwarg to deconcatenate_column with tests     by @shandou and @ericmjl</li> <li>[DOC] add contributions that did not leave <code>git</code> traces by @ericmjl</li> <li>[ENH] add inflation adjustment in finance submodule by @rahosbach</li> <li>[DOC] clarified how new functions should be implemented by @shandou</li> <li>[ENH] add optional removal of accents on functions.clean_names, enabled by     default by @mralbu</li> <li>[ENH] add camelCase conversion to snake_case on <code>clean_names</code> by @ericmjl,     h/t @jtaylor for sharing original</li> <li>[ENH] Added <code>null_flag</code> function which can mark null values in rows.     Implemented by @anzelpwj</li> <li>[ENH] add engineering submodule with unit conversion method by @rahosbach</li> <li>[DOC] add PyPI project description</li> <li>[ENH] add example notebook with use of finance submodule methods     by @rahosbach</li> </ul> <p>For changes that happened prior to v0.18.1, please consult the closed PRs, which can be found here.</p> <p>We thank all contributors who have helped make <code>pyjanitor</code> the package that it is today.</p>"},{"location":"devguide/","title":"Development Guide","text":"<p>For those of you who are interested in contributing code to the project, many previous contributors have wrestled with a variety of ways of getting set up. While we can't cover every single last configuration, we could cover some of the more common cases. Here they are for your benefit!</p>"},{"location":"devguide/#development-containers-with-vscode","title":"Development Containers with VSCode","text":"<p>As of 29 May 2020, development containers are supported! This is the preferred way to get you started up and running, as it creates a uniform setup environment that is much easier for the maintainers to debug, because you are provided with a pre-built and clean development environment free of any assumptions of your own system. You don't have to wrestle with conda wait times if you don't want to!</p> <p>To get started:</p> <ol> <li>Fork the repository.</li> <li>Ensure you have Docker running on your local machine.</li> <li>Ensure you have VSCode running on your local machine.</li> <li>In VS Code, Install an extension called <code>Remote - Containers</code>.</li> <li>In Visual Studio Code,     click on the quick actions Status Bar item in the lower left corner.</li> <li>Then select \"Remote Containers: Clone Repository In Container Volume\".</li> <li>Enter in the URL of your fork of <code>pyjanitor</code>.</li> </ol> <p>VSCode will pull down the prebuilt Docker container, git clone the repository for you inside an isolated Docker volume, and mount the repository directory inside your Docker container.</p> <p>Follow best practices to submit a pull request by making a feature branch. Now, hack away, and submit in your pull request!</p> <p>You shouldn't be able to access the cloned repo on your local hard drive. If you do want local access, then clone the repo locally first before selecting \"Remote Containers: Open Folder In Container\".</p> <p>If you find something is broken because a utility is missing in the container, submit a PR with the appropriate build command inserted in the Dockerfile. Care has been taken to document what each step does, so please read the in-line documentation in the Dockerfile carefully.</p>"},{"location":"devguide/#manual-setup","title":"Manual Setup","text":""},{"location":"devguide/#fork-the-repository","title":"Fork the repository","text":"<p>Firstly, begin by forking the <code>pyjanitor</code> repo on GitHub. Then, clone your fork locally:</p> <pre><code>git clone git@github.com:&lt;your_github_username&gt;/pyjanitor.git\n</code></pre>"},{"location":"devguide/#setup-the-conda-environment","title":"Setup the conda environment","text":"<p>Now, install your cloned repo into a conda environment. Assuming you have conda installed, this is how you set up your fork for local development</p> <p><pre><code>cd pyjanitor/\n# Activate the pyjanitor conda environment\nsource activate pyjanitor-dev\n\n# Create your conda environment\nconda env create -f environment-dev.yml\n\n# Install PyJanitor in development mode\npython setup.py develop\n\n# Register current virtual environment as a Jupyter Python kernel\npython -m ipykernel install --user --name pyjanitor-dev --display-name \"PyJanitor development\"\n</code></pre> If you plan to write any notebooks, make sure they run correctly inside the environment by selecting the correct kernel from the top right corner of JupyterLab!</p> <p>PyCharm Users</p> <p>For PyCharm users, here are some <code>instructions &lt;PYCHARM_USERS.html&gt;</code>__  to get your Conda environment set up.</p>"},{"location":"devguide/#install-the-pre-commit-hooks","title":"Install the pre-commit hooks","text":"<p><code>pre-commit</code> hooks are available to run code formatting checks automagically before git commits happen. If you did not have these installed before, run the following commands:</p> <pre><code># Update your environment to install pre-commit\nconda env update -f environment-dev.yml\n# Install pre-commit hooks\npre-commit install\n</code></pre>"},{"location":"devguide/#build-docs-locally","title":"Build docs locally","text":"<p>You should also be able to preview the docs locally. To do this, from the main <code>pyjanitor</code> directory:</p> <p><pre><code>python -m mkdocs serve\n</code></pre> The command above allows you to view the documentation locally in your browser.</p> <p>If you get any errors about importing modules when running <code>mkdocs serve</code>, first activate the development environment:</p> <pre><code>source activate pyjanitor-dev || conda activate pyjanitor-dev\n</code></pre>"},{"location":"devguide/#plan-out-the-change-youd-like-to-contribute","title":"Plan out the change you'd like to contribute","text":"<p>The old adage rings true:</p> <p>failing to plan means planning to fail.</p> <p>We'd encourage you to flesh out the idea you'd like to contribute on the GitHub issue tracker before embarking on a contribution. Submitting new code, in particular, is one where the maintainers will need more consideration, after all, any new code submitted introduces a new maintenance burden, unless you the contributor would like to join the maintainers team!</p> <p>To kickstart the discussion, submit an issue to the <code>pyjanitor</code> GitHub issue tracker describing your planned changes. The issue tracker also helps us keep track of who is working on what.</p>"},{"location":"devguide/#create-a-branch-for-local-development","title":"Create a branch for local development","text":"<p>New contributions to <code>pyjanitor</code> should be done in a new branch that you have based off the latest version of the <code>dev</code> branch.</p> <p>To create a new branch:</p> <pre><code>git checkout -b &lt;name-of-your-bugfix-or-feature&gt; dev\n</code></pre>"},{"location":"devguide/#write-the-code","title":"Write the Code","text":"<p>As you work, remember to adhere to the coding standards and practices that pyjanitor follows. If in doubt, refer to existing code or bring up your questions in the GitHub issue you created. Some tips for writing code:</p> <p>Commit Early, Commit Often: Make frequent, smaller commits. This helps to track progress and makes it easier for maintainers to follow your work. Include useful commit messages that describe the changes you're making:</p> <p>Stay Updated with dev branch: Regularly pull the latest changes from the dev branch to ensure your feature branch is up-to-date, reducing the likelihood of merge conflicts:</p> <pre><code>git fetch origin dev\ngit rebase origin/dev\n</code></pre> <p>Write Tests: For every feature or bugfix, accompanying tests are essential. They ensure the feature works as expected or the bug is truly fixed. Tests should ideally run in less than 2 seconds. If using Hypothesis for testing, apply the <code>@settings(max_examples=10, timeout=None)</code> decorator.</p>"},{"location":"devguide/#check-your-environment","title":"Check your environment","text":"<p>To ensure that your environment is properly set up, run the following command:</p> <pre><code>python -m pytest -m \"not turtle\"\n</code></pre> <p>If all tests pass then your environment is setup for development and you are ready to contribute \ud83e\udd73.</p>"},{"location":"devguide/#check-your-code","title":"Check your code","text":"<p>When you're done making changes, commit your staged files with a meaningful message. If installed correctly, you will automatically run pre-commit hooks that check code for code style adherence. These same checks will be run on GitHub Actions, so no worries if you don't have the running locally. If the pre-commit hooks fail, be sure to fix the issues (as raised by them) before committing. If you feel lost on how to fix the code, please feel free to ping the maintainers on GitHub - we can take things slowly to get it right, and make this an educational opportunity for all who come by!</p> <p>Tip</p> <p>You can run <code>python -m pytest -m \"not turtle\"</code> to run the fast tests.</p> <p>Running test locally</p> <p>When you run tests locally, the tests in <code>chemistry.py</code>, <code>biology.py</code>, <code>spark.py</code> are automatically skipped if you don't have the optional dependencies (e.g. <code>rdkit</code>) installed.</p> <p>Info</p> <ul> <li>pre-commit does not run your tests locally rather all tests are run in continuous integration (CI).</li> <li>All tests must pass in CI before the pull request is accepted, and the continuous integration system up on GitHub Actions will help run all of the tests before they are committed to the repository.</li> </ul>"},{"location":"devguide/#commit-your-changes","title":"Commit your changes","text":"<p>Now you can commit your changes and push your branch to GitHub:</p> <pre><code>git add .\ngit commit -m \"Your detailed description of your changes.\"\ngit push origin &lt;name-of-your-bugfix-or-feature&gt;\n</code></pre>"},{"location":"devguide/#submit-a-pull-request-through-the-github-website","title":"Submit a pull request through the GitHub website","text":"<p>Congratulations \ud83c\udf89\ud83c\udf89\ud83c\udf89, you've made it to the penultimate step; your code is ready to be checked and reviewed by the maintainers! Head over to the GitHub website and create a pull request. When you are picking out which branch to merge into, a.k.a. the target branch, be sure to select <code>dev</code> (not <code>master</code>).</p>"},{"location":"devguide/#fix-any-remaining-issues","title":"Fix any remaining issues","text":"<p>It's rare, but you might at this point still encounter issues, as the continuous integration (CI) system on GitHub Actions checks your code. Some of these might not be your fault; rather, it might well be the case that your code fell a little bit out of date as others' pull requests are merged into the repository.</p> <p>In any case, if there are any issues, the pipeline will fail out. We check for code style, docstring coverage, test coverage, and doc discovery. If you're comfortable looking at the pipeline logs, feel free to do so; they are open to all to view. Otherwise, one of the dev team members can help you with reviewing the code checks.</p>"},{"location":"devguide/#code-compatibility","title":"Code Compatibility","text":"<p>pyjanitor supports Python 3.6+, so all contributed code must maintain this compatibility.</p>"},{"location":"devguide/#docstring-style","title":"Docstring Style","text":"<p>We follow the Google docstring style, please read Napoleon's documentation for a detailed introduction.</p> <p>We are using the following docstring section identifiers -- please stick to them if you are contributing a docstring change:</p> <ul> <li>Examples: for sample code blocks demonstrating the use of pyjanitor. keep example blocks in the <code>pycon</code> (python-console) style, i.e., input code prefixed by <code>&gt;&gt;&gt;</code> and <code>...</code>, and output code with no prefix.</li> <li>Args: for function parameters</li> <li>Raises: for exceptions</li> <li>Returns: for function return value(s)</li> <li>Yields: for generator yield value(s)</li> </ul> <p>If possible, it is preferable to stick to this section ordering within each docstring.</p>"},{"location":"devguide/#tips","title":"Tips","text":"<p>To run a subset of tests:</p> <pre><code>pytest tests.test_functions\n</code></pre>"},{"location":"api/biology/","title":"Biology","text":"<p>Biology and bioinformatics-oriented data cleaning functions.</p>"},{"location":"api/biology/#janitor.biology.join_fasta","title":"<code>join_fasta(df, filename, id_col, column_name)</code>","text":"<p>Convenience method to join in a FASTA file as a column.</p> <p>This allows us to add the string sequence of a FASTA file as a new column of data in the dataframe.</p> <p>This method only attaches the string representation of the SeqRecord.Seq object from Biopython. Does not attach the full SeqRecord. Alphabet is also not stored, under the assumption that the data scientist has domain knowledge of what kind of sequence is being read in (nucleotide vs. amino acid.)</p> <p>This method mutates the original DataFrame.</p> <p>For more advanced functions, please use phylopandas.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import tempfile\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor.biology\n&gt;&gt;&gt; tf = tempfile.NamedTemporaryFile()\n&gt;&gt;&gt; tf.write('''&gt;SEQUENCE_1\n... MTEITAAMVKELRESTGAGMMDCK\n... &gt;SEQUENCE_2\n... SATVSEINSETDFVAKN'''.encode('utf8'))\n66\n&gt;&gt;&gt; tf.seek(0)\n0\n&gt;&gt;&gt; df = pd.DataFrame({\"sequence_accession\":\n... [\"SEQUENCE_1\", \"SEQUENCE_2\", ]})\n&gt;&gt;&gt; df = df.join_fasta(\n...     filename=tf.name,\n...     id_col='sequence_accession',\n...     column_name='sequence',\n... )\n&gt;&gt;&gt; df.sequence\n0    MTEITAAMVKELRESTGAGMMDCK\n1           SATVSEINSETDFVAKN\nName: sequence, dtype: object\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>filename</code> <code>str</code> <p>Path to the FASTA file.</p> required <code>id_col</code> <code>str</code> <p>The column in the DataFrame that houses sequence IDs.</p> required <code>column_name</code> <code>str</code> <p>The name of the new column.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with new FASTA string sequence column.</p> Source code in <code>janitor/biology.py</code> <pre><code>@pf.register_dataframe_method\n@deprecated_alias(col_name=\"column_name\")\ndef join_fasta(\n    df: pd.DataFrame, filename: str, id_col: str, column_name: str\n) -&gt; pd.DataFrame:\n    \"\"\"Convenience method to join in a FASTA file as a column.\n\n    This allows us to add the string sequence of a FASTA file as a new column\n    of data in the dataframe.\n\n    This method only attaches the string representation of the SeqRecord.Seq\n    object from Biopython. Does not attach the full SeqRecord. Alphabet is\n    also not stored, under the assumption that the data scientist has domain\n    knowledge of what kind of sequence is being read in (nucleotide vs. amino\n    acid.)\n\n    This method mutates the original DataFrame.\n\n    For more advanced functions, please use phylopandas.\n\n    Examples:\n        &gt;&gt;&gt; import tempfile\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor.biology\n        &gt;&gt;&gt; tf = tempfile.NamedTemporaryFile()\n        &gt;&gt;&gt; tf.write('''&gt;SEQUENCE_1\n        ... MTEITAAMVKELRESTGAGMMDCK\n        ... &gt;SEQUENCE_2\n        ... SATVSEINSETDFVAKN'''.encode('utf8'))\n        66\n        &gt;&gt;&gt; tf.seek(0)\n        0\n        &gt;&gt;&gt; df = pd.DataFrame({\"sequence_accession\":\n        ... [\"SEQUENCE_1\", \"SEQUENCE_2\", ]})\n        &gt;&gt;&gt; df = df.join_fasta(  # doctest: +SKIP\n        ...     filename=tf.name,\n        ...     id_col='sequence_accession',\n        ...     column_name='sequence',\n        ... )\n        &gt;&gt;&gt; df.sequence  # doctest: +SKIP\n        0    MTEITAAMVKELRESTGAGMMDCK\n        1           SATVSEINSETDFVAKN\n        Name: sequence, dtype: object\n\n    Args:\n        df: A pandas DataFrame.\n        filename: Path to the FASTA file.\n        id_col: The column in the DataFrame that houses sequence IDs.\n        column_name: The name of the new column.\n\n    Returns:\n        A pandas DataFrame with new FASTA string sequence column.\n    \"\"\"\n    seqrecords = {\n        x.id: x.seq.__str__() for x in SeqIO.parse(filename, \"fasta\")\n    }\n    seq_col = [seqrecords[i] for i in df[id_col]]\n    df[column_name] = seq_col\n    return df\n</code></pre>"},{"location":"api/chemistry/","title":"Chemistry","text":"<p>Chemistry and cheminformatics-oriented data cleaning functions.</p>"},{"location":"api/chemistry/#janitor.chemistry.maccs_keys_fingerprint","title":"<code>maccs_keys_fingerprint(df, mols_column_name)</code>","text":"<p>Convert a column of RDKIT mol objects into MACCS Keys Fingerprints.</p> <p>Returns a new dataframe without any of the original data. This is intentional to leave the user with the data requested.</p> <p>This method does not mutate the original DataFrame.</p> <p>Examples:</p> <p>Functional usage</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor.chemistry\n&gt;&gt;&gt; df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]})\n&gt;&gt;&gt; maccs = janitor.chemistry.maccs_keys_fingerprint(\n...     df=df.smiles2mol('smiles', 'mols'),\n...     mols_column_name='mols'\n... )\n&gt;&gt;&gt; len(maccs.columns)\n167\n</code></pre> <p>Method chaining usage</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor.chemistry\n&gt;&gt;&gt; df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]})\n&gt;&gt;&gt; maccs = (\n...     df.smiles2mol('smiles', 'mols')\n...         .maccs_keys_fingerprint(mols_column_name='mols')\n... )\n&gt;&gt;&gt; len(maccs.columns)\n167\n</code></pre> <p>If you wish to join the maccs keys fingerprints back into the original dataframe, this can be accomplished by doing a <code>join</code>, because the indices are preserved:</p> <pre><code>&gt;&gt;&gt; joined = df.join(maccs)\n&gt;&gt;&gt; len(joined.columns)\n169\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>mols_column_name</code> <code>Hashable</code> <p>The name of the column that has the RDKIT mol objects.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new pandas DataFrame of MACCS keys fingerprints.</p> Source code in <code>janitor/chemistry.py</code> <pre><code>@pf.register_dataframe_method\n@deprecated_alias(mols_col=\"mols_column_name\")\ndef maccs_keys_fingerprint(\n    df: pd.DataFrame, mols_column_name: Hashable\n) -&gt; pd.DataFrame:\n    \"\"\"Convert a column of RDKIT mol objects into MACCS Keys Fingerprints.\n\n    Returns a new dataframe without any of the original data.\n    This is intentional to leave the user with the data requested.\n\n    This method does not mutate the original DataFrame.\n\n    Examples:\n        Functional usage\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor.chemistry\n        &gt;&gt;&gt; df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]})\n        &gt;&gt;&gt; maccs = janitor.chemistry.maccs_keys_fingerprint(\n        ...     df=df.smiles2mol('smiles', 'mols'),\n        ...     mols_column_name='mols'\n        ... )\n        &gt;&gt;&gt; len(maccs.columns)\n        167\n\n        Method chaining usage\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor.chemistry\n        &gt;&gt;&gt; df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]})\n        &gt;&gt;&gt; maccs = (\n        ...     df.smiles2mol('smiles', 'mols')\n        ...         .maccs_keys_fingerprint(mols_column_name='mols')\n        ... )\n        &gt;&gt;&gt; len(maccs.columns)\n        167\n\n        If you wish to join the maccs keys fingerprints back into the\n        original dataframe, this can be accomplished by doing a `join`,\n        because the indices are preserved:\n\n        &gt;&gt;&gt; joined = df.join(maccs)\n        &gt;&gt;&gt; len(joined.columns)\n        169\n\n    Args:\n        df: A pandas DataFrame.\n        mols_column_name: The name of the column that has the RDKIT mol\n            objects.\n\n    Returns:\n        A new pandas DataFrame of MACCS keys fingerprints.\n    \"\"\"\n\n    maccs = [GetMACCSKeysFingerprint(m) for m in df[mols_column_name]]\n\n    np_maccs = []\n\n    for macc in maccs:\n        arr = np.zeros((1,))\n        DataStructs.ConvertToNumpyArray(macc, arr)\n        np_maccs.append(arr)\n    np_maccs = np.vstack(np_maccs)\n    fmaccs = pd.DataFrame(np_maccs)\n    fmaccs.index = df.index\n    return fmaccs\n</code></pre>"},{"location":"api/chemistry/#janitor.chemistry.molecular_descriptors","title":"<code>molecular_descriptors(df, mols_column_name)</code>","text":"<p>Convert a column of RDKIT mol objects into a Pandas DataFrame of molecular descriptors.</p> <p>Returns a new dataframe without any of the original data. This is intentional to leave the user only with the data requested.</p> <p>This method does not mutate the original DataFrame.</p> <p>The molecular descriptors are from the <code>rdkit.Chem.rdMolDescriptors</code>:</p> <pre><code>Chi0n, Chi0v, Chi1n, Chi1v, Chi2n, Chi2v, Chi3n, Chi3v,\nChi4n, Chi4v, ExactMolWt, FractionCSP3, HallKierAlpha, Kappa1,\nKappa2, Kappa3, LabuteASA, NumAliphaticCarbocycles,\nNumAliphaticHeterocycles, NumAliphaticRings, NumAmideBonds,\nNumAromaticCarbocycles, NumAromaticHeterocycles, NumAromaticRings,\nNumAtomStereoCenters, NumBridgeheadAtoms, NumHBA, NumHBD,\nNumHeteroatoms, NumHeterocycles, NumLipinskiHBA, NumLipinskiHBD,\nNumRings, NumSaturatedCarbocycles, NumSaturatedHeterocycles,\nNumSaturatedRings, NumSpiroAtoms, NumUnspecifiedAtomStereoCenters,\nTPSA.\n</code></pre> <p>Examples:</p> <p>Functional usage</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor.chemistry\n&gt;&gt;&gt; df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]})\n&gt;&gt;&gt; mol_desc = (\n...     janitor.chemistry.molecular_descriptors(\n...         df=df.smiles2mol('smiles', 'mols'),\n...         mols_column_name='mols'\n...     )\n... )\n&gt;&gt;&gt; mol_desc.TPSA\n0    34.14\n1    37.30\nName: TPSA, dtype: float64\n</code></pre> <p>Method chaining usage</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor.chemistry\n&gt;&gt;&gt; df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]})\n&gt;&gt;&gt; mol_desc = (\n...     df.smiles2mol('smiles', 'mols')\n...     .molecular_descriptors(mols_column_name='mols')\n... )\n&gt;&gt;&gt; mol_desc.TPSA\n0    34.14\n1    37.30\nName: TPSA, dtype: float64\n</code></pre> <p>If you wish to join the molecular descriptors back into the original dataframe, this can be accomplished by doing a <code>join</code>, because the indices are preserved:</p> <pre><code>&gt;&gt;&gt; joined = df.join(mol_desc)\n&gt;&gt;&gt; len(joined.columns)\n41\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>mols_column_name</code> <code>Hashable</code> <p>The name of the column that has the RDKIT mol objects.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new pandas DataFrame of molecular descriptors.</p> Source code in <code>janitor/chemistry.py</code> <pre><code>@pf.register_dataframe_method\n@deprecated_alias(mols_col=\"mols_column_name\")\ndef molecular_descriptors(\n    df: pd.DataFrame, mols_column_name: Hashable\n) -&gt; pd.DataFrame:\n    \"\"\"Convert a column of RDKIT mol objects into a Pandas DataFrame\n    of molecular descriptors.\n\n    Returns a new dataframe without any of the original data. This is\n    intentional to leave the user only with the data requested.\n\n    This method does not mutate the original DataFrame.\n\n    The molecular descriptors are from the `rdkit.Chem.rdMolDescriptors`:\n\n    ```text\n    Chi0n, Chi0v, Chi1n, Chi1v, Chi2n, Chi2v, Chi3n, Chi3v,\n    Chi4n, Chi4v, ExactMolWt, FractionCSP3, HallKierAlpha, Kappa1,\n    Kappa2, Kappa3, LabuteASA, NumAliphaticCarbocycles,\n    NumAliphaticHeterocycles, NumAliphaticRings, NumAmideBonds,\n    NumAromaticCarbocycles, NumAromaticHeterocycles, NumAromaticRings,\n    NumAtomStereoCenters, NumBridgeheadAtoms, NumHBA, NumHBD,\n    NumHeteroatoms, NumHeterocycles, NumLipinskiHBA, NumLipinskiHBD,\n    NumRings, NumSaturatedCarbocycles, NumSaturatedHeterocycles,\n    NumSaturatedRings, NumSpiroAtoms, NumUnspecifiedAtomStereoCenters,\n    TPSA.\n    ```\n\n    Examples:\n        Functional usage\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor.chemistry\n        &gt;&gt;&gt; df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]})\n        &gt;&gt;&gt; mol_desc = (\n        ...     janitor.chemistry.molecular_descriptors(\n        ...         df=df.smiles2mol('smiles', 'mols'),\n        ...         mols_column_name='mols'\n        ...     )\n        ... )\n        &gt;&gt;&gt; mol_desc.TPSA\n        0    34.14\n        1    37.30\n        Name: TPSA, dtype: float64\n\n        Method chaining usage\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor.chemistry\n        &gt;&gt;&gt; df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]})\n        &gt;&gt;&gt; mol_desc = (\n        ...     df.smiles2mol('smiles', 'mols')\n        ...     .molecular_descriptors(mols_column_name='mols')\n        ... )\n        &gt;&gt;&gt; mol_desc.TPSA\n        0    34.14\n        1    37.30\n        Name: TPSA, dtype: float64\n\n        If you wish to join the molecular descriptors back into the original\n        dataframe, this can be accomplished by doing a `join`,\n        because the indices are preserved:\n\n        &gt;&gt;&gt; joined = df.join(mol_desc)\n        &gt;&gt;&gt; len(joined.columns)\n        41\n\n    Args:\n        df: A pandas DataFrame.\n        mols_column_name: The name of the column that has the RDKIT mol\n            objects.\n\n    Returns:\n        A new pandas DataFrame of molecular descriptors.\n    \"\"\"\n    descriptors = [\n        CalcChi0n,\n        CalcChi0v,\n        CalcChi1n,\n        CalcChi1v,\n        CalcChi2n,\n        CalcChi2v,\n        CalcChi3n,\n        CalcChi3v,\n        CalcChi4n,\n        CalcChi4v,\n        CalcExactMolWt,\n        CalcFractionCSP3,\n        CalcHallKierAlpha,\n        CalcKappa1,\n        CalcKappa2,\n        CalcKappa3,\n        CalcLabuteASA,\n        CalcNumAliphaticCarbocycles,\n        CalcNumAliphaticHeterocycles,\n        CalcNumAliphaticRings,\n        CalcNumAmideBonds,\n        CalcNumAromaticCarbocycles,\n        CalcNumAromaticHeterocycles,\n        CalcNumAromaticRings,\n        CalcNumAtomStereoCenters,\n        CalcNumBridgeheadAtoms,\n        CalcNumHBA,\n        CalcNumHBD,\n        CalcNumHeteroatoms,\n        CalcNumHeterocycles,\n        CalcNumLipinskiHBA,\n        CalcNumLipinskiHBD,\n        CalcNumRings,\n        CalcNumSaturatedCarbocycles,\n        CalcNumSaturatedHeterocycles,\n        CalcNumSaturatedRings,\n        CalcNumSpiroAtoms,\n        CalcNumUnspecifiedAtomStereoCenters,\n        CalcTPSA,\n    ]\n    descriptors_mapping = {f.__name__.strip(\"Calc\"): f for f in descriptors}\n\n    feats = dict()\n    for name, func in descriptors_mapping.items():\n        feats[name] = [func(m) for m in df[mols_column_name]]\n    return pd.DataFrame(feats)\n</code></pre>"},{"location":"api/chemistry/#janitor.chemistry.morgan_fingerprint","title":"<code>morgan_fingerprint(df, mols_column_name, radius=3, nbits=2048, kind='counts')</code>","text":"<p>Convert a column of RDKIT Mol objects into Morgan Fingerprints.</p> <p>Returns a new dataframe without any of the original data. This is intentional, as Morgan fingerprints are usually high-dimensional features.</p> <p>This method does not mutate the original DataFrame.</p> <p>Examples:</p> <p>Functional usage</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor.chemistry\n&gt;&gt;&gt; df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]})\n</code></pre> <ul> <li>For \"counts\" kind</li> </ul> <pre><code>&gt;&gt;&gt; morgans = janitor.chemistry.morgan_fingerprint(\n...     df=df.smiles2mol('smiles', 'mols'),\n...     mols_column_name='mols',\n...     radius=3,      # Defaults to 3\n...     nbits=2048,    # Defaults to 2048\n...     kind='counts'  # Defaults to \"counts\"\n... )\n&gt;&gt;&gt; set(morgans.iloc[0])\n{0.0, 1.0, 2.0}\n</code></pre> <ul> <li>For \"bits\" kind</li> </ul> <pre><code>&gt;&gt;&gt; morgans = janitor.chemistry.morgan_fingerprint(\n...     df=df.smiles2mol('smiles', 'mols'),\n...     mols_column_name='mols',\n...     radius=3,      # Defaults to 3\n...     nbits=2048,    # Defaults to 2048\n...     kind='bits'    # Defaults to \"counts\"\n...  )\n&gt;&gt;&gt; set(morgans.iloc[0])\n{0.0, 1.0}\n</code></pre> <p>Method chaining usage</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor.chemistry\n&gt;&gt;&gt; df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]})\n</code></pre> <ul> <li>For \"counts\" kind</li> </ul> <pre><code>&gt;&gt;&gt; morgans = (\n...     df.smiles2mol('smiles', 'mols')\n...     .morgan_fingerprint(\n...         mols_column_name='mols',\n...         radius=3,      # Defaults to 3\n...         nbits=2048,    # Defaults to 2048\n...         kind='counts'  # Defaults to \"counts\"\n...     )\n... )\n&gt;&gt;&gt; set(morgans.iloc[0])\n{0.0, 1.0, 2.0}\n</code></pre> <ul> <li>For \"bits\" kind</li> </ul> <pre><code>&gt;&gt;&gt; morgans = (\n...     df\n...     .smiles2mol('smiles', 'mols')\n...     .morgan_fingerprint(\n...         mols_column_name='mols',\n...         radius=3,    # Defaults to 3\n...         nbits=2048,  # Defaults to 2048\n...         kind='bits'  # Defaults to \"counts\"\n...     )\n... )\n&gt;&gt;&gt; set(morgans.iloc[0])\n{0.0, 1.0}\n</code></pre> <p>If you wish to join the morgan fingerprints back into the original dataframe, this can be accomplished by doing a <code>join</code>, because the indices are preserved:</p> <pre><code>&gt;&gt;&gt; joined = df.join(morgans)\n&gt;&gt;&gt; len(joined.columns)\n2050\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>mols_column_name</code> <code>str</code> <p>The name of the column that has the RDKIT mol objects</p> required <code>radius</code> <code>int</code> <p>Radius of Morgan fingerprints.</p> <code>3</code> <code>nbits</code> <code>int</code> <p>The length of the fingerprints.</p> <code>2048</code> <code>kind</code> <code>Literal['counts', 'bits']</code> <p>Whether to return counts or bits.</p> <code>'counts'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>kind</code> is not one of <code>\"counts\"</code> or <code>\"bits\"</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A new pandas DataFrame of Morgan fingerprints.</p> Source code in <code>janitor/chemistry.py</code> <pre><code>@pf.register_dataframe_method\n@deprecated_alias(mols_col=\"mols_column_name\")\ndef morgan_fingerprint(\n    df: pd.DataFrame,\n    mols_column_name: str,\n    radius: int = 3,\n    nbits: int = 2048,\n    kind: Literal[\"counts\", \"bits\"] = \"counts\",\n) -&gt; pd.DataFrame:\n    \"\"\"Convert a column of RDKIT Mol objects into Morgan Fingerprints.\n\n    Returns a new dataframe without any of the original data. This is\n    intentional, as Morgan fingerprints are usually high-dimensional\n    features.\n\n    This method does not mutate the original DataFrame.\n\n    Examples:\n        Functional usage\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor.chemistry\n        &gt;&gt;&gt; df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]})\n\n        - For \"counts\" kind\n\n        &gt;&gt;&gt; morgans = janitor.chemistry.morgan_fingerprint(\n        ...     df=df.smiles2mol('smiles', 'mols'),\n        ...     mols_column_name='mols',\n        ...     radius=3,      # Defaults to 3\n        ...     nbits=2048,    # Defaults to 2048\n        ...     kind='counts'  # Defaults to \"counts\"\n        ... )\n        &gt;&gt;&gt; set(morgans.iloc[0])\n        {0.0, 1.0, 2.0}\n\n        - For \"bits\" kind\n\n        &gt;&gt;&gt; morgans = janitor.chemistry.morgan_fingerprint(\n        ...     df=df.smiles2mol('smiles', 'mols'),\n        ...     mols_column_name='mols',\n        ...     radius=3,      # Defaults to 3\n        ...     nbits=2048,    # Defaults to 2048\n        ...     kind='bits'    # Defaults to \"counts\"\n        ...  )\n        &gt;&gt;&gt; set(morgans.iloc[0])\n        {0.0, 1.0}\n\n        Method chaining usage\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor.chemistry\n        &gt;&gt;&gt; df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]})\n\n        - For \"counts\" kind\n\n        &gt;&gt;&gt; morgans = (\n        ...     df.smiles2mol('smiles', 'mols')\n        ...     .morgan_fingerprint(\n        ...         mols_column_name='mols',\n        ...         radius=3,      # Defaults to 3\n        ...         nbits=2048,    # Defaults to 2048\n        ...         kind='counts'  # Defaults to \"counts\"\n        ...     )\n        ... )\n        &gt;&gt;&gt; set(morgans.iloc[0])\n        {0.0, 1.0, 2.0}\n\n        - For \"bits\" kind\n\n        &gt;&gt;&gt; morgans = (\n        ...     df\n        ...     .smiles2mol('smiles', 'mols')\n        ...     .morgan_fingerprint(\n        ...         mols_column_name='mols',\n        ...         radius=3,    # Defaults to 3\n        ...         nbits=2048,  # Defaults to 2048\n        ...         kind='bits'  # Defaults to \"counts\"\n        ...     )\n        ... )\n        &gt;&gt;&gt; set(morgans.iloc[0])\n        {0.0, 1.0}\n\n        If you wish to join the morgan fingerprints back into the original\n        dataframe, this can be accomplished by doing a `join`,\n        because the indices are preserved:\n\n        &gt;&gt;&gt; joined = df.join(morgans)\n        &gt;&gt;&gt; len(joined.columns)\n        2050\n\n    Args:\n        df: A pandas DataFrame.\n        mols_column_name: The name of the column that has the RDKIT\n            mol objects\n        radius: Radius of Morgan fingerprints.\n        nbits: The length of the fingerprints.\n        kind: Whether to return counts or bits.\n\n    Raises:\n        ValueError: If `kind` is not one of `\"counts\"` or `\"bits\"`.\n\n    Returns:\n        A new pandas DataFrame of Morgan fingerprints.\n    \"\"\"\n    acceptable_kinds = [\"counts\", \"bits\"]\n    if kind not in acceptable_kinds:\n        raise ValueError(f\"`kind` must be one of {acceptable_kinds}\")\n\n    if kind == \"bits\":\n        fps = [\n            GetMorganFingerprintAsBitVect(m, radius, nbits, useChirality=True)\n            for m in df[mols_column_name]\n        ]\n    elif kind == \"counts\":\n        fps = [\n            GetHashedMorganFingerprint(m, radius, nbits, useChirality=True)\n            for m in df[mols_column_name]\n        ]\n\n    np_fps = []\n    for fp in fps:\n        arr = np.zeros((1,))\n        DataStructs.ConvertToNumpyArray(fp, arr)\n        np_fps.append(arr)\n    np_fps = np.vstack(np_fps)\n    fpdf = pd.DataFrame(np_fps)\n    fpdf.index = df.index\n    return fpdf\n</code></pre>"},{"location":"api/chemistry/#janitor.chemistry.smiles2mol","title":"<code>smiles2mol(df, smiles_column_name, mols_column_name, drop_nulls=True, progressbar=None)</code>","text":"<p>Convert a column of SMILES strings into RDKit Mol objects.</p> <p>Automatically drops invalid SMILES, as determined by RDKIT.</p> <p>This method mutates the original DataFrame.</p> <p>Examples:</p> <p>Functional usage</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor.chemistry\n&gt;&gt;&gt; df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]})\n&gt;&gt;&gt; df = janitor.chemistry.smiles2mol(\n...    df=df,\n...    smiles_column_name='smiles',\n...    mols_column_name='mols'\n... )\n&gt;&gt;&gt; df.mols[0].GetNumAtoms(), df.mols[0].GetNumBonds()\n(3, 2)\n&gt;&gt;&gt; df.mols[1].GetNumAtoms(), df.mols[1].GetNumBonds()\n(5, 4)\n</code></pre> <p>Method chaining usage</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor.chemistry\n&gt;&gt;&gt; df = df.smiles2mol(\n...     smiles_column_name='smiles',\n...     mols_column_name='rdkmol'\n... )\n&gt;&gt;&gt; df.rdkmol[0].GetNumAtoms(), df.rdkmol[0].GetNumBonds()\n(3, 2)\n</code></pre> <p>A progressbar can be optionally used.</p> <ul> <li>Pass in \"notebook\" to show a <code>tqdm</code> notebook progressbar.   (<code>ipywidgets</code> must be enabled with your Jupyter installation.)</li> <li>Pass in \"terminal\" to show a <code>tqdm</code> progressbar. Better suited for use   with scripts.</li> <li><code>None</code> is the default value - progress bar will not be shown.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>pandas DataFrame.</p> required <code>smiles_column_name</code> <code>Hashable</code> <p>Name of column that holds the SMILES strings.</p> required <code>mols_column_name</code> <code>Hashable</code> <p>Name to be given to the new mols column.</p> required <code>drop_nulls</code> <code>bool</code> <p>Whether to drop rows whose mols failed to be constructed.</p> <code>True</code> <code>progressbar</code> <code>Optional[str]</code> <p>Whether to show a progressbar or not.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>progressbar</code> is not one of <code>\"notebook\"</code>, <code>\"terminal\"</code>, or <code>None</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with new RDKIT Mol objects column.</p> Source code in <code>janitor/chemistry.py</code> <pre><code>@pf.register_dataframe_method\n@deprecated_alias(smiles_col=\"smiles_column_name\", mols_col=\"mols_column_name\")\ndef smiles2mol(\n    df: pd.DataFrame,\n    smiles_column_name: Hashable,\n    mols_column_name: Hashable,\n    drop_nulls: bool = True,\n    progressbar: Optional[str] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Convert a column of SMILES strings into RDKit Mol objects.\n\n    Automatically drops invalid SMILES, as determined by RDKIT.\n\n    This method mutates the original DataFrame.\n\n    Examples:\n        Functional usage\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor.chemistry\n        &gt;&gt;&gt; df = pd.DataFrame({\"smiles\": [\"O=C=O\", \"CCC(=O)O\"]})\n        &gt;&gt;&gt; df = janitor.chemistry.smiles2mol(\n        ...    df=df,\n        ...    smiles_column_name='smiles',\n        ...    mols_column_name='mols'\n        ... )\n        &gt;&gt;&gt; df.mols[0].GetNumAtoms(), df.mols[0].GetNumBonds()\n        (3, 2)\n        &gt;&gt;&gt; df.mols[1].GetNumAtoms(), df.mols[1].GetNumBonds()\n        (5, 4)\n\n        Method chaining usage\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor.chemistry\n        &gt;&gt;&gt; df = df.smiles2mol(\n        ...     smiles_column_name='smiles',\n        ...     mols_column_name='rdkmol'\n        ... )\n        &gt;&gt;&gt; df.rdkmol[0].GetNumAtoms(), df.rdkmol[0].GetNumBonds()\n        (3, 2)\n\n    A progressbar can be optionally used.\n\n    - Pass in \"notebook\" to show a `tqdm` notebook progressbar.\n      (`ipywidgets` must be enabled with your Jupyter installation.)\n    - Pass in \"terminal\" to show a `tqdm` progressbar. Better suited for use\n      with scripts.\n    - `None` is the default value - progress bar will not be shown.\n\n    Args:\n        df: pandas DataFrame.\n        smiles_column_name: Name of column that holds the SMILES strings.\n        mols_column_name: Name to be given to the new mols column.\n        drop_nulls: Whether to drop rows whose mols failed to be\n            constructed.\n        progressbar: Whether to show a progressbar or not.\n\n    Raises:\n        ValueError: If `progressbar` is not one of\n            `\"notebook\"`, `\"terminal\"`, or `None`.\n\n    Returns:\n        A pandas DataFrame with new RDKIT Mol objects column.\n    \"\"\"\n    valid_progress = [\"notebook\", \"terminal\", None]\n    if progressbar not in valid_progress:\n        raise ValueError(f\"progressbar kwarg must be one of {valid_progress}\")\n\n    if progressbar is None:\n        df[mols_column_name] = df[smiles_column_name].apply(\n            lambda x: Chem.MolFromSmiles(x)\n        )\n    else:\n        if progressbar == \"notebook\":\n            tqdmn().pandas(desc=\"mols\")\n        elif progressbar == \"terminal\":\n            tqdm.pandas(desc=\"mols\")\n        df[mols_column_name] = df[smiles_column_name].progress_apply(\n            lambda x: Chem.MolFromSmiles(x)\n        )\n\n    if drop_nulls:\n        df = df.dropna(subset=[mols_column_name])\n    df = df.reset_index(drop=True)\n    return df\n</code></pre>"},{"location":"api/engineering/","title":"Engineering","text":"<p>Engineering-specific data cleaning functions.</p>"},{"location":"api/engineering/#janitor.engineering.convert_units","title":"<code>convert_units(df, column_name=None, existing_units=None, to_units=None, dest_column_name=None)</code>","text":"<p>Converts a column of numeric values from one unit to another.</p> <p>Unit conversion can only take place if the <code>existing_units</code> and <code>to_units</code> are of the same type (e.g., temperature or pressure). The provided unit types can be any unit name or alternate name provided in the <code>unyt</code> package's Listing of Units table.</p> <p>Volume units are not provided natively in <code>unyt</code>.  However, exponents are supported, and therefore some volume units can be converted.  For example, a volume in cubic centimeters can be converted to cubic meters using <code>existing_units='cm**3'</code> and <code>to_units='m**3'</code>.</p> <p>This method mutates the original DataFrame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor.engineering\n&gt;&gt;&gt; df = pd.DataFrame({\"temp_F\": [-40, 112]})\n&gt;&gt;&gt; df = df.convert_units(\n...     column_name='temp_F',\n...     existing_units='degF',\n...     to_units='degC',\n...     dest_column_name='temp_C'\n... )\n&gt;&gt;&gt; df\n   temp_F     temp_C\n0     -40 -40.000000\n1     112  44.444444\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>column_name</code> <code>str</code> <p>Name of the column containing numeric values that are to be converted from one set of units to another.</p> <code>None</code> <code>existing_units</code> <code>str</code> <p>The unit type to convert from.</p> <code>None</code> <code>to_units</code> <code>str</code> <p>The unit type to convert to.</p> <code>None</code> <code>dest_column_name</code> <code>str</code> <p>The name of the new column containing the converted values that will be created.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If column is not numeric.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with a new column of unit-converted values.</p> Source code in <code>janitor/engineering.py</code> <pre><code>@pf.register_dataframe_method\ndef convert_units(\n    df: pd.DataFrame,\n    column_name: str = None,\n    existing_units: str = None,\n    to_units: str = None,\n    dest_column_name: str = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Converts a column of numeric values from one unit to another.\n\n    Unit conversion can only take place if the `existing_units` and\n    `to_units` are of the same type (e.g., temperature or pressure).\n    The provided unit types can be any unit name or alternate name provided\n    in the `unyt` package's [Listing of Units table](\n    https://unyt.readthedocs.io/en/stable/unit_listing.html#unit-listing).\n\n    Volume units are not provided natively in `unyt`.  However, exponents are\n    supported, and therefore some volume units can be converted.  For example,\n    a volume in cubic centimeters can be converted to cubic meters using\n    `existing_units='cm**3'` and `to_units='m**3'`.\n\n    This method mutates the original DataFrame.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor.engineering\n        &gt;&gt;&gt; df = pd.DataFrame({\"temp_F\": [-40, 112]})\n        &gt;&gt;&gt; df = df.convert_units(\n        ...     column_name='temp_F',\n        ...     existing_units='degF',\n        ...     to_units='degC',\n        ...     dest_column_name='temp_C'\n        ... )\n        &gt;&gt;&gt; df\n           temp_F     temp_C\n        0     -40 -40.000000\n        1     112  44.444444\n\n    Args:\n        df: A pandas DataFrame.\n        column_name: Name of the column containing numeric\n            values that are to be converted from one set of units to another.\n        existing_units: The unit type to convert from.\n        to_units: The unit type to convert to.\n        dest_column_name: The name of the new column containing the\n            converted values that will be created.\n\n    Raises:\n        TypeError: If column is not numeric.\n\n    Returns:\n        A pandas DataFrame with a new column of unit-converted values.\n    \"\"\"\n\n    # Check all inputs are correct data type\n    check(\"column_name\", column_name, [str])\n    check(\"existing_units\", existing_units, [str])\n    check(\"to_units\", to_units, [str])\n    check(\"dest_column_name\", dest_column_name, [str])\n\n    # Check that column_name is a numeric column\n    if not np.issubdtype(df[column_name].dtype, np.number):\n        raise TypeError(f\"{column_name} must be a numeric column.\")\n\n    original_vals = df[column_name].to_numpy() * unyt.Unit(existing_units)\n    converted_vals = original_vals.to(to_units)\n    df[dest_column_name] = np.array(converted_vals)\n\n    return df\n</code></pre>"},{"location":"api/finance/","title":"Finance","text":"<p>Finance-specific data cleaning functions.</p>"},{"location":"api/finance/#janitor.finance.convert_currency","title":"<code>convert_currency(df, api_key, column_name=None, from_currency=None, to_currency=None, historical_date=None, make_new_column=False)</code>","text":"<p>Deprecated function.</p> Source code in <code>janitor/finance.py</code> <pre><code>@pf.register_dataframe_method\n@deprecated_alias(colname=\"column_name\")\ndef convert_currency(\n    df: pd.DataFrame,\n    api_key: str,\n    column_name: str = None,\n    from_currency: str = None,\n    to_currency: str = None,\n    historical_date: date = None,\n    make_new_column: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Deprecated function.\n\n    &lt;!--\n    # noqa: DAR101\n    # noqa: DAR401\n    --&gt;\n    \"\"\"\n    raise JanitorError(\n        \"The `convert_currency` function has been temporarily disabled due to \"\n        \"exchangeratesapi.io disallowing free pinging of its API. \"\n        \"(Our tests started to fail due to this issue.) \"\n        \"There is no easy way around this problem \"\n        \"except to find a new API to call on.\"\n        \"Please comment on issue #829 \"\n        \"(https://github.com/pyjanitor-devs/pyjanitor/issues/829) \"\n        \"if you know of an alternative API that we can call on, \"\n        \"otherwise the function will be removed in pyjanitor's 1.0 release.\"\n    )\n</code></pre>"},{"location":"api/finance/#janitor.finance.convert_stock","title":"<code>convert_stock(stock_symbol)</code>","text":"<p>This function takes in a stock symbol as a parameter, queries an API for the companies full name and returns it</p> <p>Examples:</p> <pre><code>```python\nimport janitor.finance\njanitor.finance.convert_stock(\"aapl\")\n```\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>stock_symbol</code> <code>str</code> <p>Stock ticker Symbol</p> required <p>Raises:</p> Type Description <code>ConnectionError</code> <p>Internet connection is not available</p> <p>Returns:</p> Type Description <code>str</code> <p>Full company name</p> Source code in <code>janitor/finance.py</code> <pre><code>def convert_stock(stock_symbol: str) -&gt; str:\n    \"\"\"\n    This function takes in a stock symbol as a parameter,\n    queries an API for the companies full name and returns\n    it\n\n    Examples:\n\n        ```python\n        import janitor.finance\n        janitor.finance.convert_stock(\"aapl\")\n        ```\n\n    Args:\n        stock_symbol: Stock ticker Symbol\n\n    Raises:\n        ConnectionError: Internet connection is not available\n\n    Returns:\n        Full company name\n    \"\"\"\n    if is_connected(\"www.google.com\"):\n        stock_symbol = stock_symbol.upper()\n        return get_symbol(stock_symbol)\n    else:\n        raise ConnectionError(\n            \"Connection Error: Client Not Connected to Internet\"\n        )\n</code></pre>"},{"location":"api/finance/#janitor.finance.get_symbol","title":"<code>get_symbol(symbol)</code>","text":"<p>This is a helper function to get a companies full name based on the stock symbol.</p> <p>Examples:</p> <pre><code>```python\nimport janitor.finance\njanitor.finance.get_symbol(\"aapl\")\n```\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>symbol</code> <code>str</code> <p>This is our stock symbol that we use to query the api for the companies full name.</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Company full name</p> Source code in <code>janitor/finance.py</code> <pre><code>def get_symbol(symbol: str) -&gt; Optional[str]:\n    \"\"\"\n    This is a helper function to get a companies full\n    name based on the stock symbol.\n\n    Examples:\n\n        ```python\n        import janitor.finance\n        janitor.finance.get_symbol(\"aapl\")\n        ```\n\n    Args:\n        symbol: This is our stock symbol that we use\n            to query the api for the companies full name.\n\n    Returns:\n        Company full name\n    \"\"\"\n    result = requests.get(\n        \"http://d.yimg.com/autoc.\"\n        + \"finance.yahoo.com/autoc?query={}&amp;region=1&amp;lang=en\".format(symbol)\n    ).json()\n\n    for x in result[\"ResultSet\"][\"Result\"]:\n        if x[\"symbol\"] == symbol:\n            return x[\"name\"]\n        else:\n            return None\n</code></pre>"},{"location":"api/finance/#janitor.finance.inflate_currency","title":"<code>inflate_currency(df, column_name=None, country=None, currency_year=None, to_year=None, make_new_column=False)</code>","text":"<p>Inflates a column of monetary values from one year to another, based on the currency's country.</p> <p>The provided country can be any economy name or code from the World Bank list of economies.</p> <p>Note: This method mutates the original DataFrame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor.finance\n&gt;&gt;&gt; df = pd.DataFrame({\"profit\":[100.10, 200.20, 300.30, 400.40, 500.50]})\n&gt;&gt;&gt; df\n   profit\n0   100.1\n1   200.2\n2   300.3\n3   400.4\n4   500.5\n&gt;&gt;&gt; df.inflate_currency(\n...    column_name='profit',\n...    country='USA',\n...    currency_year=2015,\n...    to_year=2018,\n...    make_new_column=True\n... )\n   profit  profit_2018\n0   100.1   106.050596\n1   200.2   212.101191\n2   300.3   318.151787\n3   400.4   424.202382\n4   500.5   530.252978\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>column_name</code> <code>str</code> <p>Name of the column containing monetary values to inflate.</p> <code>None</code> <code>country</code> <code>str</code> <p>The country associated with the currency being inflated. May be any economy or code from the World Bank List of economies.</p> <code>None</code> <code>currency_year</code> <code>int</code> <p>The currency year to inflate from. The year should be 1960 or later.</p> <code>None</code> <code>to_year</code> <code>int</code> <p>The currency year to inflate to. The year should be 1960 or later.</p> <code>None</code> <code>make_new_column</code> <code>bool</code> <p>Generates new column for inflated currency if True, otherwise, inflates currency in place.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The DataFrame with inflated currency column.</p> Source code in <code>janitor/finance.py</code> <pre><code>@pf.register_dataframe_method\ndef inflate_currency(\n    df: pd.DataFrame,\n    column_name: str = None,\n    country: str = None,\n    currency_year: int = None,\n    to_year: int = None,\n    make_new_column: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Inflates a column of monetary values from one year to another, based on\n    the currency's country.\n\n    The provided country can be any economy name or code from the World Bank\n    [list of economies](https://databank.worldbank.org/data/download/site-content/CLASS.xls).\n\n    **Note**: This method mutates the original DataFrame.\n\n    Examples:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor.finance\n        &gt;&gt;&gt; df = pd.DataFrame({\"profit\":[100.10, 200.20, 300.30, 400.40, 500.50]})\n        &gt;&gt;&gt; df\n           profit\n        0   100.1\n        1   200.2\n        2   300.3\n        3   400.4\n        4   500.5\n        &gt;&gt;&gt; df.inflate_currency(\n        ...    column_name='profit',\n        ...    country='USA',\n        ...    currency_year=2015,\n        ...    to_year=2018,\n        ...    make_new_column=True\n        ... )\n           profit  profit_2018\n        0   100.1   106.050596\n        1   200.2   212.101191\n        2   300.3   318.151787\n        3   400.4   424.202382\n        4   500.5   530.252978\n\n    Args:\n        df: A pandas DataFrame.\n        column_name: Name of the column containing monetary\n            values to inflate.\n        country: The country associated with the currency being inflated.\n            May be any economy or code from the World Bank\n            [List of economies](https://databank.worldbank.org/data/download/site-content/CLASS.xls).\n        currency_year: The currency year to inflate from.\n            The year should be 1960 or later.\n        to_year: The currency year to inflate to.\n            The year should be 1960 or later.\n        make_new_column: Generates new column for inflated currency if\n            True, otherwise, inflates currency in place.\n\n    Returns:\n        The DataFrame with inflated currency column.\n    \"\"\"  # noqa: E501\n\n    inflator = _inflate_currency(country, currency_year, to_year)\n\n    if make_new_column:\n        new_column_name = column_name + \"_\" + str(to_year)\n        df[new_column_name] = df[column_name] * inflator\n\n    else:\n        df[column_name] = df[column_name] * inflator\n\n    return df\n</code></pre>"},{"location":"api/functions/","title":"Functions","text":""},{"location":"api/functions/#janitor.functions--general-functions","title":"General Functions","text":"<p>pyjanitor's general-purpose data cleaning functions.</p>"},{"location":"api/functions/#janitor.functions.add_columns","title":"<code>add_columns</code>","text":""},{"location":"api/functions/#janitor.functions.add_columns.add_column","title":"<code>add_column(df, column_name, value, fill_remaining=False)</code>","text":"<p>Add a column to the dataframe.</p> <p>Intended to be the method-chaining alternative to:</p> <pre><code>df[column_name] = value\n</code></pre> <p>Note</p> <p>This function will be deprecated in a 1.x release. Please use <code>pd.DataFrame.assign</code> instead.</p> <p>Examples:</p> <p>Add a column of constant values to the dataframe.</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"abc\")})\n&gt;&gt;&gt; df.add_column(column_name=\"c\", value=1)\n   a  b  c\n0  0  a  1\n1  1  b  1\n2  2  c  1\n</code></pre> <p>Add a column of different values to the dataframe.</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"abc\")})\n&gt;&gt;&gt; df.add_column(column_name=\"c\", value=list(\"efg\"))\n   a  b  c\n0  0  a  e\n1  1  b  f\n2  2  c  g\n</code></pre> <p>Add a column using an iterator.</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"abc\")})\n&gt;&gt;&gt; df.add_column(column_name=\"c\", value=range(4, 7))\n   a  b  c\n0  0  a  4\n1  1  b  5\n2  2  c  6\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>column_name</code> <code>str</code> <p>Name of the new column. Should be a string, in order for the column name to be compatible with the Feather binary format (this is a useful thing to have).</p> required <code>value</code> <code>Union[List[Any], Tuple[Any], Any]</code> <p>Either a single value, or a list/tuple of values.</p> required <code>fill_remaining</code> <code>bool</code> <p>If value is a tuple or list that is smaller than the number of rows in the DataFrame, repeat the list or tuple (R-style) to the end of the DataFrame.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If attempting to add a column that already exists.</p> <code>ValueError</code> <p>If <code>value</code> has more elements that number of rows in the DataFrame.</p> <code>ValueError</code> <p>If attempting to add an iterable of values with a length not equal to the number of DataFrame rows.</p> <code>ValueError</code> <p>If <code>value</code> has length of <code>0</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with an added column.</p> Source code in <code>janitor/functions/add_columns.py</code> <pre><code>@pf.register_dataframe_method\n@refactored_function(\n    message=(\n        \"This function will be deprecated in a 1.x release. \"\n        \"Please use `pd.DataFrame.assign` instead.\"\n    )\n)\n@deprecated_alias(col_name=\"column_name\")\ndef add_column(\n    df: pd.DataFrame,\n    column_name: str,\n    value: Union[List[Any], Tuple[Any], Any],\n    fill_remaining: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Add a column to the dataframe.\n\n    Intended to be the method-chaining alternative to:\n\n    ```python\n    df[column_name] = value\n    ```\n\n    !!!note\n\n        This function will be deprecated in a 1.x release.\n        Please use `pd.DataFrame.assign` instead.\n\n    Examples:\n        Add a column of constant values to the dataframe.\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"abc\")})\n        &gt;&gt;&gt; df.add_column(column_name=\"c\", value=1)\n           a  b  c\n        0  0  a  1\n        1  1  b  1\n        2  2  c  1\n\n        Add a column of different values to the dataframe.\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"abc\")})\n        &gt;&gt;&gt; df.add_column(column_name=\"c\", value=list(\"efg\"))\n           a  b  c\n        0  0  a  e\n        1  1  b  f\n        2  2  c  g\n\n        Add a column using an iterator.\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"abc\")})\n        &gt;&gt;&gt; df.add_column(column_name=\"c\", value=range(4, 7))\n           a  b  c\n        0  0  a  4\n        1  1  b  5\n        2  2  c  6\n\n    Args:\n        df: A pandas DataFrame.\n        column_name: Name of the new column. Should be a string, in order\n            for the column name to be compatible with the Feather binary\n            format (this is a useful thing to have).\n        value: Either a single value, or a list/tuple of values.\n        fill_remaining: If value is a tuple or list that is smaller than\n            the number of rows in the DataFrame, repeat the list or tuple\n            (R-style) to the end of the DataFrame.\n\n    Raises:\n        ValueError: If attempting to add a column that already exists.\n        ValueError: If `value` has more elements that number of\n            rows in the DataFrame.\n        ValueError: If attempting to add an iterable of values with\n            a length not equal to the number of DataFrame rows.\n        ValueError: If `value` has length of `0`.\n\n    Returns:\n        A pandas DataFrame with an added column.\n    \"\"\"\n    check(\"column_name\", column_name, [str])\n\n    if column_name in df.columns:\n        raise ValueError(\n            f\"Attempted to add column that already exists: \" f\"{column_name}.\"\n        )\n\n    nrows = len(df)\n\n    if hasattr(value, \"__len__\") and not isinstance(\n        value, (str, bytes, bytearray)\n    ):\n        len_value = len(value)\n\n        # if `value` is a list, ndarray, etc.\n        if len_value &gt; nrows:\n            raise ValueError(\n                \"`value` has more elements than number of rows \"\n                f\"in your `DataFrame`. vals: {len_value}, \"\n                f\"df: {nrows}\"\n            )\n        if len_value != nrows and not fill_remaining:\n            raise ValueError(\n                \"Attempted to add iterable of values with length\"\n                \" not equal to number of DataFrame rows\"\n            )\n        if not len_value:\n            raise ValueError(\n                \"`value` has to be an iterable of minimum length 1\"\n            )\n\n    elif fill_remaining:\n        # relevant if a scalar val was passed, yet fill_remaining == True\n        len_value = 1\n        value = [value]\n\n    df = df.copy()\n    if fill_remaining:\n        times_to_loop = int(np.ceil(nrows / len_value))\n        fill_values = list(value) * times_to_loop\n        df[column_name] = fill_values[:nrows]\n    else:\n        df[column_name] = value\n\n    return df\n</code></pre>"},{"location":"api/functions/#janitor.functions.add_columns.add_columns","title":"<code>add_columns(df, fill_remaining=False, **kwargs)</code>","text":"<p>Add multiple columns to the dataframe.</p> <p>This method does not mutate the original DataFrame.</p> <p>Method to augment <code>add_column</code> with ability to add multiple columns in one go. This replaces the need for multiple <code>add_column</code> calls.</p> <p>Usage is through supplying kwargs where the key is the col name and the values correspond to the values of the new DataFrame column.</p> <p>Values passed can be scalar or iterable (list, ndarray, etc.)</p> <p>Note</p> <p>This function will be deprecated in a 1.x release. Please use <code>pd.DataFrame.assign</code> instead.</p> <p>Examples:</p> <p>Inserting two more columns into a dataframe.</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"abc\")})\n&gt;&gt;&gt; df.add_columns(x=4, y=list(\"def\"))\n   a  b  x  y\n0  0  a  4  d\n1  1  b  4  e\n2  2  c  4  f\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>fill_remaining</code> <code>bool</code> <p>If value is a tuple or list that is smaller than the number of rows in the DataFrame, repeat the list or tuple (R-style) to the end of the DataFrame. (Passed to <code>add_column</code>)</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Column, value pairs which are looped through in <code>add_column</code> calls.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with added columns.</p> Source code in <code>janitor/functions/add_columns.py</code> <pre><code>@pf.register_dataframe_method\n@refactored_function(\n    message=(\n        \"This function will be deprecated in a 1.x release. \"\n        \"Please use `pd.DataFrame.assign` instead.\"\n    )\n)\ndef add_columns(\n    df: pd.DataFrame,\n    fill_remaining: bool = False,\n    **kwargs: Any,\n) -&gt; pd.DataFrame:\n    \"\"\"Add multiple columns to the dataframe.\n\n    This method does not mutate the original DataFrame.\n\n    Method to augment\n    [`add_column`][janitor.functions.add_columns.add_column]\n    with ability to add multiple columns in\n    one go. This replaces the need for multiple\n    [`add_column`][janitor.functions.add_columns.add_column] calls.\n\n    Usage is through supplying kwargs where the key is the col name and the\n    values correspond to the values of the new DataFrame column.\n\n    Values passed can be scalar or iterable (list, ndarray, etc.)\n\n    !!!note\n\n        This function will be deprecated in a 1.x release.\n        Please use `pd.DataFrame.assign` instead.\n\n    Examples:\n        Inserting two more columns into a dataframe.\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"abc\")})\n        &gt;&gt;&gt; df.add_columns(x=4, y=list(\"def\"))\n           a  b  x  y\n        0  0  a  4  d\n        1  1  b  4  e\n        2  2  c  4  f\n\n    Args:\n        df: A pandas DataFrame.\n        fill_remaining: If value is a tuple or list that is smaller than\n            the number of rows in the DataFrame, repeat the list or tuple\n            (R-style) to the end of the DataFrame. (Passed to\n            [`add_column`][janitor.functions.add_columns.add_column])\n        **kwargs: Column, value pairs which are looped through in\n            [`add_column`][janitor.functions.add_columns.add_column] calls.\n\n    Returns:\n        A pandas DataFrame with added columns.\n    \"\"\"\n    # Note: error checking can pretty much be handled in `add_column`\n\n    for col_name, values in kwargs.items():\n        df = df.add_column(col_name, values, fill_remaining=fill_remaining)\n\n    return df\n</code></pre>"},{"location":"api/functions/#janitor.functions.also","title":"<code>also</code>","text":"<p>Implementation source for chainable function <code>also</code>.</p>"},{"location":"api/functions/#janitor.functions.also.also","title":"<code>also(df, func, *args, **kwargs)</code>","text":"<p>Run a function with side effects.</p> <p>This function allows you to run an arbitrary function in the <code>pyjanitor</code> method chain. Doing so will let you do things like save the dataframe to disk midway while continuing to modify the dataframe afterwards.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = (\n...     pd.DataFrame({\"a\": [1, 2, 3], \"b\": list(\"abc\")})\n...     .query(\"a &gt; 1\")\n...     .also(lambda df: print(f\"DataFrame shape is: {df.shape}\"))\n...     .rename_column(old_column_name=\"a\", new_column_name=\"a_new\")\n...     .also(lambda df: df.to_csv(\"midpoint.csv\"))\n...     .also(\n...         lambda df: print(f\"Columns: {df.columns}\")\n...     )\n... )\nDataFrame shape is: (2, 2)\nColumns: Index(['a_new', 'b'], dtype='object')\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>func</code> <code>Callable</code> <p>A function you would like to run in the method chain. It should take one DataFrame object as a parameter and have no return. If there is a return, it will be ignored.</p> required <code>*args</code> <code>Any</code> <p>Optional arguments for <code>func</code>.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Optional keyword arguments for <code>func</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The input pandas DataFrame, unmodified.</p> Source code in <code>janitor/functions/also.py</code> <pre><code>@pf.register_dataframe_method\ndef also(\n    df: pd.DataFrame, func: Callable, *args: Any, **kwargs: Any\n) -&gt; pd.DataFrame:\n    \"\"\"Run a function with side effects.\n\n    This function allows you to run an arbitrary function\n    in the `pyjanitor` method chain.\n    Doing so will let you do things like save the dataframe to disk midway\n    while continuing to modify the dataframe afterwards.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = (\n        ...     pd.DataFrame({\"a\": [1, 2, 3], \"b\": list(\"abc\")})\n        ...     .query(\"a &gt; 1\")\n        ...     .also(lambda df: print(f\"DataFrame shape is: {df.shape}\"))\n        ...     .rename_column(old_column_name=\"a\", new_column_name=\"a_new\")\n        ...     .also(lambda df: df.to_csv(\"midpoint.csv\"))\n        ...     .also(\n        ...         lambda df: print(f\"Columns: {df.columns}\")\n        ...     )\n        ... )\n        DataFrame shape is: (2, 2)\n        Columns: Index(['a_new', 'b'], dtype='object')\n\n    Args:\n        df: A pandas DataFrame.\n        func: A function you would like to run in the method chain.\n            It should take one DataFrame object as a parameter and have no return.\n            If there is a return, it will be ignored.\n        *args: Optional arguments for `func`.\n        **kwargs: Optional keyword arguments for `func`.\n\n    Returns:\n        The input pandas DataFrame, unmodified.\n    \"\"\"  # noqa: E501\n    func(df.copy(), *args, **kwargs)\n    return df\n</code></pre>"},{"location":"api/functions/#janitor.functions.bin_numeric","title":"<code>bin_numeric</code>","text":"<p>Implementation source for <code>bin_numeric</code>.</p>"},{"location":"api/functions/#janitor.functions.bin_numeric.bin_numeric","title":"<code>bin_numeric(df, from_column_name, to_column_name, bins=5, **kwargs)</code>","text":"<p>Generate a new column that labels bins for a specified numeric column.</p> <p>This method does not mutate the original DataFrame.</p> <p>A wrapper around the pandas <code>cut()</code> function to bin data of one column, generating a new column with the results.</p> <p>Examples:</p> <p>Binning a numeric column with specific bin edges.</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\"a\": [3, 6, 9, 12, 15]})\n&gt;&gt;&gt; df.bin_numeric(\n...     from_column_name=\"a\", to_column_name=\"a_binned\",\n...     bins=[0, 5, 11, 15],\n... )\n    a  a_binned\n0   3    (0, 5]\n1   6   (5, 11]\n2   9   (5, 11]\n3  12  (11, 15]\n4  15  (11, 15]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>from_column_name</code> <code>str</code> <p>The column whose data you want binned.</p> required <code>to_column_name</code> <code>str</code> <p>The new column to be created with the binned data.</p> required <code>bins</code> <code>Optional[Union[int, ScalarSequence, IntervalIndex]]</code> <p>The binning strategy to be utilized. Read the <code>pd.cut</code> documentation for more details.</p> <code>5</code> <code>**kwargs</code> <code>Any</code> <p>Additional kwargs to pass to <code>pd.cut</code>, except <code>retbins</code>.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>retbins</code> is passed in as a kwarg.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame.</p> Source code in <code>janitor/functions/bin_numeric.py</code> <pre><code>@pf.register_dataframe_method\n@deprecated_alias(\n    from_column=\"from_column_name\",\n    to_column=\"to_column_name\",\n    num_bins=\"bins\",\n)\ndef bin_numeric(\n    df: pd.DataFrame,\n    from_column_name: str,\n    to_column_name: str,\n    bins: Optional[Union[int, ScalarSequence, pd.IntervalIndex]] = 5,\n    **kwargs: Any,\n) -&gt; pd.DataFrame:\n    \"\"\"Generate a new column that labels bins for a specified numeric column.\n\n    This method does not mutate the original DataFrame.\n\n    A wrapper around the pandas [`cut()`][pd_cut_docs] function to bin data of\n    one column, generating a new column with the results.\n\n    [pd_cut_docs]: https://pandas.pydata.org/docs/reference/api/pandas.cut.html\n\n    Examples:\n        Binning a numeric column with specific bin edges.\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\"a\": [3, 6, 9, 12, 15]})\n        &gt;&gt;&gt; df.bin_numeric(\n        ...     from_column_name=\"a\", to_column_name=\"a_binned\",\n        ...     bins=[0, 5, 11, 15],\n        ... )\n            a  a_binned\n        0   3    (0, 5]\n        1   6   (5, 11]\n        2   9   (5, 11]\n        3  12  (11, 15]\n        4  15  (11, 15]\n\n    Args:\n        df: A pandas DataFrame.\n        from_column_name: The column whose data you want binned.\n        to_column_name: The new column to be created with the binned data.\n        bins: The binning strategy to be utilized. Read the `pd.cut`\n            documentation for more details.\n        **kwargs: Additional kwargs to pass to `pd.cut`, except `retbins`.\n\n    Raises:\n        ValueError: If `retbins` is passed in as a kwarg.\n\n    Returns:\n        A pandas DataFrame.\n    \"\"\"\n    if \"retbins\" in kwargs:\n        raise ValueError(\"`retbins` is not an acceptable keyword argument.\")\n\n    check(\"from_column_name\", from_column_name, [str])\n    check(\"to_column_name\", to_column_name, [str])\n    check_column(df, from_column_name)\n\n    df = df.assign(\n        **{\n            to_column_name: pd.cut(df[from_column_name], bins=bins, **kwargs),\n        }\n    )\n\n    return df\n</code></pre>"},{"location":"api/functions/#janitor.functions.case_when","title":"<code>case_when</code>","text":"<p>Implementation source for <code>case_when</code>.</p>"},{"location":"api/functions/#janitor.functions.case_when.case_when","title":"<code>case_when(df, *args, default=None, column_name)</code>","text":"<p>Create a column based on a condition or multiple conditions.</p> <p>Similar to SQL and dplyr's case_when with inspiration from <code>pydatatable</code> if_else function.</p> <p>If your scenario requires direct replacement of values, pandas' <code>replace</code> method or <code>map</code> method should be better suited and more efficient; if the conditions check if a value is within a range of values, pandas' <code>cut</code> or <code>qcut</code> should be more efficient; <code>np.where/np.select</code> are also performant options.</p> <p>This function relies on <code>pd.Series.mask</code> method.</p> <p>When multiple conditions are satisfied, the first one is used.</p> <p>The variable <code>*args</code> parameters takes arguments of the form : <code>condition0</code>, <code>value0</code>, <code>condition1</code>, <code>value1</code>, ..., <code>default</code>. If <code>condition0</code> evaluates to <code>True</code>, then assign <code>value0</code> to <code>column_name</code>, if <code>condition1</code> evaluates to <code>True</code>, then assign <code>value1</code> to <code>column_name</code>, and so on. If none of the conditions evaluate to <code>True</code>, assign <code>default</code> to <code>column_name</code>.</p> <p>This function can be likened to SQL's <code>case_when</code>:</p> <pre><code>CASE WHEN condition0 THEN value0\n    WHEN condition1 THEN value1\n    --- more conditions\n    ELSE default\n    END AS column_name\n</code></pre> <p>compared to python's <code>if-elif-else</code>:</p> <pre><code>if condition0:\n    value0\nelif condition1:\n    value1\n# more elifs\nelse:\n    default\n</code></pre> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame(\n...     {\n...         \"a\": [0, 0, 1, 2, \"hi\"],\n...         \"b\": [0, 3, 4, 5, \"bye\"],\n...         \"c\": [6, 7, 8, 9, \"wait\"],\n...     }\n... )\n&gt;&gt;&gt; df\n    a    b     c\n0   0    0     6\n1   0    3     7\n2   1    4     8\n3   2    5     9\n4  hi  bye  wait\n&gt;&gt;&gt; df.case_when(\n...     ((df.a == 0) &amp; (df.b != 0)) | (df.c == \"wait\"), df.a,\n...     (df.b == 0) &amp; (df.a == 0), \"x\",\n...     default = df.c,\n...     column_name = \"value\",\n... )\n    a    b     c value\n0   0    0     6     x\n1   0    3     7     0\n2   1    4     8     8\n3   2    5     9     9\n4  hi  bye  wait    hi\n</code></pre> <p>Version Changed</p> <ul> <li>0.24.0<ul> <li>Added <code>default</code> parameter.</li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>*args</code> <code>Any</code> <p>Variable argument of conditions and expected values. Takes the form <code>condition0</code>, <code>value0</code>, <code>condition1</code>, <code>value1</code>, ... . <code>condition</code> can be a 1-D boolean array, a callable, or a string. If <code>condition</code> is a callable, it should evaluate to a 1-D boolean array. The array should have the same length as the DataFrame. If it is a string, it is computed on the dataframe, via <code>df.eval</code>, and should return a 1-D boolean array. <code>result</code> can be a scalar, a 1-D array, or a callable. If <code>result</code> is a callable, it should evaluate to a 1-D array. For a 1-D array, it should have the same length as the DataFrame.</p> <code>()</code> <code>default</code> <code>Any</code> <p>This is the element inserted in the output when all conditions evaluate to False. Can be scalar, 1-D array or callable. If callable, it should evaluate to a 1-D array. The 1-D array should be the same length as the DataFrame.</p> <code>None</code> <code>column_name</code> <code>str</code> <p>Name of column to assign results to. A new column is created if it does not already exist in the DataFrame.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If condition/value fails to evaluate.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame.</p> Source code in <code>janitor/functions/case_when.py</code> <pre><code>@pf.register_dataframe_method\n@refactored_function(\n    message=(\n        \"This function will be deprecated in a 1.x release. \"\n        \"Please use `pd.Series.case_when` instead.\"\n    )\n)\ndef case_when(\n    df: pd.DataFrame, *args: Any, default: Any = None, column_name: str\n) -&gt; pd.DataFrame:\n    \"\"\"Create a column based on a condition or multiple conditions.\n\n    Similar to SQL and dplyr's case_when\n    with inspiration from `pydatatable` if_else function.\n\n    If your scenario requires direct replacement of values,\n    pandas' `replace` method or `map` method should be better\n    suited and more efficient; if the conditions check\n    if a value is within a range of values, pandas' `cut` or `qcut`\n    should be more efficient; `np.where/np.select` are also\n    performant options.\n\n    This function relies on `pd.Series.mask` method.\n\n    When multiple conditions are satisfied, the first one is used.\n\n    The variable `*args` parameters takes arguments of the form :\n    `condition0`, `value0`, `condition1`, `value1`, ..., `default`.\n    If `condition0` evaluates to `True`, then assign `value0` to\n    `column_name`, if `condition1` evaluates to `True`, then\n    assign `value1` to `column_name`, and so on. If none of the\n    conditions evaluate to `True`, assign `default` to\n    `column_name`.\n\n    This function can be likened to SQL's `case_when`:\n\n    ```sql\n    CASE WHEN condition0 THEN value0\n        WHEN condition1 THEN value1\n        --- more conditions\n        ELSE default\n        END AS column_name\n    ```\n\n    compared to python's `if-elif-else`:\n\n    ```python\n    if condition0:\n        value0\n    elif condition1:\n        value1\n    # more elifs\n    else:\n        default\n    ```\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame(\n        ...     {\n        ...         \"a\": [0, 0, 1, 2, \"hi\"],\n        ...         \"b\": [0, 3, 4, 5, \"bye\"],\n        ...         \"c\": [6, 7, 8, 9, \"wait\"],\n        ...     }\n        ... )\n        &gt;&gt;&gt; df\n            a    b     c\n        0   0    0     6\n        1   0    3     7\n        2   1    4     8\n        3   2    5     9\n        4  hi  bye  wait\n        &gt;&gt;&gt; df.case_when(\n        ...     ((df.a == 0) &amp; (df.b != 0)) | (df.c == \"wait\"), df.a,\n        ...     (df.b == 0) &amp; (df.a == 0), \"x\",\n        ...     default = df.c,\n        ...     column_name = \"value\",\n        ... )\n            a    b     c value\n        0   0    0     6     x\n        1   0    3     7     0\n        2   1    4     8     8\n        3   2    5     9     9\n        4  hi  bye  wait    hi\n\n    !!! abstract \"Version Changed\"\n\n        - 0.24.0\n            - Added `default` parameter.\n\n    Args:\n        df: A pandas DataFrame.\n        *args: Variable argument of conditions and expected values.\n            Takes the form\n            `condition0`, `value0`, `condition1`, `value1`, ... .\n            `condition` can be a 1-D boolean array, a callable, or a string.\n            If `condition` is a callable, it should evaluate\n            to a 1-D boolean array. The array should have the same length\n            as the DataFrame. If it is a string, it is computed on the dataframe,\n            via `df.eval`, and should return a 1-D boolean array.\n            `result` can be a scalar, a 1-D array, or a callable.\n            If `result` is a callable, it should evaluate to a 1-D array.\n            For a 1-D array, it should have the same length as the DataFrame.\n        default: This is the element inserted in the output\n            when all conditions evaluate to False.\n            Can be scalar, 1-D array or callable.\n            If callable, it should evaluate to a 1-D array.\n            The 1-D array should be the same length as the DataFrame.\n        column_name: Name of column to assign results to. A new column\n            is created if it does not already exist in the DataFrame.\n\n    Raises:\n        ValueError: If condition/value fails to evaluate.\n\n    Returns:\n        A pandas DataFrame.\n    \"\"\"  # noqa: E501\n    # Preliminary checks on the case_when function.\n    # The bare minimum checks are done; the remaining checks\n    # are done within `pd.Series.mask`.\n    check(\"column_name\", column_name, [str])\n    len_args = len(args)\n    if len_args &lt; 2:\n        raise ValueError(\n            \"At least two arguments are required for the `args` parameter\"\n        )\n\n    if len_args % 2:\n        if default is None:\n            warnings.warn(\n                \"The last argument in the variable arguments \"\n                \"has been assigned as the default. \"\n                \"Note however that this will be deprecated \"\n                \"in a future release; use an even number \"\n                \"of boolean conditions and values, \"\n                \"and pass the default argument to the `default` \"\n                \"parameter instead.\",\n                DeprecationWarning,\n                stacklevel=find_stack_level(),\n            )\n            *args, default = args\n        else:\n            raise ValueError(\n                \"The number of conditions and values do not match. \"\n                f\"There are {len_args - len_args//2} conditions \"\n                f\"and {len_args//2} values.\"\n            )\n\n    booleans = []\n    replacements = []\n\n    for index, value in enumerate(args):\n        if index % 2:\n            if callable(value):\n                value = apply_if_callable(value, df)\n            replacements.append(value)\n        else:\n            if callable(value):\n                value = apply_if_callable(value, df)\n            elif isinstance(value, str):\n                value = df.eval(value)\n            booleans.append(value)\n\n    if callable(default):\n        default = apply_if_callable(default, df)\n    if is_scalar(default):\n        default = pd.Series([default]).repeat(len(df))\n    if not hasattr(default, \"shape\"):\n        default = pd.Series([*default])\n    if isinstance(default, pd.Index):\n        arr_ndim = default.nlevels\n    else:\n        arr_ndim = default.ndim\n    if arr_ndim != 1:\n        raise ValueError(\n            \"The argument for the `default` parameter \"\n            \"should either be a 1-D array, a scalar, \"\n            \"or a callable that can evaluate to a 1-D array.\"\n        )\n    if not isinstance(default, pd.Series):\n        default = pd.Series(default)\n    default.index = df.index\n    # actual computation\n    # ensures value assignment is on a first come basis\n    booleans = booleans[::-1]\n    replacements = replacements[::-1]\n    for index, (condition, value) in enumerate(zip(booleans, replacements)):\n        try:\n            default = default.mask(condition, value)\n        # error `feedoff` idea from SO\n        # https://stackoverflow.com/a/46091127/7175713\n        except Exception as error:\n            raise ValueError(\n                f\"condition{index} and value{index} failed to evaluate. \"\n                f\"Original error message: {error}\"\n            ) from error\n\n    return df.assign(**{column_name: default})\n</code></pre>"},{"location":"api/functions/#janitor.functions.change_type","title":"<code>change_type</code>","text":""},{"location":"api/functions/#janitor.functions.change_type.change_type","title":"<code>change_type(df, column_name, dtype, ignore_exception=False)</code>","text":"<p>Change the type of a column.</p> <p>This method does not mutate the original DataFrame.</p> <p>Exceptions that are raised can be ignored. For example, if one has a mixed dtype column that has non-integer strings and integers, and you want to coerce everything to integers, you can optionally ignore the non-integer strings and replace them with <code>NaN</code> or keep the original value.</p> <p>Intended to be the method-chaining alternative to:</p> <pre><code>df[col] = df[col].astype(dtype)\n</code></pre> <p>Note</p> <p>This function will be deprecated in a 1.x release. Please use <code>pd.DataFrame.astype</code> instead.</p> <p>Examples:</p> <p>Change the type of a column.</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\"col1\": range(3), \"col2\": [\"m\", 5, True]})\n&gt;&gt;&gt; df\n   col1  col2\n0     0     m\n1     1     5\n2     2  True\n&gt;&gt;&gt; df.change_type(\n...     \"col1\", dtype=str,\n... ).change_type(\n...     \"col2\", dtype=float, ignore_exception=\"fillna\",\n... )\n  col1  col2\n0    0   NaN\n1    1   5.0\n2    2   1.0\n</code></pre> <p>Change the type of multiple columns. To change the type of all columns, please use <code>DataFrame.astype</code> instead.</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\"col1\": range(3), \"col2\": [\"m\", 5, True]})\n&gt;&gt;&gt; df.change_type(['col1', 'col2'], str)\n  col1  col2\n0    0     m\n1    1     5\n2    2  True\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>column_name</code> <code>Hashable | list[Hashable] | Index</code> <p>The column(s) in the dataframe.</p> required <code>dtype</code> <code>type</code> <p>The datatype to convert to. Should be one of the standard Python types, or a numpy datatype.</p> required <code>ignore_exception</code> <code>bool</code> <p>One of <code>{False, \"fillna\", \"keep_values\"}</code>.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If unknown option provided for <code>ignore_exception</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with changed column types.</p> Source code in <code>janitor/functions/change_type.py</code> <pre><code>@pf.register_dataframe_method\n@refactored_function(\n    message=(\n        \"This function will be deprecated in a 1.x release. \"\n        \"Please use `pd.DataFrame.astype` instead.\"\n    )\n)\n@deprecated_alias(column=\"column_name\")\ndef change_type(\n    df: pd.DataFrame,\n    column_name: Hashable | list[Hashable] | pd.Index,\n    dtype: type,\n    ignore_exception: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Change the type of a column.\n\n    This method does not mutate the original DataFrame.\n\n    Exceptions that are raised can be ignored. For example, if one has a mixed\n    dtype column that has non-integer strings and integers, and you want to\n    coerce everything to integers, you can optionally ignore the non-integer\n    strings and replace them with `NaN` or keep the original value.\n\n    Intended to be the method-chaining alternative to:\n\n    ```python\n    df[col] = df[col].astype(dtype)\n    ```\n\n    !!!note\n\n        This function will be deprecated in a 1.x release.\n        Please use `pd.DataFrame.astype` instead.\n\n    Examples:\n        Change the type of a column.\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\"col1\": range(3), \"col2\": [\"m\", 5, True]})\n        &gt;&gt;&gt; df\n           col1  col2\n        0     0     m\n        1     1     5\n        2     2  True\n        &gt;&gt;&gt; df.change_type(\n        ...     \"col1\", dtype=str,\n        ... ).change_type(\n        ...     \"col2\", dtype=float, ignore_exception=\"fillna\",\n        ... )\n          col1  col2\n        0    0   NaN\n        1    1   5.0\n        2    2   1.0\n\n        Change the type of multiple columns. To change the type of all columns,\n        please use `DataFrame.astype` instead.\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\"col1\": range(3), \"col2\": [\"m\", 5, True]})\n        &gt;&gt;&gt; df.change_type(['col1', 'col2'], str)\n          col1  col2\n        0    0     m\n        1    1     5\n        2    2  True\n\n    Args:\n        df: A pandas DataFrame.\n        column_name: The column(s) in the dataframe.\n        dtype: The datatype to convert to. Should be one of the standard\n            Python types, or a numpy datatype.\n        ignore_exception: One of `{False, \"fillna\", \"keep_values\"}`.\n\n    Raises:\n        ValueError: If unknown option provided for `ignore_exception`.\n\n    Returns:\n        A pandas DataFrame with changed column types.\n    \"\"\"  # noqa: E501\n\n    df = df.copy()  # avoid mutating the original DataFrame\n    if not ignore_exception:\n        df[column_name] = df[column_name].astype(dtype)\n    elif ignore_exception == \"keep_values\":\n        df[column_name] = df[column_name].astype(dtype, errors=\"ignore\")\n    elif ignore_exception == \"fillna\":\n        if isinstance(column_name, Hashable):\n            column_name = [column_name]\n        df[column_name] = df[column_name].map(_convert, dtype=dtype)\n    else:\n        raise ValueError(\"Unknown option for ignore_exception\")\n\n    return df\n</code></pre>"},{"location":"api/functions/#janitor.functions.clean_names","title":"<code>clean_names</code>","text":"<p>Functions for cleaning columns/index names and/or column values.</p>"},{"location":"api/functions/#janitor.functions.clean_names.clean_names","title":"<code>clean_names(df, axis='columns', column_names=None, strip_underscores=None, case_type='lower', remove_special=False, strip_accents=True, preserve_original_labels=True, enforce_string=True, truncate_limit=None)</code>","text":"<p>Clean column/index names. It can also be applied to column values.</p> <p>Takes all column names, converts them to lowercase, then replaces all spaces with underscores.</p> <p>By default, column names are converted to string types. This can be switched off by passing in <code>enforce_string=False</code>.</p> <p>This method does not mutate the original DataFrame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame(\n...     {\n...         \"Aloha\": range(3),\n...         \"Bell Chart\": range(3),\n...         \"Animals@#$%^\": range(3)\n...     }\n... )\n&gt;&gt;&gt; df\n   Aloha  Bell Chart  Animals@#$%^\n0      0           0             0\n1      1           1             1\n2      2           2             2\n&gt;&gt;&gt; df.clean_names()\n   aloha  bell_chart  animals@#$%^\n0      0           0             0\n1      1           1             1\n2      2           2             2\n&gt;&gt;&gt; df.clean_names(remove_special=True)\n   aloha  bell_chart  animals\n0      0           0        0\n1      1           1        1\n2      2           2        2\n</code></pre> <p>Version Changed</p> <ul> <li>0.26.0<ul> <li>Added <code>axis</code> and <code>column_names</code> parameters.</li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The pandas DataFrame object.</p> required <code>axis</code> <code>str</code> <p>Whether to clean the labels on the index or columns. If <code>None</code>, applies to a defined column or columns in <code>column_names</code>.</p> <code>'columns'</code> <code>column_names</code> <code>str | list</code> <p>Clean the values in a column. <code>axis</code> should be <code>None</code>. Column selection is possible using the <code>select</code> syntax.</p> <code>None</code> <code>strip_underscores</code> <code>str | bool</code> <p>Removes the outer underscores from all column names/values. Default None keeps outer underscores. Values can be either 'left', 'right' or 'both' or the respective shorthand 'l', 'r' and True.</p> <code>None</code> <code>case_type</code> <code>str</code> <p>Whether to make columns lower or uppercase. Current case may be preserved with 'preserve', while snake case conversion (from CamelCase or camelCase only) can be turned on using \"snake\". Default 'lower' makes all characters lowercase.</p> <code>'lower'</code> <code>remove_special</code> <code>bool</code> <p>Remove special characters from columns. Only letters, numbers and underscores are preserved.</p> <code>False</code> <code>strip_accents</code> <code>bool</code> <p>Whether or not to remove accents from columns names/values.</p> <code>True</code> <code>preserve_original_labels</code> <code>bool</code> <p>Preserve original names. This is later retrievable using <code>df.original_labels</code>. Applies if <code>axis</code> is not None.</p> <code>True</code> <code>enforce_string</code> <code>bool</code> <p>Whether or not to convert all column names/values to string type. Defaults to True, but can be turned off. Columns with &gt;1 levels will not be converted by default.</p> <code>True</code> <code>truncate_limit</code> <code>int</code> <p>Truncates formatted column names/values to the specified length. Default None does not truncate.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>axis=None</code> and <code>column_names=None</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame.</p> Source code in <code>janitor/functions/clean_names.py</code> <pre><code>@pf.register_dataframe_method\n@deprecated_alias(preserve_original_columns=\"preserve_original_labels\")\ndef clean_names(\n    df: pd.DataFrame,\n    axis: str = \"columns\",\n    column_names: str | list = None,\n    strip_underscores: str | bool = None,\n    case_type: str = \"lower\",\n    remove_special: bool = False,\n    strip_accents: bool = True,\n    preserve_original_labels: bool = True,\n    enforce_string: bool = True,\n    truncate_limit: int = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Clean column/index names. It can also be applied to column values.\n\n    Takes all column names, converts them to lowercase,\n    then replaces all spaces with underscores.\n\n    By default, column names are converted to string types.\n    This can be switched off by passing in `enforce_string=False`.\n\n    This method does not mutate the original DataFrame.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame(\n        ...     {\n        ...         \"Aloha\": range(3),\n        ...         \"Bell Chart\": range(3),\n        ...         \"Animals@#$%^\": range(3)\n        ...     }\n        ... )\n        &gt;&gt;&gt; df\n           Aloha  Bell Chart  Animals@#$%^\n        0      0           0             0\n        1      1           1             1\n        2      2           2             2\n        &gt;&gt;&gt; df.clean_names()\n           aloha  bell_chart  animals@#$%^\n        0      0           0             0\n        1      1           1             1\n        2      2           2             2\n        &gt;&gt;&gt; df.clean_names(remove_special=True)\n           aloha  bell_chart  animals\n        0      0           0        0\n        1      1           1        1\n        2      2           2        2\n\n    !!! summary \"Version Changed\"\n\n        - 0.26.0\n             - Added `axis` and `column_names` parameters.\n\n    Args:\n        df: The pandas DataFrame object.\n        axis: Whether to clean the labels on the index or columns.\n            If `None`, applies to a defined column\n            or columns in `column_names`.\n        column_names: Clean the values in a column.\n            `axis` should be `None`.\n            Column selection is possible using the\n            [`select`][janitor.functions.select.select] syntax.\n        strip_underscores: Removes the outer underscores from all\n            column names/values. Default None keeps outer underscores.\n            Values can be either 'left', 'right' or 'both'\n            or the respective shorthand 'l',\n            'r' and True.\n        case_type: Whether to make columns lower or uppercase.\n            Current case may be preserved with 'preserve',\n            while snake case conversion (from CamelCase or camelCase only)\n            can be turned on using \"snake\".\n            Default 'lower' makes all characters lowercase.\n        remove_special: Remove special characters from columns.\n            Only letters, numbers and underscores are preserved.\n        strip_accents: Whether or not to remove accents from\n            columns names/values.\n        preserve_original_labels: Preserve original names.\n            This is later retrievable using `df.original_labels`.\n            Applies if `axis` is not None.\n        enforce_string: Whether or not to convert all\n            column names/values to string type.\n            Defaults to True, but can be turned off.\n            Columns with &gt;1 levels will not be converted by default.\n        truncate_limit: Truncates formatted column names/values\n            to the specified length.\n            Default None does not truncate.\n\n    Raises:\n        ValueError: If `axis=None` and `column_names=None`.\n\n    Returns:\n        A pandas DataFrame.\n    \"\"\"\n    if not axis and not column_names:\n        raise ValueError(\n            \"Kindly provide an argument to `column_names`, if axis is None.\"\n        )\n    if axis is None:\n        column_names = get_index_labels(\n            arg=column_names, df=df, axis=\"columns\"\n        )\n        if is_scalar(column_names):\n            column_names = [column_names]\n        df = df.copy()\n        for column_name in column_names:\n            df[column_name] = _clean_names(\n                obj=df[column_name],\n                enforce_string=enforce_string,\n                case_type=case_type,\n                remove_special=remove_special,\n                strip_accents=strip_accents,\n                strip_underscores=strip_underscores,\n                truncate_limit=truncate_limit,\n            )\n        return df\n\n    assert axis in {\"index\", \"columns\"}\n    df = df[:]\n    target_axis = getattr(df, axis)\n    if isinstance(target_axis, pd.MultiIndex):\n        target_axis = [\n            target_axis.get_level_values(number)\n            for number in range(target_axis.nlevels)\n        ]\n        target_axis = [\n            _clean_names(\n                obj=obj,\n                enforce_string=enforce_string,\n                case_type=case_type,\n                remove_special=remove_special,\n                strip_accents=strip_accents,\n                strip_underscores=strip_underscores,\n                truncate_limit=truncate_limit,\n            )\n            for obj in target_axis\n        ]\n    else:\n        target_axis = _clean_names(\n            obj=target_axis,\n            enforce_string=enforce_string,\n            case_type=case_type,\n            remove_special=remove_special,\n            strip_accents=strip_accents,\n            strip_underscores=strip_underscores,\n            truncate_limit=truncate_limit,\n        )\n    # Store the original column names, if enabled by user\n    if preserve_original_labels:\n        df.__dict__[\"original_labels\"] = getattr(df, axis)\n    setattr(df, axis, target_axis)\n    return df\n</code></pre>"},{"location":"api/functions/#janitor.functions.coalesce","title":"<code>coalesce</code>","text":"<p>Function for performing coalesce.</p>"},{"location":"api/functions/#janitor.functions.coalesce.coalesce","title":"<code>coalesce(df, *column_names, target_column_name=None, default_value=None)</code>","text":"<p>Coalesce two or more columns of data in order of column names provided.</p> <p>Given the variable arguments of column names, <code>coalesce</code> finds and returns the first non-missing value from these columns, for every row in the input dataframe. If all the column values are null for a particular row, then the <code>default_value</code> will be filled in.</p> <p>If <code>target_column_name</code> is not provided, then the first column is coalesced.</p> <p>This method does not mutate the original DataFrame.</p> <p>The <code>select</code> syntax can be used in <code>column_names</code>.</p> <p>Examples:</p> <p>Use <code>coalesce</code> with 3 columns, \"a\", \"b\" and \"c\".</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"a\": [np.nan, 1, np.nan],\n...     \"b\": [2, 3, np.nan],\n...     \"c\": [4, np.nan, np.nan],\n... })\n&gt;&gt;&gt; df.coalesce(\"a\", \"b\", \"c\")\n     a    b    c\n0  2.0  2.0  4.0\n1  1.0  3.0  NaN\n2  NaN  NaN  NaN\n</code></pre> <p>Provide a target_column_name.</p> <pre><code>&gt;&gt;&gt; df.coalesce(\"a\", \"b\", \"c\", target_column_name=\"new_col\")\n     a    b    c  new_col\n0  NaN  2.0  4.0      2.0\n1  1.0  3.0  NaN      1.0\n2  NaN  NaN  NaN      NaN\n</code></pre> <p>Provide a default value.</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"a\": [1, np.nan, np.nan],\n...     \"b\": [2, 3, np.nan],\n... })\n&gt;&gt;&gt; df.coalesce(\n...     \"a\", \"b\",\n...     target_column_name=\"new_col\",\n...     default_value=-1,\n... )\n     a    b  new_col\n0  1.0  2.0      1.0\n1  NaN  3.0      3.0\n2  NaN  NaN     -1.0\n</code></pre> <p>This is more syntactic diabetes! For R users, this should look familiar to <code>dplyr</code>'s <code>coalesce</code> function; for Python users, the interface should be more intuitive than the <code>pandas.Series.combine_first</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>column_names</code> <code>Any</code> <p>A list of column names.</p> <code>()</code> <code>target_column_name</code> <code>Optional[str]</code> <p>The new column name after combining. If <code>None</code>, then the first column in <code>column_names</code> is updated, with the Null values replaced.</p> <code>None</code> <code>default_value</code> <code>Optional[Union[int, float, str]]</code> <p>A scalar to replace any remaining nulls after coalescing.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If length of <code>column_names</code> is less than 2.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with coalesced columns.</p> Source code in <code>janitor/functions/coalesce.py</code> <pre><code>@pf.register_dataframe_method\n@deprecated_alias(columns=\"column_names\", new_column_name=\"target_column_name\")\ndef coalesce(\n    df: pd.DataFrame,\n    *column_names: Any,\n    target_column_name: Optional[str] = None,\n    default_value: Optional[Union[int, float, str]] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Coalesce two or more columns of data in order of column names provided.\n\n    Given the variable arguments of column names,\n    `coalesce` finds and returns the first non-missing value\n    from these columns, for every row in the input dataframe.\n    If all the column values are null for a particular row,\n    then the `default_value` will be filled in.\n\n    If `target_column_name` is not provided,\n    then the first column is coalesced.\n\n    This method does not mutate the original DataFrame.\n\n    The [`select`][janitor.functions.select.select] syntax\n    can be used in `column_names`.\n\n    Examples:\n        Use `coalesce` with 3 columns, \"a\", \"b\" and \"c\".\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     \"a\": [np.nan, 1, np.nan],\n        ...     \"b\": [2, 3, np.nan],\n        ...     \"c\": [4, np.nan, np.nan],\n        ... })\n        &gt;&gt;&gt; df.coalesce(\"a\", \"b\", \"c\")\n             a    b    c\n        0  2.0  2.0  4.0\n        1  1.0  3.0  NaN\n        2  NaN  NaN  NaN\n\n        Provide a target_column_name.\n\n        &gt;&gt;&gt; df.coalesce(\"a\", \"b\", \"c\", target_column_name=\"new_col\")\n             a    b    c  new_col\n        0  NaN  2.0  4.0      2.0\n        1  1.0  3.0  NaN      1.0\n        2  NaN  NaN  NaN      NaN\n\n        Provide a default value.\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     \"a\": [1, np.nan, np.nan],\n        ...     \"b\": [2, 3, np.nan],\n        ... })\n        &gt;&gt;&gt; df.coalesce(\n        ...     \"a\", \"b\",\n        ...     target_column_name=\"new_col\",\n        ...     default_value=-1,\n        ... )\n             a    b  new_col\n        0  1.0  2.0      1.0\n        1  NaN  3.0      3.0\n        2  NaN  NaN     -1.0\n\n    This is more syntactic diabetes! For R users, this should look familiar to\n    `dplyr`'s `coalesce` function; for Python users, the interface\n    should be more intuitive than the `pandas.Series.combine_first`\n    method.\n\n    Args:\n        df: A pandas DataFrame.\n        column_names: A list of column names.\n        target_column_name: The new column name after combining.\n            If `None`, then the first column in `column_names` is updated,\n            with the Null values replaced.\n        default_value: A scalar to replace any remaining nulls\n            after coalescing.\n\n    Raises:\n        ValueError: If length of `column_names` is less than 2.\n\n    Returns:\n        A pandas DataFrame with coalesced columns.\n    \"\"\"\n\n    if not column_names:\n        return df\n\n    indexers = _select_index([*column_names], df, axis=\"columns\")\n\n    if len(indexers) &lt; 2:\n        raise ValueError(\n            \"The number of columns to coalesce should be a minimum of 2.\"\n        )\n\n    if target_column_name:\n        check(\"target_column_name\", target_column_name, [str])\n\n    if default_value:\n        check(\"default_value\", default_value, [int, float, str])\n\n    df = df.copy()\n\n    outcome = df.iloc[:, indexers[0]]\n\n    for num in range(1, len(indexers)):\n        position = indexers[num]\n        replacement = df.iloc[:, position]\n        outcome = outcome.fillna(replacement)\n\n    if outcome.hasnans and (default_value is not None):\n        outcome = outcome.fillna(default_value)\n\n    if target_column_name is None:\n        df.iloc[:, indexers[0]] = outcome\n    else:\n        df[target_column_name] = outcome\n\n    return df\n</code></pre>"},{"location":"api/functions/#janitor.functions.collapse_levels","title":"<code>collapse_levels</code>","text":"<p>Implementation of the <code>collapse_levels</code> function.</p>"},{"location":"api/functions/#janitor.functions.collapse_levels.collapse_levels","title":"<code>collapse_levels(df, sep=None, glue=None, axis='columns')</code>","text":"<p>Flatten multi-level index/column dataframe to a single level.</p> <p>This method does not mutate the original DataFrame.</p> <p>Given a DataFrame containing multi-level index/columns, flatten to single-level by string-joining the labels in each level.</p> <p>After a <code>groupby</code> / <code>aggregate</code> operation where <code>.agg()</code> is passed a list of multiple aggregation functions, a multi-level DataFrame is returned with the name of the function applied in the second level.</p> <p>It is sometimes convenient for later indexing to flatten out this multi-level configuration back into a single level. This function does this through a simple string-joining of all the names across different levels in a single column.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"class\": [\"bird\", \"bird\", \"bird\", \"mammal\", \"mammal\"],\n...     \"max_speed\": [389, 389, 24, 80, 21],\n...     \"type\": [\"falcon\", \"falcon\", \"parrot\", \"Lion\", \"Monkey\"],\n... })\n&gt;&gt;&gt; df\n    class  max_speed    type\n0    bird        389  falcon\n1    bird        389  falcon\n2    bird         24  parrot\n3  mammal         80    Lion\n4  mammal         21  Monkey\n&gt;&gt;&gt; grouped_df = df.groupby(\"class\")[['max_speed']].agg([\"mean\", \"median\"])\n&gt;&gt;&gt; grouped_df\n         max_speed\n              mean median\nclass\nbird    267.333333  389.0\nmammal   50.500000   50.5\n&gt;&gt;&gt; grouped_df.collapse_levels(sep=\"_\")\n        max_speed_mean  max_speed_median\nclass\nbird        267.333333             389.0\nmammal       50.500000              50.5\n</code></pre> <p>Before applying <code>.collapse_levels</code>, the <code>.agg</code> operation returns a multi-level column DataFrame whose columns are <code>(level 1, level 2)</code>:</p> <pre><code>[(\"max_speed\", \"mean\"), (\"max_speed\", \"median\")]\n</code></pre> <p><code>.collapse_levels</code> then flattens the column MultiIndex into a single level index with names:</p> <pre><code>[\"max_speed_mean\", \"max_speed_median\"]\n</code></pre> <p>For more control, a <code>glue</code> specification can be passed, where the names of the levels are used to control the output of the flattened index:</p> <pre><code>&gt;&gt;&gt; (grouped_df\n...  .rename_axis(columns=['column_name', 'agg_name'])\n...  .collapse_levels(glue=\"{agg_name}_{column_name}\")\n... )\n        mean_max_speed  median_max_speed\nclass\nbird        267.333333             389.0\nmammal       50.500000              50.5\n</code></pre> <p>Note that for <code>glue</code> to work, the keyword arguments in the glue specification should be the names of the levels in the MultiIndex.</p> <p>Version Changed</p> <ul> <li>0.27.0<ul> <li>Added <code>glue</code> and <code>axis</code> parameters.</li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>sep</code> <code>str</code> <p>String separator used to join the column level names.</p> <code>None</code> <code>glue</code> <code>str</code> <p>A specification on how the column levels should be combined. It allows for a more granular composition, and serves as an alternative to <code>sep</code>.</p> <code>None</code> <code>axis</code> <code>str</code> <p>Determines whether to collapse the levels on the index or columns.</p> <code>'columns'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with single-level column index.</p> Source code in <code>janitor/functions/collapse_levels.py</code> <pre><code>@pf.register_dataframe_method\ndef collapse_levels(\n    df: pd.DataFrame,\n    sep: str = None,\n    glue: str = None,\n    axis: str = \"columns\",\n) -&gt; pd.DataFrame:\n    \"\"\"Flatten multi-level index/column dataframe to a single level.\n\n    This method does not mutate the original DataFrame.\n\n    Given a DataFrame containing multi-level index/columns, flatten to single-level\n    by string-joining the labels in each level.\n\n    After a `groupby` / `aggregate` operation where `.agg()` is passed a\n    list of multiple aggregation functions, a multi-level DataFrame is\n    returned with the name of the function applied in the second level.\n\n    It is sometimes convenient for later indexing to flatten out this\n    multi-level configuration back into a single level. This function does\n    this through a simple string-joining of all the names across different\n    levels in a single column.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     \"class\": [\"bird\", \"bird\", \"bird\", \"mammal\", \"mammal\"],\n        ...     \"max_speed\": [389, 389, 24, 80, 21],\n        ...     \"type\": [\"falcon\", \"falcon\", \"parrot\", \"Lion\", \"Monkey\"],\n        ... })\n        &gt;&gt;&gt; df\n            class  max_speed    type\n        0    bird        389  falcon\n        1    bird        389  falcon\n        2    bird         24  parrot\n        3  mammal         80    Lion\n        4  mammal         21  Monkey\n        &gt;&gt;&gt; grouped_df = df.groupby(\"class\")[['max_speed']].agg([\"mean\", \"median\"])\n        &gt;&gt;&gt; grouped_df  # doctest: +NORMALIZE_WHITESPACE\n                 max_speed\n                      mean median\n        class\n        bird    267.333333  389.0\n        mammal   50.500000   50.5\n        &gt;&gt;&gt; grouped_df.collapse_levels(sep=\"_\")  # doctest: +NORMALIZE_WHITESPACE\n                max_speed_mean  max_speed_median\n        class\n        bird        267.333333             389.0\n        mammal       50.500000              50.5\n\n        Before applying `.collapse_levels`, the `.agg` operation returns a\n        multi-level column DataFrame whose columns are `(level 1, level 2)`:\n\n        ```python\n        [(\"max_speed\", \"mean\"), (\"max_speed\", \"median\")]\n        ```\n\n        `.collapse_levels` then flattens the column MultiIndex into a single\n        level index with names:\n\n        ```python\n        [\"max_speed_mean\", \"max_speed_median\"]\n        ```\n\n        For more control, a `glue` specification can be passed,\n        where the names of the levels are used to control the output of the\n        flattened index:\n        &gt;&gt;&gt; (grouped_df\n        ...  .rename_axis(columns=['column_name', 'agg_name'])\n        ...  .collapse_levels(glue=\"{agg_name}_{column_name}\")\n        ... )\n                mean_max_speed  median_max_speed\n        class\n        bird        267.333333             389.0\n        mammal       50.500000              50.5\n\n        Note that for `glue` to work, the keyword arguments\n        in the glue specification\n        should be the names of the levels in the MultiIndex.\n\n    !!! abstract \"Version Changed\"\n\n        - 0.27.0\n            - Added `glue` and `axis` parameters.\n\n    Args:\n        df: A pandas DataFrame.\n        sep: String separator used to join the column level names.\n        glue: A specification on how the column levels should be combined.\n            It allows for a more granular composition,\n            and serves as an alternative to `sep`.\n        axis: Determines whether to collapse the\n            levels on the index or columns.\n\n    Returns:\n        A pandas DataFrame with single-level column index.\n    \"\"\"  # noqa: E501\n    if (sep is not None) and (glue is not None):\n        raise ValueError(\"Only one of sep or glue should be provided.\")\n    if sep is not None:\n        check(\"sep\", sep, [str])\n    if glue is not None:\n        check(\"glue\", glue, [str])\n    check(\"axis\", axis, [str])\n    if axis not in {\"index\", \"columns\"}:\n        raise ValueError(\n            \"axis argument should be either 'index' or 'columns'.\"\n        )\n\n    if not isinstance(getattr(df, axis), pd.MultiIndex):\n        return df\n\n    # TODO: Pyarrow offers faster string computations\n    # future work should take this into consideration,\n    # which would require a different route from python's string.join\n    # since work is only on the columns\n    # it is safe, and more efficient to slice/view the dataframe\n    # plus Pandas creates a new Index altogether\n    # as such, the original dataframe is not modified\n    df = df[:]\n    new_index = getattr(df, axis)\n    if glue is not None:\n        new_index = [dict(zip(new_index.names, entry)) for entry in new_index]\n        new_index = [glue.format_map(mapping) for mapping in new_index]\n        setattr(df, axis, new_index)\n        return df\n    sep = \"_\" if sep is None else sep\n    levels = [level for level in new_index.levels]\n    all_strings = all(map(is_string_dtype, levels))\n    if all_strings:\n        no_empty_string = all((entry != \"\").all() for entry in levels)\n        if no_empty_string:\n            new_index = new_index.map(sep.join)\n            setattr(df, axis, new_index)\n            return df\n    new_index = (map(str, entry) for entry in new_index)\n    new_index = [\n        # faster to use a list comprehension within string.join\n        # compared to a generator\n        # https://stackoverflow.com/a/37782238\n        sep.join([entry for entry in word if entry])\n        for word in new_index\n    ]\n    setattr(df, axis, new_index)\n    return df\n</code></pre>"},{"location":"api/functions/#janitor.functions.complete","title":"<code>complete</code>","text":""},{"location":"api/functions/#janitor.functions.complete.complete","title":"<code>complete(df, *columns, sort=False, by=None, fill_value=None, explicit=True)</code>","text":"<p>Complete a data frame with missing combinations of data.</p> <p>It is modeled after tidyr's <code>complete</code> function. In a way, it is the inverse of <code>pd.dropna</code>, as it exposes implicitly missing rows.</p> <p>The variable <code>columns</code> parameter can be a column name, a list of column names, or a pandas Index, Series, or DataFrame. If a pandas Index, Series, or DataFrame is passed, it should have a name or names that exist in <code>df</code>.</p> <p>A callable can also be passed - the callable should evaluate to a pandas Index, Series, or DataFrame, and the names of the pandas object should exist in <code>df</code>.</p> <p>A dictionary can also be passed - the values of the dictionary should be either be a 1D array or a callable that evaluates to a 1D array, while the keys of the dictionary should exist in <code>df</code>.</p> <p>User should ensure that the pandas object is unique and/or sorted - no checks are done to ensure uniqueness and/or sortedness.</p> <p>If <code>by</code> is present, the DataFrame is completed per group. <code>by</code> should be a column name, or a list of column names.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; df = pd.DataFrame(\n...     {\n...         \"Year\": [1999, 2000, 2004, 1999, 2004],\n...         \"Taxon\": [\n...             \"Saccharina\",\n...             \"Saccharina\",\n...             \"Saccharina\",\n...             \"Agarum\",\n...             \"Agarum\",\n...         ],\n...         \"Abundance\": [4, 5, 2, 1, 8],\n...     }\n... )\n&gt;&gt;&gt; df\n   Year       Taxon  Abundance\n0  1999  Saccharina          4\n1  2000  Saccharina          5\n2  2004  Saccharina          2\n3  1999      Agarum          1\n4  2004      Agarum          8\n</code></pre> <p>Expose missing pairings of <code>Year</code> and <code>Taxon</code>:</p> <pre><code>&gt;&gt;&gt; df.complete(\"Year\", \"Taxon\", sort=True)\n   Year       Taxon  Abundance\n0  1999      Agarum        1.0\n1  1999  Saccharina        4.0\n2  2000      Agarum        NaN\n3  2000  Saccharina        5.0\n4  2004      Agarum        8.0\n5  2004  Saccharina        2.0\n</code></pre> <p>Expose missing years from 1999 to 2004:</p> <pre><code>&gt;&gt;&gt; index = pd.Index(range(1999,2005),name='Year')\n&gt;&gt;&gt; df.complete(index, \"Taxon\", sort=True)\n    Year       Taxon  Abundance\n0   1999      Agarum        1.0\n1   1999  Saccharina        4.0\n2   2000      Agarum        NaN\n3   2000  Saccharina        5.0\n4   2001      Agarum        NaN\n5   2001  Saccharina        NaN\n6   2002      Agarum        NaN\n7   2002  Saccharina        NaN\n8   2003      Agarum        NaN\n9   2003  Saccharina        NaN\n10  2004      Agarum        8.0\n11  2004  Saccharina        2.0\n</code></pre> <p>A dictionary can be used as well:</p> <pre><code>&gt;&gt;&gt; dictionary = {'Year':range(1999,2005)}\n&gt;&gt;&gt; df.complete(dictionary, \"Taxon\", sort=True)\n    Year       Taxon  Abundance\n0   1999      Agarum        1.0\n1   1999  Saccharina        4.0\n2   2000      Agarum        NaN\n3   2000  Saccharina        5.0\n4   2001      Agarum        NaN\n5   2001  Saccharina        NaN\n6   2002      Agarum        NaN\n7   2002  Saccharina        NaN\n8   2003      Agarum        NaN\n9   2003  Saccharina        NaN\n10  2004      Agarum        8.0\n11  2004  Saccharina        2.0\n</code></pre> <p>Fill missing values:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame(\n...     dict(\n...         group=(1, 2, 1, 2),\n...         item_id=(1, 2, 2, 3),\n...         item_name=(\"a\", \"a\", \"b\", \"b\"),\n...         value1=(1, np.nan, 3, 4),\n...         value2=range(4, 8),\n...     )\n... )\n&gt;&gt;&gt; df\n   group  item_id item_name  value1  value2\n0      1        1         a     1.0       4\n1      2        2         a     NaN       5\n2      1        2         b     3.0       6\n3      2        3         b     4.0       7\n</code></pre> <pre><code>&gt;&gt;&gt; df.complete(\n...     \"group\",\n...     [\"item_id\", \"item_name\"],\n...     fill_value={\"value1\": 0, \"value2\": 99},\n...     sort=True\n... )\n   group  item_id item_name  value1  value2\n0      1        1         a     1.0     4.0\n1      1        2         a     0.0    99.0\n2      1        2         b     3.0     6.0\n3      1        3         b     0.0    99.0\n4      2        1         a     0.0    99.0\n5      2        2         a     0.0     5.0\n6      2        2         b     0.0    99.0\n7      2        3         b     4.0     7.0\n</code></pre> <p>Limit the fill to only implicit missing values by setting explicit to <code>False</code>:</p> <pre><code>&gt;&gt;&gt; df.complete(\n...     \"group\",\n...     [\"item_id\", \"item_name\"],\n...     fill_value={\"value1\": 0, \"value2\": 99},\n...     explicit=False,\n...     sort=True\n... )\n   group  item_id item_name  value1  value2\n0      1        1         a     1.0     4.0\n1      1        2         a     0.0    99.0\n2      1        2         b     3.0     6.0\n3      1        3         b     0.0    99.0\n4      2        1         a     0.0    99.0\n5      2        2         a     NaN     5.0\n6      2        2         b     0.0    99.0\n7      2        3         b     4.0     7.0\n</code></pre> <p>Expose missing rows per group, using a callable:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame(\n...     {\n...         \"state\": [\"CA\", \"CA\", \"HI\", \"HI\", \"HI\", \"NY\", \"NY\"],\n...         \"year\": [2010, 2013, 2010, 2012, 2016, 2009, 2013],\n...         \"value\": [1, 3, 1, 2, 3, 2, 5],\n...     }\n... )\n&gt;&gt;&gt; df\n  state  year  value\n0    CA  2010      1\n1    CA  2013      3\n2    HI  2010      1\n3    HI  2012      2\n4    HI  2016      3\n5    NY  2009      2\n6    NY  2013      5\n</code></pre> <pre><code>&gt;&gt;&gt; def new_year_values(df):\n...     return pd.RangeIndex(start=df.year.min(), stop=df.year.max() + 1, name='year')\n&gt;&gt;&gt; df.complete(new_year_values, by='state',sort=True)\n    state  year  value\n0     CA  2010    1.0\n1     CA  2011    NaN\n2     CA  2012    NaN\n3     CA  2013    3.0\n4     HI  2010    1.0\n5     HI  2011    NaN\n6     HI  2012    2.0\n7     HI  2013    NaN\n8     HI  2014    NaN\n9     HI  2015    NaN\n10    HI  2016    3.0\n11    NY  2009    2.0\n12    NY  2010    NaN\n13    NY  2011    NaN\n14    NY  2012    NaN\n15    NY  2013    5.0\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>*columns</code> <code>Any</code> <p>This refers to the columns to be completed. It could be a column name, a list of column names, or a pandas Index, Series, or DataFrame.</p> <p>It can also be a callable that gets evaluated to a pandas Index, Series, or DataFrame.</p> <p>It can also be a dictionary, where the values are either a 1D array or a callable that evaluates to a 1D array, while the keys of the dictionary should exist in <code>df</code>.</p> <code>()</code> <code>sort</code> <code>bool</code> <p>Sort DataFrame based on *columns.</p> <code>False</code> <code>by</code> <code>str | list</code> <p>Label or list of labels to group by. The explicit missing rows are returned per group.</p> <code>None</code> <code>fill_value</code> <code>dict | Any</code> <p>Scalar value to use instead of NaN for missing combinations. A dictionary, mapping columns names to a scalar value is also accepted.</p> <code>None</code> <code>explicit</code> <code>bool</code> <p>Determines if only implicitly missing values should be filled (<code>False</code>), or all nulls existing in the dataframe (<code>True</code>). <code>explicit</code> is applicable only if <code>fill_value</code> is not <code>None</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with explicit missing rows, if any.</p> Source code in <code>janitor/functions/complete.py</code> <pre><code>@pf.register_dataframe_method\ndef complete(\n    df: pd.DataFrame,\n    *columns: Any,\n    sort: bool = False,\n    by: str | list = None,\n    fill_value: dict | Any = None,\n    explicit: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Complete a data frame with missing combinations of data.\n\n    It is modeled after tidyr's `complete` function.\n    In a way, it is the inverse of `pd.dropna`, as it exposes\n    implicitly missing rows.\n\n    The variable `columns` parameter can be a column name,\n    a list of column names,\n    or a pandas Index, Series, or DataFrame.\n    If a pandas Index, Series, or DataFrame is passed, it should\n    have a name or names that exist in `df`.\n\n    A callable can also be passed - the callable should evaluate\n    to a pandas Index, Series, or DataFrame,\n    and the names of the pandas object should exist in `df`.\n\n    A dictionary can also be passed -\n    the values of the dictionary should be\n    either be a 1D array\n    or a callable that evaluates to a\n    1D array,\n    while the keys of the dictionary\n    should exist in `df`.\n\n    User should ensure that the pandas object is unique and/or sorted\n    - no checks are done to ensure uniqueness and/or sortedness.\n\n    If `by` is present, the DataFrame is *completed* per group.\n    `by` should be a column name, or a list of column names.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; df = pd.DataFrame(\n        ...     {\n        ...         \"Year\": [1999, 2000, 2004, 1999, 2004],\n        ...         \"Taxon\": [\n        ...             \"Saccharina\",\n        ...             \"Saccharina\",\n        ...             \"Saccharina\",\n        ...             \"Agarum\",\n        ...             \"Agarum\",\n        ...         ],\n        ...         \"Abundance\": [4, 5, 2, 1, 8],\n        ...     }\n        ... )\n        &gt;&gt;&gt; df\n           Year       Taxon  Abundance\n        0  1999  Saccharina          4\n        1  2000  Saccharina          5\n        2  2004  Saccharina          2\n        3  1999      Agarum          1\n        4  2004      Agarum          8\n\n        Expose missing pairings of `Year` and `Taxon`:\n        &gt;&gt;&gt; df.complete(\"Year\", \"Taxon\", sort=True)\n           Year       Taxon  Abundance\n        0  1999      Agarum        1.0\n        1  1999  Saccharina        4.0\n        2  2000      Agarum        NaN\n        3  2000  Saccharina        5.0\n        4  2004      Agarum        8.0\n        5  2004  Saccharina        2.0\n\n        Expose missing years from 1999 to 2004:\n        &gt;&gt;&gt; index = pd.Index(range(1999,2005),name='Year')\n        &gt;&gt;&gt; df.complete(index, \"Taxon\", sort=True)\n            Year       Taxon  Abundance\n        0   1999      Agarum        1.0\n        1   1999  Saccharina        4.0\n        2   2000      Agarum        NaN\n        3   2000  Saccharina        5.0\n        4   2001      Agarum        NaN\n        5   2001  Saccharina        NaN\n        6   2002      Agarum        NaN\n        7   2002  Saccharina        NaN\n        8   2003      Agarum        NaN\n        9   2003  Saccharina        NaN\n        10  2004      Agarum        8.0\n        11  2004  Saccharina        2.0\n\n        A dictionary can be used as well:\n        &gt;&gt;&gt; dictionary = {'Year':range(1999,2005)}\n        &gt;&gt;&gt; df.complete(dictionary, \"Taxon\", sort=True)\n            Year       Taxon  Abundance\n        0   1999      Agarum        1.0\n        1   1999  Saccharina        4.0\n        2   2000      Agarum        NaN\n        3   2000  Saccharina        5.0\n        4   2001      Agarum        NaN\n        5   2001  Saccharina        NaN\n        6   2002      Agarum        NaN\n        7   2002  Saccharina        NaN\n        8   2003      Agarum        NaN\n        9   2003  Saccharina        NaN\n        10  2004      Agarum        8.0\n        11  2004  Saccharina        2.0\n\n        Fill missing values:\n        &gt;&gt;&gt; df = pd.DataFrame(\n        ...     dict(\n        ...         group=(1, 2, 1, 2),\n        ...         item_id=(1, 2, 2, 3),\n        ...         item_name=(\"a\", \"a\", \"b\", \"b\"),\n        ...         value1=(1, np.nan, 3, 4),\n        ...         value2=range(4, 8),\n        ...     )\n        ... )\n        &gt;&gt;&gt; df\n           group  item_id item_name  value1  value2\n        0      1        1         a     1.0       4\n        1      2        2         a     NaN       5\n        2      1        2         b     3.0       6\n        3      2        3         b     4.0       7\n\n        &gt;&gt;&gt; df.complete(\n        ...     \"group\",\n        ...     [\"item_id\", \"item_name\"],\n        ...     fill_value={\"value1\": 0, \"value2\": 99},\n        ...     sort=True\n        ... )\n           group  item_id item_name  value1  value2\n        0      1        1         a     1.0     4.0\n        1      1        2         a     0.0    99.0\n        2      1        2         b     3.0     6.0\n        3      1        3         b     0.0    99.0\n        4      2        1         a     0.0    99.0\n        5      2        2         a     0.0     5.0\n        6      2        2         b     0.0    99.0\n        7      2        3         b     4.0     7.0\n\n        Limit the fill to only implicit missing values\n        by setting explicit to `False`:\n        &gt;&gt;&gt; df.complete(\n        ...     \"group\",\n        ...     [\"item_id\", \"item_name\"],\n        ...     fill_value={\"value1\": 0, \"value2\": 99},\n        ...     explicit=False,\n        ...     sort=True\n        ... )\n           group  item_id item_name  value1  value2\n        0      1        1         a     1.0     4.0\n        1      1        2         a     0.0    99.0\n        2      1        2         b     3.0     6.0\n        3      1        3         b     0.0    99.0\n        4      2        1         a     0.0    99.0\n        5      2        2         a     NaN     5.0\n        6      2        2         b     0.0    99.0\n        7      2        3         b     4.0     7.0\n\n        Expose missing rows per group, using a callable:\n        &gt;&gt;&gt; df = pd.DataFrame(\n        ...     {\n        ...         \"state\": [\"CA\", \"CA\", \"HI\", \"HI\", \"HI\", \"NY\", \"NY\"],\n        ...         \"year\": [2010, 2013, 2010, 2012, 2016, 2009, 2013],\n        ...         \"value\": [1, 3, 1, 2, 3, 2, 5],\n        ...     }\n        ... )\n        &gt;&gt;&gt; df\n          state  year  value\n        0    CA  2010      1\n        1    CA  2013      3\n        2    HI  2010      1\n        3    HI  2012      2\n        4    HI  2016      3\n        5    NY  2009      2\n        6    NY  2013      5\n\n        &gt;&gt;&gt; def new_year_values(df):\n        ...     return pd.RangeIndex(start=df.year.min(), stop=df.year.max() + 1, name='year')\n        &gt;&gt;&gt; df.complete(new_year_values, by='state',sort=True)\n            state  year  value\n        0     CA  2010    1.0\n        1     CA  2011    NaN\n        2     CA  2012    NaN\n        3     CA  2013    3.0\n        4     HI  2010    1.0\n        5     HI  2011    NaN\n        6     HI  2012    2.0\n        7     HI  2013    NaN\n        8     HI  2014    NaN\n        9     HI  2015    NaN\n        10    HI  2016    3.0\n        11    NY  2009    2.0\n        12    NY  2010    NaN\n        13    NY  2011    NaN\n        14    NY  2012    NaN\n        15    NY  2013    5.0\n\n    Args:\n        df: A pandas DataFrame.\n        *columns: This refers to the columns to be completed.\n            It could be a column name,\n            a list of column names,\n            or a pandas Index, Series, or DataFrame.\n\n            It can also be a callable that gets evaluated\n            to a pandas Index, Series, or DataFrame.\n\n            It can also be a dictionary,\n            where the values are either a 1D array\n            or a callable that evaluates to a\n            1D array,\n            while the keys of the dictionary\n            should exist in `df`.\n        sort: Sort DataFrame based on *columns.\n        by: Label or list of labels to group by.\n            The explicit missing rows are returned per group.\n        fill_value: Scalar value to use instead of NaN\n            for missing combinations. A dictionary, mapping columns names\n            to a scalar value is also accepted.\n        explicit: Determines if only implicitly missing values\n            should be filled (`False`), or all nulls existing in the dataframe\n            (`True`). `explicit` is applicable only\n            if `fill_value` is not `None`.\n\n    Returns:\n        A pandas DataFrame with explicit missing rows, if any.\n    \"\"\"  # noqa: E501\n\n    if not columns:\n        return df\n    return _computations_complete(df, columns, sort, by, fill_value, explicit)\n</code></pre>"},{"location":"api/functions/#janitor.functions.concatenate_columns","title":"<code>concatenate_columns</code>","text":""},{"location":"api/functions/#janitor.functions.concatenate_columns.concatenate_columns","title":"<code>concatenate_columns(df, column_names, new_column_name, sep='-', ignore_empty=True)</code>","text":"<p>Concatenates the set of columns into a single column.</p> <p>Used to quickly generate an index based on a group of columns.</p> <p>This method mutates the original DataFrame.</p> <p>Examples:</p> <p>Concatenate two columns row-wise.</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\"a\": [1, 3, 5], \"b\": list(\"xyz\")})\n&gt;&gt;&gt; df\n   a  b\n0  1  x\n1  3  y\n2  5  z\n&gt;&gt;&gt; df.concatenate_columns(\n...     column_names=[\"a\", \"b\"], new_column_name=\"m\",\n... )\n   a  b    m\n0  1  x  1-x\n1  3  y  3-y\n2  5  z  5-z\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>column_names</code> <code>List[Hashable]</code> <p>A list of columns to concatenate together.</p> required <code>new_column_name</code> <code>Hashable</code> <p>The name of the new column.</p> required <code>sep</code> <code>str</code> <p>The separator between each column's data.</p> <code>'-'</code> <code>ignore_empty</code> <code>bool</code> <p>Ignore null values if exists.</p> <code>True</code> <p>Raises:</p> Type Description <code>JanitorError</code> <p>If at least two columns are not provided within <code>column_names</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with concatenated columns.</p> Source code in <code>janitor/functions/concatenate_columns.py</code> <pre><code>@pf.register_dataframe_method\n@deprecated_alias(columns=\"column_names\")\ndef concatenate_columns(\n    df: pd.DataFrame,\n    column_names: List[Hashable],\n    new_column_name: Hashable,\n    sep: str = \"-\",\n    ignore_empty: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"Concatenates the set of columns into a single column.\n\n    Used to quickly generate an index based on a group of columns.\n\n    This method mutates the original DataFrame.\n\n    Examples:\n        Concatenate two columns row-wise.\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\"a\": [1, 3, 5], \"b\": list(\"xyz\")})\n        &gt;&gt;&gt; df\n           a  b\n        0  1  x\n        1  3  y\n        2  5  z\n        &gt;&gt;&gt; df.concatenate_columns(\n        ...     column_names=[\"a\", \"b\"], new_column_name=\"m\",\n        ... )\n           a  b    m\n        0  1  x  1-x\n        1  3  y  3-y\n        2  5  z  5-z\n\n    Args:\n        df: A pandas DataFrame.\n        column_names: A list of columns to concatenate together.\n        new_column_name: The name of the new column.\n        sep: The separator between each column's data.\n        ignore_empty: Ignore null values if exists.\n\n    Raises:\n        JanitorError: If at least two columns are not provided\n            within `column_names`.\n\n    Returns:\n        A pandas DataFrame with concatenated columns.\n    \"\"\"\n    if len(column_names) &lt; 2:\n        raise JanitorError(\"At least two columns must be specified\")\n\n    df[new_column_name] = (\n        df[column_names].astype(str).fillna(\"\").agg(sep.join, axis=1)\n    )\n\n    if ignore_empty:\n\n        def remove_empty_string(x):\n            \"\"\"Ignore empty/null string values from the concatenated output.\"\"\"\n            return sep.join(x for x in x.split(sep) if x)\n\n        df[new_column_name] = df[new_column_name].transform(\n            remove_empty_string\n        )\n\n    return df\n</code></pre>"},{"location":"api/functions/#janitor.functions.conditional_join","title":"<code>conditional_join</code>","text":""},{"location":"api/functions/#janitor.functions.conditional_join.conditional_join","title":"<code>conditional_join(df, right, *conditions, how='inner', df_columns=slice(None), right_columns=slice(None), keep='all', use_numba=False, indicator=False, force=False)</code>","text":"<p>The conditional_join function operates similarly to <code>pd.merge</code>, but supports joins on inequality operators, or a combination of equi and non-equi joins.</p> <p>Joins solely on equality are not supported.</p> <p>If the join is solely on equality, <code>pd.merge</code> function covers that; if you are interested in nearest joins, asof joins, or rolling joins, then <code>pd.merge_asof</code> covers that. There is also pandas' IntervalIndex, which is efficient for range joins, especially if the intervals do not overlap.</p> <p>Column selection in <code>df_columns</code> and <code>right_columns</code> is possible using the <code>select</code> syntax.</p> <p>Performance might be improved by setting <code>use_numba</code> to <code>True</code> - this can be handy for equi joins that have lots of duplicated keys. This can also be handy for non-equi joins, where there are more than two join conditions, or there is significant overlap in the range join columns. This assumes that <code>numba</code> is installed.</p> <p>Noticeable performance can be observed for range joins, if both join columns from the right dataframe are monotonically increasing.</p> <p>This function returns rows, if any, where values from <code>df</code> meet the condition(s) for values from <code>right</code>. The conditions are passed in as a variable argument of tuples, where the tuple is of the form <code>(left_on, right_on, op)</code>; <code>left_on</code> is the column label from <code>df</code>, <code>right_on</code> is the column label from <code>right</code>, while <code>op</code> is the operator.</p> <p>For multiple conditions, the and(<code>&amp;</code>) operator is used to combine the results of the individual conditions.</p> <p>In some scenarios there might be performance gains if the less than join, or the greater than join condition, or the range condition is executed before the equi join - pass <code>force=True</code> to force this.</p> <p>The operator can be any of <code>==</code>, <code>!=</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>&gt;</code>.</p> <p>There is no optimisation for the <code>!=</code> operator.</p> <p>The join is done only on the columns.</p> <p>For non-equi joins, only numeric, timedelta and date columns are supported.</p> <p><code>inner</code>, <code>left</code>, <code>right</code> and <code>outer</code> joins are supported.</p> <p>If the columns from <code>df</code> and <code>right</code> have nothing in common, a single index column is returned; else, a MultiIndex column is returned.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df1 = pd.DataFrame({\"value_1\": [2, 5, 7, 1, 3, 4]})\n&gt;&gt;&gt; df2 = pd.DataFrame({\"value_2A\": [0, 3, 7, 12, 0, 2, 3, 1],\n...                     \"value_2B\": [1, 5, 9, 15, 1, 4, 6, 3],\n...                    })\n&gt;&gt;&gt; df1\n   value_1\n0        2\n1        5\n2        7\n3        1\n4        3\n5        4\n&gt;&gt;&gt; df2\n   value_2A  value_2B\n0         0         1\n1         3         5\n2         7         9\n3        12        15\n4         0         1\n5         2         4\n6         3         6\n7         1         3\n</code></pre> <pre><code>&gt;&gt;&gt; df1.conditional_join(\n...     df2,\n...     (\"value_1\", \"value_2A\", \"&gt;\"),\n...     (\"value_1\", \"value_2B\", \"&lt;\")\n... )\n   value_1  value_2A  value_2B\n0        2         1         3\n1        5         3         6\n2        3         2         4\n3        4         3         5\n4        4         3         6\n</code></pre> <p>Select specific columns, after the join:</p> <pre><code>&gt;&gt;&gt; df1.conditional_join(\n...     df2,\n...     (\"value_1\", \"value_2A\", \"&gt;\"),\n...     (\"value_1\", \"value_2B\", \"&lt;\"),\n...     right_columns='value_2B',\n...     how='left'\n... )\n   value_1  value_2B\n0        2       3.0\n1        5       6.0\n2        3       4.0\n3        4       5.0\n4        4       6.0\n5        7       NaN\n6        1       NaN\n</code></pre> <p>Rename columns, before the join:</p> <pre><code>&gt;&gt;&gt; (df1\n...  .rename(columns={'value_1':'left_column'})\n...  .conditional_join(\n...      df2,\n...     (\"left_column\", \"value_2A\", \"&gt;\"),\n...     (\"left_column\", \"value_2B\", \"&lt;\"),\n...      right_columns='value_2B',\n...      how='outer')\n... )\n    left_column  value_2B\n0           2.0       3.0\n1           5.0       6.0\n2           3.0       4.0\n3           4.0       5.0\n4           4.0       6.0\n5           7.0       NaN\n6           1.0       NaN\n7           NaN       1.0\n8           NaN       9.0\n9           NaN      15.0\n10          NaN       1.0\n</code></pre> <p>Get the first match:</p> <pre><code>&gt;&gt;&gt; df1.conditional_join(\n...     df2,\n...     (\"value_1\", \"value_2A\", \"&gt;\"),\n...     (\"value_1\", \"value_2B\", \"&lt;\"),\n...     keep='first'\n... )\n   value_1  value_2A  value_2B\n0        2         1         3\n1        5         3         6\n2        3         2         4\n3        4         3         5\n</code></pre> <p>Get the last match:</p> <pre><code>&gt;&gt;&gt; df1.conditional_join(\n...     df2,\n...     (\"value_1\", \"value_2A\", \"&gt;\"),\n...     (\"value_1\", \"value_2B\", \"&lt;\"),\n...     keep='last'\n... )\n   value_1  value_2A  value_2B\n0        2         1         3\n1        5         3         6\n2        3         2         4\n3        4         3         6\n</code></pre> <p>Add an indicator column:</p> <pre><code>&gt;&gt;&gt; df1.conditional_join(\n...     df2,\n...     (\"value_1\", \"value_2A\", \"&gt;\"),\n...     (\"value_1\", \"value_2B\", \"&lt;\"),\n...     how='outer',\n...     indicator=True\n... )\n    value_1  value_2A  value_2B      _merge\n0       2.0       1.0       3.0        both\n1       5.0       3.0       6.0        both\n2       3.0       2.0       4.0        both\n3       4.0       3.0       5.0        both\n4       4.0       3.0       6.0        both\n5       7.0       NaN       NaN   left_only\n6       1.0       NaN       NaN   left_only\n7       NaN       0.0       1.0  right_only\n8       NaN       7.0       9.0  right_only\n9       NaN      12.0      15.0  right_only\n10      NaN       0.0       1.0  right_only\n</code></pre> <p>Version Changed</p> <ul> <li>0.24.0<ul> <li>Added <code>df_columns</code>, <code>right_columns</code>, <code>keep</code> and <code>use_numba</code> parameters.</li> </ul> </li> <li>0.24.1<ul> <li>Added <code>indicator</code> parameter.</li> </ul> </li> <li>0.25.0<ul> <li><code>col</code> class supported.</li> <li>Outer join supported. <code>sort_by_appearance</code> deprecated.</li> <li>Numba support for equi join</li> </ul> </li> <li>0.27.0<ul> <li>Added support for timedelta dtype.</li> </ul> </li> <li>0.28.0<ul> <li><code>col</code> class deprecated.</li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>right</code> <code>Union[DataFrame, Series]</code> <p>Named Series or DataFrame to join to.</p> required <code>conditions</code> <code>Any</code> <p>Variable argument of tuple(s) of the form <code>(left_on, right_on, op)</code>, where <code>left_on</code> is the column label from <code>df</code>, <code>right_on</code> is the column label from <code>right</code>, while <code>op</code> is the operator. The <code>col</code> class is also supported. The operator can be any of <code>==</code>, <code>!=</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>&gt;</code>. For multiple conditions, the and(<code>&amp;</code>) operator is used to combine the results of the individual conditions.</p> <code>()</code> <code>how</code> <code>Literal['inner', 'left', 'right', 'outer']</code> <p>Indicates the type of join to be performed. It can be one of <code>inner</code>, <code>left</code>, <code>right</code> or <code>outer</code>.</p> <code>'inner'</code> <code>df_columns</code> <code>Optional[Any]</code> <p>Columns to select from <code>df</code> in the final output dataframe. Column selection is based on the <code>select</code> syntax.</p> <code>slice(None)</code> <code>right_columns</code> <code>Optional[Any]</code> <p>Columns to select from <code>right</code> in the final output dataframe. Column selection is based on the <code>select</code> syntax.</p> <code>slice(None)</code> <code>use_numba</code> <code>bool</code> <p>Use numba, if installed, to accelerate the computation.</p> <code>False</code> <code>keep</code> <code>Literal['first', 'last', 'all']</code> <p>Choose whether to return the first match, last match or all matches.</p> <code>'all'</code> <code>indicator</code> <code>Optional[Union[bool, str]]</code> <p>If <code>True</code>, adds a column to the output DataFrame called <code>_merge</code> with information on the source of each row. The column can be given a different name by providing a string argument. The column will have a Categorical type with the value of <code>left_only</code> for observations whose merge key only appears in the left DataFrame, <code>right_only</code> for observations whose merge key only appears in the right DataFrame, and <code>both</code> if the observation\u2019s merge key is found in both DataFrames.</p> <code>False</code> <code>force</code> <code>bool</code> <p>If <code>True</code>, force the non-equi join conditions to execute before the equi join.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame of the two merged Pandas objects.</p> Source code in <code>janitor/functions/conditional_join.py</code> <pre><code>@pf.register_dataframe_method\ndef conditional_join(\n    df: pd.DataFrame,\n    right: Union[pd.DataFrame, pd.Series],\n    *conditions: Any,\n    how: Literal[\"inner\", \"left\", \"right\", \"outer\"] = \"inner\",\n    df_columns: Optional[Any] = slice(None),\n    right_columns: Optional[Any] = slice(None),\n    keep: Literal[\"first\", \"last\", \"all\"] = \"all\",\n    use_numba: bool = False,\n    indicator: Optional[Union[bool, str]] = False,\n    force: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"The conditional_join function operates similarly to `pd.merge`,\n    but supports joins on inequality operators,\n    or a combination of equi and non-equi joins.\n\n    Joins solely on equality are not supported.\n\n    If the join is solely on equality, `pd.merge` function\n    covers that; if you are interested in nearest joins, asof joins,\n    or rolling joins, then `pd.merge_asof` covers that.\n    There is also pandas' IntervalIndex, which is efficient for range joins,\n    especially if the intervals do not overlap.\n\n    Column selection in `df_columns` and `right_columns` is possible using the\n    [`select`][janitor.functions.select.select] syntax.\n\n    Performance might be improved by setting `use_numba` to `True` -\n    this can be handy for equi joins that have lots of duplicated keys.\n    This can also be handy for non-equi joins, where there are more than\n    two join conditions,\n    or there is significant overlap in the range join columns.\n    This assumes that `numba` is installed.\n\n    Noticeable performance can be observed for range joins,\n    if both join columns from the right dataframe\n    are monotonically increasing.\n\n    This function returns rows, if any, where values from `df` meet the\n    condition(s) for values from `right`. The conditions are passed in\n    as a variable argument of tuples, where the tuple is of\n    the form `(left_on, right_on, op)`; `left_on` is the column\n    label from `df`, `right_on` is the column label from `right`,\n    while `op` is the operator.\n\n    For multiple conditions, the and(`&amp;`)\n    operator is used to combine the results of the individual conditions.\n\n    In some scenarios there might be performance gains if the less than join,\n    or the greater than join condition, or the range condition\n    is executed before the equi join - pass `force=True` to force this.\n\n    The operator can be any of `==`, `!=`, `&lt;=`, `&lt;`, `&gt;=`, `&gt;`.\n\n    There is no optimisation for the `!=` operator.\n\n    The join is done only on the columns.\n\n    For non-equi joins, only numeric, timedelta and date columns are supported.\n\n    `inner`, `left`, `right` and `outer` joins are supported.\n\n    If the columns from `df` and `right` have nothing in common,\n    a single index column is returned; else, a MultiIndex column\n    is returned.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df1 = pd.DataFrame({\"value_1\": [2, 5, 7, 1, 3, 4]})\n        &gt;&gt;&gt; df2 = pd.DataFrame({\"value_2A\": [0, 3, 7, 12, 0, 2, 3, 1],\n        ...                     \"value_2B\": [1, 5, 9, 15, 1, 4, 6, 3],\n        ...                    })\n        &gt;&gt;&gt; df1\n           value_1\n        0        2\n        1        5\n        2        7\n        3        1\n        4        3\n        5        4\n        &gt;&gt;&gt; df2\n           value_2A  value_2B\n        0         0         1\n        1         3         5\n        2         7         9\n        3        12        15\n        4         0         1\n        5         2         4\n        6         3         6\n        7         1         3\n\n        &gt;&gt;&gt; df1.conditional_join(\n        ...     df2,\n        ...     (\"value_1\", \"value_2A\", \"&gt;\"),\n        ...     (\"value_1\", \"value_2B\", \"&lt;\")\n        ... )\n           value_1  value_2A  value_2B\n        0        2         1         3\n        1        5         3         6\n        2        3         2         4\n        3        4         3         5\n        4        4         3         6\n\n        Select specific columns, after the join:\n        &gt;&gt;&gt; df1.conditional_join(\n        ...     df2,\n        ...     (\"value_1\", \"value_2A\", \"&gt;\"),\n        ...     (\"value_1\", \"value_2B\", \"&lt;\"),\n        ...     right_columns='value_2B',\n        ...     how='left'\n        ... )\n           value_1  value_2B\n        0        2       3.0\n        1        5       6.0\n        2        3       4.0\n        3        4       5.0\n        4        4       6.0\n        5        7       NaN\n        6        1       NaN\n\n        Rename columns, before the join:\n        &gt;&gt;&gt; (df1\n        ...  .rename(columns={'value_1':'left_column'})\n        ...  .conditional_join(\n        ...      df2,\n        ...     (\"left_column\", \"value_2A\", \"&gt;\"),\n        ...     (\"left_column\", \"value_2B\", \"&lt;\"),\n        ...      right_columns='value_2B',\n        ...      how='outer')\n        ... )\n            left_column  value_2B\n        0           2.0       3.0\n        1           5.0       6.0\n        2           3.0       4.0\n        3           4.0       5.0\n        4           4.0       6.0\n        5           7.0       NaN\n        6           1.0       NaN\n        7           NaN       1.0\n        8           NaN       9.0\n        9           NaN      15.0\n        10          NaN       1.0\n\n        Get the first match:\n        &gt;&gt;&gt; df1.conditional_join(\n        ...     df2,\n        ...     (\"value_1\", \"value_2A\", \"&gt;\"),\n        ...     (\"value_1\", \"value_2B\", \"&lt;\"),\n        ...     keep='first'\n        ... )\n           value_1  value_2A  value_2B\n        0        2         1         3\n        1        5         3         6\n        2        3         2         4\n        3        4         3         5\n\n        Get the last match:\n        &gt;&gt;&gt; df1.conditional_join(\n        ...     df2,\n        ...     (\"value_1\", \"value_2A\", \"&gt;\"),\n        ...     (\"value_1\", \"value_2B\", \"&lt;\"),\n        ...     keep='last'\n        ... )\n           value_1  value_2A  value_2B\n        0        2         1         3\n        1        5         3         6\n        2        3         2         4\n        3        4         3         6\n\n        Add an indicator column:\n        &gt;&gt;&gt; df1.conditional_join(\n        ...     df2,\n        ...     (\"value_1\", \"value_2A\", \"&gt;\"),\n        ...     (\"value_1\", \"value_2B\", \"&lt;\"),\n        ...     how='outer',\n        ...     indicator=True\n        ... )\n            value_1  value_2A  value_2B      _merge\n        0       2.0       1.0       3.0        both\n        1       5.0       3.0       6.0        both\n        2       3.0       2.0       4.0        both\n        3       4.0       3.0       5.0        both\n        4       4.0       3.0       6.0        both\n        5       7.0       NaN       NaN   left_only\n        6       1.0       NaN       NaN   left_only\n        7       NaN       0.0       1.0  right_only\n        8       NaN       7.0       9.0  right_only\n        9       NaN      12.0      15.0  right_only\n        10      NaN       0.0       1.0  right_only\n\n    !!! abstract \"Version Changed\"\n\n        - 0.24.0\n            - Added `df_columns`, `right_columns`, `keep` and `use_numba` parameters.\n        - 0.24.1\n            - Added `indicator` parameter.\n        - 0.25.0\n            - `col` class supported.\n            - Outer join supported. `sort_by_appearance` deprecated.\n            - Numba support for equi join\n        - 0.27.0\n            - Added support for timedelta dtype.\n        - 0.28.0\n            - `col` class deprecated.\n\n    Args:\n        df: A pandas DataFrame.\n        right: Named Series or DataFrame to join to.\n        conditions: Variable argument of tuple(s) of the form\n            `(left_on, right_on, op)`, where `left_on` is the column\n            label from `df`, `right_on` is the column label from `right`,\n            while `op` is the operator.\n            The `col` class is also supported. The operator can be any of\n            `==`, `!=`, `&lt;=`, `&lt;`, `&gt;=`, `&gt;`. For multiple conditions,\n            the and(`&amp;`) operator is used to combine the results\n            of the individual conditions.\n        how: Indicates the type of join to be performed.\n            It can be one of `inner`, `left`, `right` or `outer`.\n        df_columns: Columns to select from `df` in the final output dataframe.\n            Column selection is based on the\n            [`select`][janitor.functions.select.select] syntax.\n        right_columns: Columns to select from `right` in the final output dataframe.\n            Column selection is based on the\n            [`select`][janitor.functions.select.select] syntax.\n        use_numba: Use numba, if installed, to accelerate the computation.\n        keep: Choose whether to return the first match, last match or all matches.\n        indicator: If `True`, adds a column to the output DataFrame\n            called `_merge` with information on the source of each row.\n            The column can be given a different name by providing a string argument.\n            The column will have a Categorical type with the value of `left_only`\n            for observations whose merge key only appears in the left DataFrame,\n            `right_only` for observations whose merge key\n            only appears in the right DataFrame, and `both` if the observation\u2019s\n            merge key is found in both DataFrames.\n        force: If `True`, force the non-equi join conditions to execute before the equi join.\n\n\n    Returns:\n        A pandas DataFrame of the two merged Pandas objects.\n    \"\"\"  # noqa: E501\n\n    return _conditional_join_compute(\n        df=df,\n        right=right,\n        conditions=conditions,\n        how=how,\n        df_columns=df_columns,\n        right_columns=right_columns,\n        keep=keep,\n        use_numba=use_numba,\n        indicator=indicator,\n        force=force,\n    )\n</code></pre>"},{"location":"api/functions/#janitor.functions.conditional_join.get_join_indices","title":"<code>get_join_indices(df, right, conditions, keep='all', use_numba=False, force=False, return_ragged_arrays=False)</code>","text":"<p>Convenience function to return the matching indices from an inner join.</p> <p>New in version 0.27.0</p> <p>Version Changed</p> <ul> <li>0.29.0<ul> <li>Add support for ragged array indices.</li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>right</code> <code>Union[DataFrame, Series]</code> <p>Named Series or DataFrame to join to.</p> required <code>conditions</code> <code>list[tuple[str]]</code> <p>List of arguments of tuple(s) of the form <code>(left_on, right_on, op)</code>, where <code>left_on</code> is the column label from <code>df</code>, <code>right_on</code> is the column label from <code>right</code>, while <code>op</code> is the operator. The <code>col</code> class is also supported. The operator can be any of <code>==</code>, <code>!=</code>, <code>&lt;=</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>&gt;</code>. For multiple conditions, the and(<code>&amp;</code>) operator is used to combine the results of the individual conditions.</p> required <code>use_numba</code> <code>bool</code> <p>Use numba, if installed, to accelerate the computation.</p> <code>False</code> <code>keep</code> <code>Literal['first', 'last', 'all']</code> <p>Choose whether to return the first match, last match or all matches.</p> <code>'all'</code> <code>force</code> <code>bool</code> <p>If <code>True</code>, force the non-equi join conditions to execute before the equi join.</p> <code>False</code> <code>return_ragged_arrays</code> <code>bool</code> <p>If <code>True</code>, return slices/ranges of matching right indices for each matching left index. Not applicable if <code>use_numba</code> is <code>True</code>. If <code>return_ragged_arrays</code> is <code>True</code>, the join condition should be a single join, or a range join, where the right columns are both monotonically increasing.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray]</code> <p>A tuple of indices for the rows in the dataframes that match.</p> Source code in <code>janitor/functions/conditional_join.py</code> <pre><code>def get_join_indices(\n    df: pd.DataFrame,\n    right: Union[pd.DataFrame, pd.Series],\n    conditions: list[tuple[str]],\n    keep: Literal[\"first\", \"last\", \"all\"] = \"all\",\n    use_numba: bool = False,\n    force: bool = False,\n    return_ragged_arrays: bool = False,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Convenience function to return the matching indices from an inner join.\n\n    !!! info \"New in version 0.27.0\"\n\n    !!! abstract \"Version Changed\"\n\n        - 0.29.0\n            - Add support for ragged array indices.\n\n    Args:\n        df: A pandas DataFrame.\n        right: Named Series or DataFrame to join to.\n        conditions: List of arguments of tuple(s) of the form\n            `(left_on, right_on, op)`, where `left_on` is the column\n            label from `df`, `right_on` is the column label from `right`,\n            while `op` is the operator.\n            The `col` class is also supported. The operator can be any of\n            `==`, `!=`, `&lt;=`, `&lt;`, `&gt;=`, `&gt;`. For multiple conditions,\n            the and(`&amp;`) operator is used to combine the results\n            of the individual conditions.\n        use_numba: Use numba, if installed, to accelerate the computation.\n        keep: Choose whether to return the first match, last match or all matches.\n        force: If `True`, force the non-equi join conditions\n            to execute before the equi join.\n        return_ragged_arrays: If `True`, return slices/ranges of matching right indices\n            for each matching left index. Not applicable if `use_numba` is `True`.\n            If `return_ragged_arrays` is `True`, the join condition\n            should be a single join, or a range join,\n            where the right columns are both monotonically increasing.\n\n    Returns:\n        A tuple of indices for the rows in the dataframes that match.\n    \"\"\"\n    return _conditional_join_compute(\n        df=df,\n        right=right,\n        conditions=conditions,\n        how=\"inner\",\n        df_columns=None,\n        right_columns=None,\n        keep=keep,\n        use_numba=use_numba,\n        indicator=False,\n        force=force,\n        return_matching_indices=True,\n        return_ragged_arrays=return_ragged_arrays,\n    )\n</code></pre>"},{"location":"api/functions/#janitor.functions.convert_date","title":"<code>convert_date</code>","text":""},{"location":"api/functions/#janitor.functions.convert_date.convert_excel_date","title":"<code>convert_excel_date(df, column_names)</code>","text":"<p>Convert Excel's serial date format into Python datetime format.</p> <p>This method does not mutate the original DataFrame.</p> <p>Implementation is based on Stack Overflow.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\"date\": [39690, 39690, 37118]})\n&gt;&gt;&gt; df\n    date\n0  39690\n1  39690\n2  37118\n&gt;&gt;&gt; df.convert_excel_date('date')\n        date\n0 2008-08-30\n1 2008-08-30\n2 2001-08-15\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>column_names</code> <code>Union[Hashable, list]</code> <p>A column name, or a list of column names.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with corrected dates.</p> Source code in <code>janitor/functions/convert_date.py</code> <pre><code>@pf.register_dataframe_method\n@deprecated_alias(column=\"column_names\")\ndef convert_excel_date(\n    df: pd.DataFrame, column_names: Union[Hashable, list]\n) -&gt; pd.DataFrame:\n    \"\"\"Convert Excel's serial date format into Python datetime format.\n\n    This method does not mutate the original DataFrame.\n\n    Implementation is based on\n    [Stack Overflow](https://stackoverflow.com/questions/38454403/convert-excel-style-date-with-pandas).\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\"date\": [39690, 39690, 37118]})\n        &gt;&gt;&gt; df\n            date\n        0  39690\n        1  39690\n        2  37118\n        &gt;&gt;&gt; df.convert_excel_date('date')\n                date\n        0 2008-08-30\n        1 2008-08-30\n        2 2001-08-15\n\n    Args:\n        df: A pandas DataFrame.\n        column_names: A column name, or a list of column names.\n\n    Returns:\n        A pandas DataFrame with corrected dates.\n    \"\"\"  # noqa: E501\n\n    if not isinstance(column_names, list):\n        column_names = [column_names]\n    # https://stackoverflow.com/a/65460255/7175713\n    dictionary = {\n        column_name: pd.to_datetime(\n            df[column_name], unit=\"D\", origin=\"1899-12-30\"\n        )\n        for column_name in column_names\n    }\n\n    return df.assign(**dictionary)\n</code></pre>"},{"location":"api/functions/#janitor.functions.convert_date.convert_matlab_date","title":"<code>convert_matlab_date(df, column_names)</code>","text":"<p>Convert Matlab's serial date number into Python datetime format.</p> <p>Implementation is based on Stack Overflow.</p> <p>This method does not mutate the original DataFrame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\"date\": [737125.0, 737124.815863, 737124.4985, 737124]})\n&gt;&gt;&gt; df\n            date\n0  737125.000000\n1  737124.815863\n2  737124.498500\n3  737124.000000\n&gt;&gt;&gt; df.convert_matlab_date('date')\n                           date\n0 2018-03-06 00:00:00.000000000\n1 2018-03-05 19:34:50.563199671\n2 2018-03-05 11:57:50.399998876\n3 2018-03-05 00:00:00.000000000\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>column_names</code> <code>Union[Hashable, list]</code> <p>A column name, or a list of column names.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with corrected dates.</p> Source code in <code>janitor/functions/convert_date.py</code> <pre><code>@pf.register_dataframe_method\n@deprecated_alias(column=\"column_names\")\ndef convert_matlab_date(\n    df: pd.DataFrame, column_names: Union[Hashable, list]\n) -&gt; pd.DataFrame:\n    \"\"\"Convert Matlab's serial date number into Python datetime format.\n\n    Implementation is based on\n    [Stack Overflow](https://stackoverflow.com/questions/13965740/converting-matlabs-datenum-format-to-python).\n\n    This method does not mutate the original DataFrame.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\"date\": [737125.0, 737124.815863, 737124.4985, 737124]})\n        &gt;&gt;&gt; df\n                    date\n        0  737125.000000\n        1  737124.815863\n        2  737124.498500\n        3  737124.000000\n        &gt;&gt;&gt; df.convert_matlab_date('date')\n                                   date\n        0 2018-03-06 00:00:00.000000000\n        1 2018-03-05 19:34:50.563199671\n        2 2018-03-05 11:57:50.399998876\n        3 2018-03-05 00:00:00.000000000\n\n    Args:\n        df: A pandas DataFrame.\n        column_names: A column name, or a list of column names.\n\n    Returns:\n        A pandas DataFrame with corrected dates.\n    \"\"\"  # noqa: E501\n    # https://stackoverflow.com/a/49135037/7175713\n    if not isinstance(column_names, list):\n        column_names = [column_names]\n    dictionary = {\n        column_name: pd.to_datetime(df[column_name] - 719529, unit=\"D\")\n        for column_name in column_names\n    }\n\n    return df.assign(**dictionary)\n</code></pre>"},{"location":"api/functions/#janitor.functions.convert_date.convert_unix_date","title":"<code>convert_unix_date(df, column_name)</code>","text":"<p>Convert unix epoch time into Python datetime format.</p> <p>Note that this ignores local tz and convert all timestamps to naive datetime based on UTC!</p> <p>This method mutates the original DataFrame.</p> <p>Note</p> <p>This function will be deprecated in a 1.x release. Please use <code>pd.to_datetime</code> instead.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\"date\": [1651510462, 53394822, 1126233195]})\n&gt;&gt;&gt; df\n         date\n0  1651510462\n1    53394822\n2  1126233195\n&gt;&gt;&gt; df.convert_unix_date('date')\n                 date\n0 2022-05-02 16:54:22\n1 1971-09-10 23:53:42\n2 2005-09-09 02:33:15\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>column_name</code> <code>Hashable</code> <p>A column name.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with corrected dates.</p> Source code in <code>janitor/functions/convert_date.py</code> <pre><code>@pf.register_dataframe_method\n@refactored_function(\n    message=(\n        \"This function will be deprecated in a 1.x release. \"\n        \"Please use `pd.to_datetime` instead.\"\n    )\n)\n@deprecated_alias(column=\"column_name\")\ndef convert_unix_date(df: pd.DataFrame, column_name: Hashable) -&gt; pd.DataFrame:\n    \"\"\"Convert unix epoch time into Python datetime format.\n\n    Note that this ignores local tz and convert all timestamps to naive\n    datetime based on UTC!\n\n    This method mutates the original DataFrame.\n\n    !!!note\n\n        This function will be deprecated in a 1.x release.\n        Please use `pd.to_datetime` instead.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\"date\": [1651510462, 53394822, 1126233195]})\n        &gt;&gt;&gt; df\n                 date\n        0  1651510462\n        1    53394822\n        2  1126233195\n        &gt;&gt;&gt; df.convert_unix_date('date')\n                         date\n        0 2022-05-02 16:54:22\n        1 1971-09-10 23:53:42\n        2 2005-09-09 02:33:15\n\n    Args:\n        df: A pandas DataFrame.\n        column_name: A column name.\n\n    Returns:\n        A pandas DataFrame with corrected dates.\n    \"\"\"\n\n    try:\n        df[column_name] = pd.to_datetime(df[column_name], unit=\"s\")\n    except OutOfBoundsDatetime:  # Indicates time is in milliseconds.\n        df[column_name] = pd.to_datetime(df[column_name], unit=\"ms\")\n    return df\n</code></pre>"},{"location":"api/functions/#janitor.functions.count_cumulative_unique","title":"<code>count_cumulative_unique</code>","text":"<p>Implementation of count_cumulative_unique.</p>"},{"location":"api/functions/#janitor.functions.count_cumulative_unique.count_cumulative_unique","title":"<code>count_cumulative_unique(df, column_name, dest_column_name, case_sensitive=True)</code>","text":"<p>Generates a running total of cumulative unique values in a given column.</p> <p>A new column will be created containing a running count of unique values in the specified column. If <code>case_sensitive</code> is <code>True</code>, then the case of any letters will matter (i.e., <code>a != A</code>); otherwise, the case of any letters will not matter.</p> <p>This method does not mutate the original DataFrame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"letters\": list(\"aabABb\"),\n...     \"numbers\": range(4, 10),\n... })\n&gt;&gt;&gt; df\n  letters  numbers\n0       a        4\n1       a        5\n2       b        6\n3       A        7\n4       B        8\n5       b        9\n&gt;&gt;&gt; df.count_cumulative_unique(\n...     column_name=\"letters\",\n...     dest_column_name=\"letters_unique_count\",\n... )\n  letters  numbers  letters_unique_count\n0       a        4                     1\n1       a        5                     1\n2       b        6                     2\n3       A        7                     3\n4       B        8                     4\n5       b        9                     4\n</code></pre> <p>Cumulative counts, ignoring casing.</p> <pre><code>&gt;&gt;&gt; df.count_cumulative_unique(\n...     column_name=\"letters\",\n...     dest_column_name=\"letters_unique_count\",\n...     case_sensitive=False,\n... )\n  letters  numbers  letters_unique_count\n0       a        4                     1\n1       a        5                     1\n2       b        6                     2\n3       A        7                     2\n4       B        8                     2\n5       b        9                     2\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>column_name</code> <code>Hashable</code> <p>Name of the column containing values from which a running count of unique values will be created.</p> required <code>dest_column_name</code> <code>str</code> <p>The name of the new column containing the cumulative count of unique values that will be created.</p> required <code>case_sensitive</code> <code>bool</code> <p>Whether or not uppercase and lowercase letters will be considered equal. Only valid with string-like columns.</p> <code>True</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>case_sensitive</code> is False when counting a non-string <code>column_name</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with a new column containing a cumulative count of unique values from another column.</p> Source code in <code>janitor/functions/count_cumulative_unique.py</code> <pre><code>@pf.register_dataframe_method\ndef count_cumulative_unique(\n    df: pd.DataFrame,\n    column_name: Hashable,\n    dest_column_name: str,\n    case_sensitive: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"Generates a running total of cumulative unique values in a given column.\n\n    A new column will be created containing a running\n    count of unique values in the specified column.\n    If `case_sensitive` is `True`, then the case of\n    any letters will matter (i.e., `a != A`);\n    otherwise, the case of any letters will not matter.\n\n    This method does not mutate the original DataFrame.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     \"letters\": list(\"aabABb\"),\n        ...     \"numbers\": range(4, 10),\n        ... })\n        &gt;&gt;&gt; df\n          letters  numbers\n        0       a        4\n        1       a        5\n        2       b        6\n        3       A        7\n        4       B        8\n        5       b        9\n        &gt;&gt;&gt; df.count_cumulative_unique(\n        ...     column_name=\"letters\",\n        ...     dest_column_name=\"letters_unique_count\",\n        ... )\n          letters  numbers  letters_unique_count\n        0       a        4                     1\n        1       a        5                     1\n        2       b        6                     2\n        3       A        7                     3\n        4       B        8                     4\n        5       b        9                     4\n\n        Cumulative counts, ignoring casing.\n\n        &gt;&gt;&gt; df.count_cumulative_unique(\n        ...     column_name=\"letters\",\n        ...     dest_column_name=\"letters_unique_count\",\n        ...     case_sensitive=False,\n        ... )\n          letters  numbers  letters_unique_count\n        0       a        4                     1\n        1       a        5                     1\n        2       b        6                     2\n        3       A        7                     2\n        4       B        8                     2\n        5       b        9                     2\n\n    Args:\n        df: A pandas DataFrame.\n        column_name: Name of the column containing values from which a\n            running count of unique values will be created.\n        dest_column_name: The name of the new column containing the\n            cumulative count of unique values that will be created.\n        case_sensitive: Whether or not uppercase and lowercase letters\n            will be considered equal. Only valid with string-like columns.\n\n    Raises:\n        TypeError: If `case_sensitive` is False when counting a non-string\n            `column_name`.\n\n    Returns:\n        A pandas DataFrame with a new column containing a cumulative\n            count of unique values from another column.\n    \"\"\"\n    check_column(df, column_name)\n    check_column(df, dest_column_name, present=False)\n\n    counter = df[column_name]\n    if not case_sensitive:\n        try:\n            # Make it so that the the same uppercase and lowercase\n            # letter are treated as one unique value\n            counter = counter.str.lower()\n        except (AttributeError, TypeError) as e:\n            # AttributeError is raised by pandas when .str is used on\n            # non-string types, e.g. int.\n            # TypeError is raised by pandas when .str.lower is used on a\n            # forbidden string type, e.g. bytes.\n            raise TypeError(\n                \"case_sensitive=False can only be used with a string-like \"\n                f\"type. Column {column_name} is {counter.dtype} type.\"\n            ) from e\n\n    counter = (\n        counter.groupby(counter, sort=False).cumcount().to_numpy(copy=False)\n    )\n    counter = np.cumsum(counter == 0)\n\n    return df.assign(**{dest_column_name: counter})\n</code></pre>"},{"location":"api/functions/#janitor.functions.currency_column_to_numeric","title":"<code>currency_column_to_numeric</code>","text":""},{"location":"api/functions/#janitor.functions.currency_column_to_numeric.currency_column_to_numeric","title":"<code>currency_column_to_numeric(df, column_name, cleaning_style=None, cast_non_numeric=None, fill_all_non_numeric=None, remove_non_numeric=False)</code>","text":"<p>Convert currency column to numeric.</p> <p>This method does not mutate the original DataFrame.</p> <p>This method allows one to take a column containing currency values, inadvertently imported as a string, and cast it as a float. This is usually the case when reading CSV files that were modified in Excel. Empty strings (i.e. <code>''</code>) are retained as <code>NaN</code> values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"a_col\": [\" 24.56\", \"-\", \"(12.12)\", \"1,000,000\"],\n...     \"d_col\": [\"\", \"foo\", \"1.23 dollars\", \"-1,000 yen\"],\n... })\n&gt;&gt;&gt; df\n       a_col         d_col\n0      24.56\n1          -           foo\n2    (12.12)  1.23 dollars\n3  1,000,000    -1,000 yen\n</code></pre> <p>The default cleaning style.</p> <pre><code>&gt;&gt;&gt; df.currency_column_to_numeric(\"d_col\")\n       a_col    d_col\n0      24.56      NaN\n1          -      NaN\n2    (12.12)     1.23\n3  1,000,000 -1000.00\n</code></pre> <p>The accounting cleaning style.</p> <pre><code>&gt;&gt;&gt; df.currency_column_to_numeric(\"a_col\", cleaning_style=\"accounting\")\n        a_col         d_col\n0       24.56\n1        0.00           foo\n2      -12.12  1.23 dollars\n3  1000000.00    -1,000 yen\n</code></pre> <p>Valid cleaning styles are:</p> <ul> <li><code>None</code>: Default cleaning is applied. Empty strings are always retained as     <code>NaN</code>. Numbers, <code>-</code>, <code>.</code> are extracted and the resulting string     is cast to a float.</li> <li><code>'accounting'</code>: Replaces numbers in parentheses with negatives, removes commas.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The pandas DataFrame.</p> required <code>column_name</code> <code>str</code> <p>The column containing currency values to modify.</p> required <code>cleaning_style</code> <code>Optional[str]</code> <p>What style of cleaning to perform.</p> <code>None</code> <code>cast_non_numeric</code> <code>Optional[dict]</code> <p>A dict of how to coerce certain strings to numeric type. For example, if there are values of 'REORDER' in the DataFrame, <code>{'REORDER': 0}</code> will cast all instances of 'REORDER' to 0. Only takes effect in the default cleaning style.</p> <code>None</code> <code>fill_all_non_numeric</code> <code>Optional[Union[float, int]]</code> <p>Similar to <code>cast_non_numeric</code>, but fills all strings to the same value. For example, <code>fill_all_non_numeric=1</code>, will make everything that doesn't coerce to a currency <code>1</code>. Only takes effect in the default cleaning style.</p> <code>None</code> <code>remove_non_numeric</code> <code>bool</code> <p>If set to True, rows of <code>df</code> that contain non-numeric values in the <code>column_name</code> column will be removed. Only takes effect in the default cleaning style.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>cleaning_style</code> is not one of the accepted styles.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame.</p> Source code in <code>janitor/functions/currency_column_to_numeric.py</code> <pre><code>@pf.register_dataframe_method\n@deprecated_alias(col_name=\"column_name\", type=\"cleaning_style\")\ndef currency_column_to_numeric(\n    df: pd.DataFrame,\n    column_name: str,\n    cleaning_style: Optional[str] = None,\n    cast_non_numeric: Optional[dict] = None,\n    fill_all_non_numeric: Optional[Union[float, int]] = None,\n    remove_non_numeric: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Convert currency column to numeric.\n\n    This method does not mutate the original DataFrame.\n\n    This method allows one to take a column containing currency values,\n    inadvertently imported as a string, and cast it as a float. This is\n    usually the case when reading CSV files that were modified in Excel.\n    Empty strings (i.e. `''`) are retained as `NaN` values.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     \"a_col\": [\" 24.56\", \"-\", \"(12.12)\", \"1,000,000\"],\n        ...     \"d_col\": [\"\", \"foo\", \"1.23 dollars\", \"-1,000 yen\"],\n        ... })\n        &gt;&gt;&gt; df  # doctest: +NORMALIZE_WHITESPACE\n               a_col         d_col\n        0      24.56\n        1          -           foo\n        2    (12.12)  1.23 dollars\n        3  1,000,000    -1,000 yen\n\n        The default cleaning style.\n\n        &gt;&gt;&gt; df.currency_column_to_numeric(\"d_col\")\n               a_col    d_col\n        0      24.56      NaN\n        1          -      NaN\n        2    (12.12)     1.23\n        3  1,000,000 -1000.00\n\n        The accounting cleaning style.\n\n        &gt;&gt;&gt; df.currency_column_to_numeric(\"a_col\", cleaning_style=\"accounting\")  # doctest: +NORMALIZE_WHITESPACE\n                a_col         d_col\n        0       24.56\n        1        0.00           foo\n        2      -12.12  1.23 dollars\n        3  1000000.00    -1,000 yen\n\n    Valid cleaning styles are:\n\n    - `None`: Default cleaning is applied. Empty strings are always retained as\n        `NaN`. Numbers, `-`, `.` are extracted and the resulting string\n        is cast to a float.\n    - `'accounting'`: Replaces numbers in parentheses with negatives, removes commas.\n\n    Args:\n        df: The pandas DataFrame.\n        column_name: The column containing currency values to modify.\n        cleaning_style: What style of cleaning to perform.\n        cast_non_numeric: A dict of how to coerce certain strings to numeric\n            type. For example, if there are values of 'REORDER' in the DataFrame,\n            `{'REORDER': 0}` will cast all instances of 'REORDER' to 0.\n            Only takes effect in the default cleaning style.\n        fill_all_non_numeric: Similar to `cast_non_numeric`, but fills all\n            strings to the same value. For example, `fill_all_non_numeric=1`, will\n            make everything that doesn't coerce to a currency `1`.\n            Only takes effect in the default cleaning style.\n        remove_non_numeric: If set to True, rows of `df` that contain\n            non-numeric values in the `column_name` column will be removed.\n            Only takes effect in the default cleaning style.\n\n    Raises:\n        ValueError: If `cleaning_style` is not one of the accepted styles.\n\n    Returns:\n        A pandas DataFrame.\n    \"\"\"  # noqa: E501\n\n    check(\"column_name\", column_name, [str])\n    check_column(df, column_name)\n\n    column_series = df[column_name]\n    if cleaning_style == \"accounting\":\n        outcome = (\n            df[column_name]\n            .str.strip()\n            .str.replace(\",\", \"\", regex=False)\n            .str.replace(\")\", \"\", regex=False)\n            .str.replace(\"(\", \"-\", regex=False)\n            .replace({\"-\": 0.0})\n            .astype(float)\n        )\n        return df.assign(**{column_name: outcome})\n    if cleaning_style is not None:\n        raise ValueError(\n            \"`cleaning_style` is expected to be one of ('accounting', None). \"\n            f\"Got {cleaning_style!r} instead.\"\n        )\n\n    if cast_non_numeric:\n        check(\"cast_non_numeric\", cast_non_numeric, [dict])\n\n    _make_cc_patrial = partial(\n        _currency_column_to_numeric,\n        cast_non_numeric=cast_non_numeric,\n    )\n    column_series = column_series.apply(_make_cc_patrial)\n\n    if remove_non_numeric:\n        df = df.loc[column_series != \"\", :]\n\n    # _replace_empty_string_with_none is applied here after the check on\n    # remove_non_numeric since \"\" is our indicator that a string was coerced\n    # in the original column\n    column_series = _replace_empty_string_with_none(column_series)\n\n    if fill_all_non_numeric is not None:\n        check(\"fill_all_non_numeric\", fill_all_non_numeric, [int, float])\n        column_series = column_series.fillna(fill_all_non_numeric)\n\n    column_series = _replace_original_empty_string_with_none(column_series)\n\n    df = df.assign(**{column_name: pd.to_numeric(column_series)})\n\n    return df\n</code></pre>"},{"location":"api/functions/#janitor.functions.deconcatenate_column","title":"<code>deconcatenate_column</code>","text":"<p>Implementation of deconcatenating columns.</p>"},{"location":"api/functions/#janitor.functions.deconcatenate_column.deconcatenate_column","title":"<code>deconcatenate_column(df, column_name, sep=None, new_column_names=None, autoname=None, preserve_position=False)</code>","text":"<p>De-concatenates a single column into multiple columns.</p> <p>The column to de-concatenate can be either a collection (list, tuple, ...) which can be separated out with <code>pd.Series.tolist()</code>, or a string to slice based on <code>sep</code>.</p> <p>To determine this behaviour automatically, the first element in the column specified is inspected.</p> <p>If it is a string, then <code>sep</code> must be specified. Else, the function assumes that it is an iterable type (e.g. <code>list</code> or <code>tuple</code>), and will attempt to deconcatenate by splitting the list.</p> <p>Given a column with string values, this is the inverse of the <code>concatenate_columns</code> function.</p> <p>Used to quickly split columns out of a single column.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\"m\": [\"1-x\", \"2-y\", \"3-z\"]})\n&gt;&gt;&gt; df\n     m\n0  1-x\n1  2-y\n2  3-z\n&gt;&gt;&gt; df.deconcatenate_column(\"m\", sep=\"-\", autoname=\"col\")\n     m col1 col2\n0  1-x    1    x\n1  2-y    2    y\n2  3-z    3    z\n</code></pre> <p>The keyword argument <code>preserve_position</code> takes <code>True</code> or <code>False</code> boolean that controls whether the <code>new_column_names</code> will take the original position of the to-be-deconcatenated <code>column_name</code>:</p> <ul> <li>When <code>preserve_position=False</code> (default), <code>df.columns</code> change from   <code>[..., column_name, ...]</code> to <code>[..., column_name, ..., new_column_names]</code>.   In other words, the deconcatenated new columns are appended to the right   of the original dataframe and the original <code>column_name</code> is NOT dropped.</li> <li>When <code>preserve_position=True</code>, <code>df.column</code> change from   <code>[..., column_name, ...]</code> to <code>[..., new_column_names, ...]</code>.   In other words, the deconcatenated new column will REPLACE the original   <code>column_name</code> at its original position, and <code>column_name</code> itself   is dropped.</li> </ul> <p>The keyword argument <code>autoname</code> accepts a base string and then automatically creates numbered column names based off the base string. For example, if <code>col</code> is passed in as the argument to <code>autoname</code>, and 4 columns are created, then the resulting columns will be named <code>col1, col2, col3, col4</code>. Numbering is always 1-indexed, not 0-indexed, in order to make the column names human-friendly.</p> <p>This method does not mutate the original DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>column_name</code> <code>Hashable</code> <p>The column to split.</p> required <code>sep</code> <code>Optional[str]</code> <p>The separator delimiting the column's data.</p> <code>None</code> <code>new_column_names</code> <code>Optional[Union[List[str], Tuple[str]]]</code> <p>A list of new column names post-splitting.</p> <code>None</code> <code>autoname</code> <code>str</code> <p>A base name for automatically naming the new columns. Takes precedence over <code>new_column_names</code> if both are provided.</p> <code>None</code> <code>preserve_position</code> <code>bool</code> <p>Boolean for whether or not to preserve original position of the column upon de-concatenation.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>column_name</code> is not present in the DataFrame.</p> <code>ValueError</code> <p>If <code>sep</code> is not provided and the column values are of type <code>str</code>.</p> <code>ValueError</code> <p>If either <code>new_column_names</code> or <code>autoname</code> is not supplied.</p> <code>JanitorError</code> <p>If incorrect number of names is provided within <code>new_column_names</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with a deconcatenated column.</p> Source code in <code>janitor/functions/deconcatenate_column.py</code> <pre><code>@pf.register_dataframe_method\n@deprecated_alias(column=\"column_name\")\ndef deconcatenate_column(\n    df: pd.DataFrame,\n    column_name: Hashable,\n    sep: Optional[str] = None,\n    new_column_names: Optional[Union[List[str], Tuple[str]]] = None,\n    autoname: str = None,\n    preserve_position: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"De-concatenates a single column into multiple columns.\n\n    The column to de-concatenate can be either a collection (list, tuple, ...)\n    which can be separated out with `pd.Series.tolist()`,\n    or a string to slice based on `sep`.\n\n    To determine this behaviour automatically,\n    the first element in the column specified is inspected.\n\n    If it is a string, then `sep` must be specified.\n    Else, the function assumes that it is an iterable type\n    (e.g. `list` or `tuple`),\n    and will attempt to deconcatenate by splitting the list.\n\n    Given a column with string values, this is the inverse of the\n    [`concatenate_columns`][janitor.functions.concatenate_columns.concatenate_columns]\n    function.\n\n    Used to quickly split columns out of a single column.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\"m\": [\"1-x\", \"2-y\", \"3-z\"]})\n        &gt;&gt;&gt; df\n             m\n        0  1-x\n        1  2-y\n        2  3-z\n        &gt;&gt;&gt; df.deconcatenate_column(\"m\", sep=\"-\", autoname=\"col\")\n             m col1 col2\n        0  1-x    1    x\n        1  2-y    2    y\n        2  3-z    3    z\n\n    The keyword argument `preserve_position`\n    takes `True` or `False` boolean\n    that controls whether the `new_column_names`\n    will take the original position\n    of the to-be-deconcatenated `column_name`:\n\n    - When `preserve_position=False` (default), `df.columns` change from\n      `[..., column_name, ...]` to `[..., column_name, ..., new_column_names]`.\n      In other words, the deconcatenated new columns are appended to the right\n      of the original dataframe and the original `column_name` is NOT dropped.\n    - When `preserve_position=True`, `df.column` change from\n      `[..., column_name, ...]` to `[..., new_column_names, ...]`.\n      In other words, the deconcatenated new column will REPLACE the original\n      `column_name` at its original position, and `column_name` itself\n      is dropped.\n\n    The keyword argument `autoname` accepts a base string\n    and then automatically creates numbered column names\n    based off the base string.\n    For example, if `col` is passed in as the argument to `autoname`,\n    and 4 columns are created, then the resulting columns will be named\n    `col1, col2, col3, col4`.\n    Numbering is always 1-indexed, not 0-indexed,\n    in order to make the column names human-friendly.\n\n    This method does not mutate the original DataFrame.\n\n    Args:\n        df: A pandas DataFrame.\n        column_name: The column to split.\n        sep: The separator delimiting the column's data.\n        new_column_names: A list of new column names post-splitting.\n        autoname: A base name for automatically naming the new columns.\n            Takes precedence over `new_column_names` if both are provided.\n        preserve_position: Boolean for whether or not to preserve original\n            position of the column upon de-concatenation.\n\n    Raises:\n        ValueError: If `column_name` is not present in the DataFrame.\n        ValueError: If `sep` is not provided and the column values\n            are of type `str`.\n        ValueError: If either `new_column_names` or `autoname`\n            is not supplied.\n        JanitorError: If incorrect number of names is provided\n            within `new_column_names`.\n\n    Returns:\n        A pandas DataFrame with a deconcatenated column.\n    \"\"\"  # noqa: E501\n\n    if column_name not in df.columns:\n        raise ValueError(f\"column name {column_name} not present in DataFrame\")\n\n    if isinstance(df[column_name].iloc[0], str):\n        if sep is None:\n            raise ValueError(\n                \"`sep` must be specified if the column values \"\n                \"are of type `str`.\"\n            )\n        df_deconcat = df[column_name].str.split(sep, expand=True)\n    else:\n        df_deconcat = pd.DataFrame(\n            df[column_name].to_list(), columns=new_column_names, index=df.index\n        )\n\n    if new_column_names is None and autoname is None:\n        raise ValueError(\n            \"One of `new_column_names` or `autoname` must be supplied.\"\n        )\n\n    if autoname:\n        new_column_names = [\n            f\"{autoname}{i}\" for i in range(1, df_deconcat.shape[1] + 1)\n        ]\n\n    if not len(new_column_names) == df_deconcat.shape[1]:\n        raise JanitorError(\n            f\"You need to provide {len(df_deconcat.shape[1])} names \"\n            \"to `new_column_names`\"\n        )\n\n    df_deconcat.columns = new_column_names\n    df_new = pd.concat([df, df_deconcat], axis=1)\n\n    if preserve_position:\n        df_original = df.copy()\n        cols = list(df_original.columns)\n        index_original = cols.index(column_name)\n\n        for i, col_new in enumerate(new_column_names):\n            cols.insert(index_original + i, col_new)\n\n        df_new = df_new.select(cols, axis=\"columns\").drop(columns=column_name)\n\n    return df_new\n</code></pre>"},{"location":"api/functions/#janitor.functions.drop_constant_columns","title":"<code>drop_constant_columns</code>","text":"<p>Implementation of drop_constant_columns.</p>"},{"location":"api/functions/#janitor.functions.drop_constant_columns.drop_constant_columns","title":"<code>drop_constant_columns(df)</code>","text":"<p>Finds and drops the constant columns from a Pandas DataFrame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; data_dict = {\n...     \"a\": [1, 1, 1],\n...     \"b\": [1, 2, 3],\n...     \"c\": [1, 1, 1],\n...     \"d\": [\"rabbit\", \"leopard\", \"lion\"],\n...     \"e\": [\"Cambridge\", \"Shanghai\", \"Basel\"]\n... }\n&gt;&gt;&gt; df = pd.DataFrame(data_dict)\n&gt;&gt;&gt; df\n   a  b  c        d          e\n0  1  1  1   rabbit  Cambridge\n1  1  2  1  leopard   Shanghai\n2  1  3  1     lion      Basel\n&gt;&gt;&gt; df.drop_constant_columns()\n   b        d          e\n0  1   rabbit  Cambridge\n1  2  leopard   Shanghai\n2  3     lion      Basel\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input Pandas DataFrame</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>The Pandas DataFrame with the constant columns dropped.</p> Source code in <code>janitor/functions/drop_constant_columns.py</code> <pre><code>@pf.register_dataframe_method\ndef drop_constant_columns(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Finds and drops the constant columns from a Pandas DataFrame.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; data_dict = {\n        ...     \"a\": [1, 1, 1],\n        ...     \"b\": [1, 2, 3],\n        ...     \"c\": [1, 1, 1],\n        ...     \"d\": [\"rabbit\", \"leopard\", \"lion\"],\n        ...     \"e\": [\"Cambridge\", \"Shanghai\", \"Basel\"]\n        ... }\n        &gt;&gt;&gt; df = pd.DataFrame(data_dict)\n        &gt;&gt;&gt; df\n           a  b  c        d          e\n        0  1  1  1   rabbit  Cambridge\n        1  1  2  1  leopard   Shanghai\n        2  1  3  1     lion      Basel\n        &gt;&gt;&gt; df.drop_constant_columns()\n           b        d          e\n        0  1   rabbit  Cambridge\n        1  2  leopard   Shanghai\n        2  3     lion      Basel\n\n    Args:\n        df: Input Pandas DataFrame\n\n    Returns:\n        The Pandas DataFrame with the constant columns dropped.\n    \"\"\"\n    return df.loc[:, df.nunique().ne(1)]\n</code></pre>"},{"location":"api/functions/#janitor.functions.drop_duplicate_columns","title":"<code>drop_duplicate_columns</code>","text":"<p>Implementation for <code>drop_duplicate_columns</code>.</p>"},{"location":"api/functions/#janitor.functions.drop_duplicate_columns.drop_duplicate_columns","title":"<code>drop_duplicate_columns(df, column_name, nth_index=0)</code>","text":"<p>Remove a duplicated column specified by <code>column_name</code>.</p> <p>Specifying <code>nth_index=0</code> will remove the first column, <code>nth_index=1</code> will remove the second column, and so on and so forth.</p> <p>The corresponding tidyverse R's library is: <code>select(-&lt;column_name&gt;_&lt;nth_index + 1&gt;)</code></p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"a\": range(2, 5),\n...     \"b\": range(3, 6),\n...     \"A\": range(4, 7),\n...     \"a*\": range(6, 9),\n... }).clean_names(remove_special=True)\n&gt;&gt;&gt; df\n   a  b  a  a\n0  2  3  4  6\n1  3  4  5  7\n2  4  5  6  8\n&gt;&gt;&gt; df.drop_duplicate_columns(column_name=\"a\", nth_index=1)\n   a  b  a\n0  2  3  6\n1  3  4  7\n2  4  5  8\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame</p> required <code>column_name</code> <code>Hashable</code> <p>Name of duplicated columns.</p> required <code>nth_index</code> <code>int</code> <p>Among the duplicated columns, select the nth column to drop.</p> <code>0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame</p> Source code in <code>janitor/functions/drop_duplicate_columns.py</code> <pre><code>@pf.register_dataframe_method\ndef drop_duplicate_columns(\n    df: pd.DataFrame, column_name: Hashable, nth_index: int = 0\n) -&gt; pd.DataFrame:\n    \"\"\"Remove a duplicated column specified by `column_name`.\n\n    Specifying `nth_index=0` will remove the first column,\n    `nth_index=1` will remove the second column,\n    and so on and so forth.\n\n    The corresponding tidyverse R's library is:\n    `select(-&lt;column_name&gt;_&lt;nth_index + 1&gt;)`\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     \"a\": range(2, 5),\n        ...     \"b\": range(3, 6),\n        ...     \"A\": range(4, 7),\n        ...     \"a*\": range(6, 9),\n        ... }).clean_names(remove_special=True)\n        &gt;&gt;&gt; df\n           a  b  a  a\n        0  2  3  4  6\n        1  3  4  5  7\n        2  4  5  6  8\n        &gt;&gt;&gt; df.drop_duplicate_columns(column_name=\"a\", nth_index=1)\n           a  b  a\n        0  2  3  6\n        1  3  4  7\n        2  4  5  8\n\n    Args:\n        df: A pandas DataFrame\n        column_name: Name of duplicated columns.\n        nth_index: Among the duplicated columns,\n            select the nth column to drop.\n\n    Returns:\n        A pandas DataFrame\n    \"\"\"\n    col_indexes = [\n        col_idx\n        for col_idx, col_name in enumerate(df.columns)\n        if col_name == column_name\n    ]\n\n    # Select the column to remove based on nth_index.\n    removed_col_idx = col_indexes[nth_index]\n    # Filter out columns except for the one to be removed.\n    filtered_cols = [\n        c_i for c_i, _ in enumerate(df.columns) if c_i != removed_col_idx\n    ]\n\n    return df.iloc[:, filtered_cols]\n</code></pre>"},{"location":"api/functions/#janitor.functions.dropnotnull","title":"<code>dropnotnull</code>","text":"<p>Implementation source for <code>dropnotnull</code>.</p>"},{"location":"api/functions/#janitor.functions.dropnotnull.dropnotnull","title":"<code>dropnotnull(df, column_name)</code>","text":"<p>Drop rows that do not have null values in the given column.</p> <p>This method does not mutate the original DataFrame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\"a\": [1., np.NaN, 3.], \"b\": [None, \"y\", \"z\"]})\n&gt;&gt;&gt; df\n     a     b\n0  1.0  None\n1  NaN     y\n2  3.0     z\n&gt;&gt;&gt; df.dropnotnull(\"a\")\n    a  b\n1 NaN  y\n&gt;&gt;&gt; df.dropnotnull(\"b\")\n     a     b\n0  1.0  None\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>column_name</code> <code>Hashable</code> <p>The column name to drop rows from.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with dropped rows.</p> Source code in <code>janitor/functions/dropnotnull.py</code> <pre><code>@pf.register_dataframe_method\n@deprecated_alias(column=\"column_name\")\ndef dropnotnull(df: pd.DataFrame, column_name: Hashable) -&gt; pd.DataFrame:\n    \"\"\"Drop rows that do *not* have null values in the given column.\n\n    This method does not mutate the original DataFrame.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\"a\": [1., np.NaN, 3.], \"b\": [None, \"y\", \"z\"]})\n        &gt;&gt;&gt; df\n             a     b\n        0  1.0  None\n        1  NaN     y\n        2  3.0     z\n        &gt;&gt;&gt; df.dropnotnull(\"a\")\n            a  b\n        1 NaN  y\n        &gt;&gt;&gt; df.dropnotnull(\"b\")\n             a     b\n        0  1.0  None\n\n    Args:\n        df: A pandas DataFrame.\n        column_name: The column name to drop rows from.\n\n    Returns:\n        A pandas DataFrame with dropped rows.\n    \"\"\"\n    return df[pd.isna(df[column_name])]\n</code></pre>"},{"location":"api/functions/#janitor.functions.encode_categorical","title":"<code>encode_categorical</code>","text":""},{"location":"api/functions/#janitor.functions.encode_categorical.encode_categorical","title":"<code>encode_categorical(df, column_names=None, **kwargs)</code>","text":"<p>Encode the specified columns with Pandas' category dtype.</p> <p>It is syntactic sugar around <code>pd.Categorical</code>.</p> <p>This method does not mutate the original DataFrame.</p> <p>Simply pass a string, or a sequence of column names to <code>column_names</code>; alternatively, you can pass kwargs, where the keys are the column names and the values can either be None, <code>sort</code>, <code>appearance</code> or a 1-D array-like object.</p> <ul> <li>None: column is cast to an unordered categorical.</li> <li><code>sort</code>: column is cast to an ordered categorical,           with the order defined by the sort-order of the categories.</li> <li><code>appearance</code>: column is cast to an ordered categorical,                 with the order defined by the order of appearance                 in the original column.</li> <li>1d-array-like object: column is cast to an ordered categorical,                         with the categories and order as specified                         in the input array.</li> </ul> <p><code>column_names</code> and <code>kwargs</code> parameters cannot be used at the same time.</p> <p>Examples:</p> <p>Using <code>column_names</code></p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"foo\": [\"b\", \"b\", \"a\", \"c\", \"b\"],\n...     \"bar\": range(4, 9),\n... })\n&gt;&gt;&gt; df\n  foo  bar\n0   b    4\n1   b    5\n2   a    6\n3   c    7\n4   b    8\n&gt;&gt;&gt; df.dtypes\nfoo    object\nbar     int64\ndtype: object\n&gt;&gt;&gt; enc_df = df.encode_categorical(column_names=\"foo\")\n&gt;&gt;&gt; enc_df.dtypes\nfoo    category\nbar       int64\ndtype: object\n&gt;&gt;&gt; enc_df[\"foo\"].cat.categories\nIndex(['a', 'b', 'c'], dtype='object')\n&gt;&gt;&gt; enc_df[\"foo\"].cat.ordered\nFalse\n</code></pre> <p>Using <code>kwargs</code> to specify an ordered categorical.</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"foo\": [\"b\", \"b\", \"a\", \"c\", \"b\"],\n...     \"bar\": range(4, 9),\n... })\n&gt;&gt;&gt; df.dtypes\nfoo    object\nbar     int64\ndtype: object\n&gt;&gt;&gt; enc_df = df.encode_categorical(foo=\"appearance\")\n&gt;&gt;&gt; enc_df.dtypes\nfoo    category\nbar       int64\ndtype: object\n&gt;&gt;&gt; enc_df[\"foo\"].cat.categories\nIndex(['b', 'a', 'c'], dtype='object')\n&gt;&gt;&gt; enc_df[\"foo\"].cat.ordered\nTrue\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame object.</p> required <code>column_names</code> <code>Union[str, Iterable[str], Hashable]</code> <p>A column name or an iterable (list or tuple) of column names.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>A mapping from column name to either <code>None</code>, <code>'sort'</code> or <code>'appearance'</code>, or a 1-D array. This is useful in creating categorical columns that are ordered, or if the user needs to explicitly specify the categories.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both <code>column_names</code> and <code>kwargs</code> are provided.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame.</p> Source code in <code>janitor/functions/encode_categorical.py</code> <pre><code>@pf.register_dataframe_method\n@deprecated_alias(columns=\"column_names\")\ndef encode_categorical(\n    df: pd.DataFrame,\n    column_names: Union[str, Iterable[str], Hashable] = None,\n    **kwargs: Any,\n) -&gt; pd.DataFrame:\n    \"\"\"Encode the specified columns with Pandas' [category dtype][cat].\n\n    [cat]: http://pandas.pydata.org/pandas-docs/stable/user_guide/categorical.html\n\n    It is syntactic sugar around `pd.Categorical`.\n\n    This method does not mutate the original DataFrame.\n\n    Simply pass a string, or a sequence of column names to `column_names`;\n    alternatively, you can pass kwargs, where the keys are the column names\n    and the values can either be None, `sort`, `appearance`\n    or a 1-D array-like object.\n\n    - None: column is cast to an unordered categorical.\n    - `sort`: column is cast to an ordered categorical,\n              with the order defined by the sort-order of the categories.\n    - `appearance`: column is cast to an ordered categorical,\n                    with the order defined by the order of appearance\n                    in the original column.\n    - 1d-array-like object: column is cast to an ordered categorical,\n                            with the categories and order as specified\n                            in the input array.\n\n    `column_names` and `kwargs` parameters cannot be used at the same time.\n\n    Examples:\n        Using `column_names`\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     \"foo\": [\"b\", \"b\", \"a\", \"c\", \"b\"],\n        ...     \"bar\": range(4, 9),\n        ... })\n        &gt;&gt;&gt; df\n          foo  bar\n        0   b    4\n        1   b    5\n        2   a    6\n        3   c    7\n        4   b    8\n        &gt;&gt;&gt; df.dtypes\n        foo    object\n        bar     int64\n        dtype: object\n        &gt;&gt;&gt; enc_df = df.encode_categorical(column_names=\"foo\")\n        &gt;&gt;&gt; enc_df.dtypes\n        foo    category\n        bar       int64\n        dtype: object\n        &gt;&gt;&gt; enc_df[\"foo\"].cat.categories\n        Index(['a', 'b', 'c'], dtype='object')\n        &gt;&gt;&gt; enc_df[\"foo\"].cat.ordered\n        False\n\n        Using `kwargs` to specify an ordered categorical.\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     \"foo\": [\"b\", \"b\", \"a\", \"c\", \"b\"],\n        ...     \"bar\": range(4, 9),\n        ... })\n        &gt;&gt;&gt; df.dtypes\n        foo    object\n        bar     int64\n        dtype: object\n        &gt;&gt;&gt; enc_df = df.encode_categorical(foo=\"appearance\")\n        &gt;&gt;&gt; enc_df.dtypes\n        foo    category\n        bar       int64\n        dtype: object\n        &gt;&gt;&gt; enc_df[\"foo\"].cat.categories\n        Index(['b', 'a', 'c'], dtype='object')\n        &gt;&gt;&gt; enc_df[\"foo\"].cat.ordered\n        True\n\n    Args:\n        df: A pandas DataFrame object.\n        column_names: A column name or an iterable (list or tuple)\n            of column names.\n        **kwargs: A mapping from column name to either `None`,\n            `'sort'` or `'appearance'`, or a 1-D array. This is useful\n            in creating categorical columns that are ordered, or\n            if the user needs to explicitly specify the categories.\n\n    Raises:\n        ValueError: If both `column_names` and `kwargs` are provided.\n\n    Returns:\n        A pandas DataFrame.\n    \"\"\"  # noqa: E501\n\n    if all((column_names, kwargs)):\n        raise ValueError(\n            \"Only one of `column_names` or `kwargs` can be provided.\"\n        )\n    # column_names deal with only category dtype (unordered)\n    # kwargs takes care of scenarios where user wants an ordered category\n    # or user supplies specific categories to create the categorical\n    if column_names is not None:\n        column_names = get_index_labels([column_names], df, axis=\"columns\")\n        dtypes = {col: \"category\" for col in column_names}\n        return df.astype(dtypes)\n\n    return _computations_as_categorical(df, **kwargs)\n</code></pre>"},{"location":"api/functions/#janitor.functions.expand_column","title":"<code>expand_column</code>","text":"<p>Implementation for expand_column.</p>"},{"location":"api/functions/#janitor.functions.expand_column.expand_column","title":"<code>expand_column(df, column_name, sep='|', concat=True)</code>","text":"<p>Expand a categorical column with multiple labels into dummy-coded columns.</p> <p>Super sugary syntax that wraps <code>pandas.Series.str.get_dummies</code>.</p> <p>This method does not mutate the original DataFrame.</p> <p>Examples:</p> <p>Functional usage syntax:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame(\n...     {\n...         \"col1\": [\"A, B\", \"B, C, D\", \"E, F\", \"A, E, F\"],\n...         \"col2\": [1, 2, 3, 4],\n...     }\n... )\n&gt;&gt;&gt; df = expand_column(\n...     df,\n...     column_name=\"col1\",\n...     sep=\", \"  # note space in sep\n... )\n&gt;&gt;&gt; df\n      col1  col2  A  B  C  D  E  F\n0     A, B     1  1  1  0  0  0  0\n1  B, C, D     2  0  1  1  1  0  0\n2     E, F     3  0  0  0  0  1  1\n3  A, E, F     4  1  0  0  0  1  1\n</code></pre> <p>Method chaining syntax:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = (\n...     pd.DataFrame(\n...         {\n...             \"col1\": [\"A, B\", \"B, C, D\", \"E, F\", \"A, E, F\"],\n...             \"col2\": [1, 2, 3, 4],\n...         }\n...     )\n...     .expand_column(\n...         column_name='col1',\n...         sep=', '\n...     )\n... )\n&gt;&gt;&gt; df\n      col1  col2  A  B  C  D  E  F\n0     A, B     1  1  1  0  0  0  0\n1  B, C, D     2  0  1  1  1  0  0\n2     E, F     3  0  0  0  0  1  1\n3  A, E, F     4  1  0  0  0  1  1\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>column_name</code> <code>Hashable</code> <p>Which column to expand.</p> required <code>sep</code> <code>str</code> <p>The delimiter, same to <code>pandas.Series.str.get_dummies</code>'s <code>sep</code>.</p> <code>'|'</code> <code>concat</code> <code>bool</code> <p>Whether to return the expanded column concatenated to the original dataframe (<code>concat=True</code>), or to return it standalone (<code>concat=False</code>).</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with an expanded column.</p> Source code in <code>janitor/functions/expand_column.py</code> <pre><code>@pf.register_dataframe_method\n@deprecated_alias(column=\"column_name\")\ndef expand_column(\n    df: pd.DataFrame,\n    column_name: Hashable,\n    sep: str = \"|\",\n    concat: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"Expand a categorical column with multiple labels into dummy-coded columns.\n\n    Super sugary syntax that wraps `pandas.Series.str.get_dummies`.\n\n    This method does not mutate the original DataFrame.\n\n    Examples:\n        Functional usage syntax:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; df = pd.DataFrame(\n        ...     {\n        ...         \"col1\": [\"A, B\", \"B, C, D\", \"E, F\", \"A, E, F\"],\n        ...         \"col2\": [1, 2, 3, 4],\n        ...     }\n        ... )\n        &gt;&gt;&gt; df = expand_column(\n        ...     df,\n        ...     column_name=\"col1\",\n        ...     sep=\", \"  # note space in sep\n        ... )\n        &gt;&gt;&gt; df\n              col1  col2  A  B  C  D  E  F\n        0     A, B     1  1  1  0  0  0  0\n        1  B, C, D     2  0  1  1  1  0  0\n        2     E, F     3  0  0  0  0  1  1\n        3  A, E, F     4  1  0  0  0  1  1\n\n        Method chaining syntax:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = (\n        ...     pd.DataFrame(\n        ...         {\n        ...             \"col1\": [\"A, B\", \"B, C, D\", \"E, F\", \"A, E, F\"],\n        ...             \"col2\": [1, 2, 3, 4],\n        ...         }\n        ...     )\n        ...     .expand_column(\n        ...         column_name='col1',\n        ...         sep=', '\n        ...     )\n        ... )\n        &gt;&gt;&gt; df\n              col1  col2  A  B  C  D  E  F\n        0     A, B     1  1  1  0  0  0  0\n        1  B, C, D     2  0  1  1  1  0  0\n        2     E, F     3  0  0  0  0  1  1\n        3  A, E, F     4  1  0  0  0  1  1\n\n    Args:\n        df: A pandas DataFrame.\n        column_name: Which column to expand.\n        sep: The delimiter, same to\n            `pandas.Series.str.get_dummies`'s `sep`.\n        concat: Whether to return the expanded column concatenated to\n            the original dataframe (`concat=True`), or to return it standalone\n            (`concat=False`).\n\n    Returns:\n        A pandas DataFrame with an expanded column.\n    \"\"\"  # noqa: E501\n    expanded_df = df[column_name].str.get_dummies(sep=sep)\n    if concat:\n        return df.join(expanded_df)\n    return expanded_df\n</code></pre>"},{"location":"api/functions/#janitor.functions.expand_grid","title":"<code>expand_grid</code>","text":"<p>Implementation source for <code>expand_grid</code>.</p>"},{"location":"api/functions/#janitor.functions.expand_grid.cartesian_product","title":"<code>cartesian_product(*inputs, sort=False)</code>","text":"<p>Creates a DataFrame from a cartesian combination of all inputs.</p> <p>Inspiration is from tidyr's expand_grid() function.</p> <p>The input argument should be a pandas Index/Series/DataFrame, or a dictionary - the values of the dictionary should be a 1D array.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor as jn\n&gt;&gt;&gt; df = pd.DataFrame({\"x\": [1, 2], \"y\": [2, 1]})\n&gt;&gt;&gt; data = pd.Series([1, 2, 3], name='z')\n&gt;&gt;&gt; jn.cartesian_product(df, data)\n   x  y  z\n0  1  2  1\n1  1  2  2\n2  1  2  3\n3  2  1  1\n4  2  1  2\n5  2  1  3\n</code></pre> <p><code>cartesian_product</code> also works with non-pandas objects:</p> <pre><code>&gt;&gt;&gt; data = {\"x\": [1, 2, 3], \"y\": [1, 2]}\n&gt;&gt;&gt; cartesian_product(data)\n   x  y\n0  1  1\n1  1  2\n2  2  1\n3  2  2\n4  3  1\n5  3  2\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>*inputs</code> <code>tuple</code> <p>Variable arguments. The arguments should be a pandas Index/Series/DataFrame, or a dictionary, where the values in the dictionary is a 1D array.</p> <code>()</code> <code>sort</code> <code>bool</code> <p>If True, sort the output DataFrame.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame.</p> Source code in <code>janitor/functions/expand_grid.py</code> <pre><code>def cartesian_product(*inputs: tuple, sort: bool = False) -&gt; pd.DataFrame:\n    \"\"\"Creates a DataFrame from a cartesian combination of all inputs.\n\n    Inspiration is from tidyr's expand_grid() function.\n\n    The input argument should be a pandas Index/Series/DataFrame,\n    or a dictionary - the values of the dictionary should be\n    a 1D array.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor as jn\n        &gt;&gt;&gt; df = pd.DataFrame({\"x\": [1, 2], \"y\": [2, 1]})\n        &gt;&gt;&gt; data = pd.Series([1, 2, 3], name='z')\n        &gt;&gt;&gt; jn.cartesian_product(df, data)\n           x  y  z\n        0  1  2  1\n        1  1  2  2\n        2  1  2  3\n        3  2  1  1\n        4  2  1  2\n        5  2  1  3\n\n        `cartesian_product` also works with non-pandas objects:\n\n        &gt;&gt;&gt; data = {\"x\": [1, 2, 3], \"y\": [1, 2]}\n        &gt;&gt;&gt; cartesian_product(data)\n           x  y\n        0  1  1\n        1  1  2\n        2  2  1\n        3  2  2\n        4  3  1\n        5  3  2\n\n    Args:\n        *inputs: Variable arguments. The arguments should be\n            a pandas Index/Series/DataFrame, or a dictionary,\n            where the values in the dictionary is a 1D array.\n        sort: If True, sort the output DataFrame.\n\n    Returns:\n        A pandas DataFrame.\n    \"\"\"\n    contents = []\n    for entry in inputs:\n        if isinstance(entry, dict):\n            for label, value in entry.items():\n                arr = pd.Series(value, name=label)\n                contents.append(arr)\n        else:\n            contents.append(entry)\n    outcome = _compute_cartesian_product(inputs=contents, sort=sort)\n    # the values in the outcome dictionary are copies,\n    # based on numpy indexing semantics;\n    # as such, it is safe to pass copy=False\n    return pd.DataFrame(data=outcome, copy=False)\n</code></pre>"},{"location":"api/functions/#janitor.functions.expand_grid.expand","title":"<code>expand(df, *columns, sort=False, by=None)</code>","text":"<p>Creates a DataFrame from a cartesian combination of all inputs.</p> <p>Inspiration is from tidyr's expand() function.</p> <p>expand() is often useful with pd.merge to convert implicit missing values to explicit missing values - similar to <code>complete</code>.</p> <p>It can also be used to figure out which combinations are missing (e.g identify gaps in your DataFrame).</p> <p>The variable <code>columns</code> parameter can be a column name, a list of column names, a pandas Index/Series/DataFrame, or a callable, which when applied to the DataFrame, evaluates to a pandas Index/Series/DataFrame.</p> <p>A dictionary can also be passed to the variable <code>columns</code> parameter - the values of the dictionary should be either be a 1D array or a callable that evaluates to a 1D array. The array should be unique; no check is done to verify this.</p> <p>If <code>by</code> is present, the DataFrame is expanded per group. <code>by</code> should be a column name, or a list of column names.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; data = [{'type': 'apple', 'year': 2010, 'size': 'XS'},\n...         {'type': 'orange', 'year': 2010, 'size': 'S'},\n...         {'type': 'apple', 'year': 2012, 'size': 'M'},\n...         {'type': 'orange', 'year': 2010, 'size': 'S'},\n...         {'type': 'orange', 'year': 2011, 'size': 'S'},\n...         {'type': 'orange', 'year': 2012, 'size': 'M'}]\n&gt;&gt;&gt; df = pd.DataFrame(data)\n&gt;&gt;&gt; df\n     type  year size\n0   apple  2010   XS\n1  orange  2010    S\n2   apple  2012    M\n3  orange  2010    S\n4  orange  2011    S\n5  orange  2012    M\n</code></pre> <p>Get unique observations:</p> <pre><code>&gt;&gt;&gt; df.expand('type')\n     type\n0   apple\n1  orange\n&gt;&gt;&gt; df.expand('size')\n  size\n0   XS\n1    S\n2    M\n&gt;&gt;&gt; df.expand('type', 'size')\n     type size\n0   apple   XS\n1   apple    S\n2   apple    M\n3  orange   XS\n4  orange    S\n5  orange    M\n&gt;&gt;&gt; df.expand('type','size','year')\n      type size  year\n0    apple   XS  2010\n1    apple   XS  2012\n2    apple   XS  2011\n3    apple    S  2010\n4    apple    S  2012\n5    apple    S  2011\n6    apple    M  2010\n7    apple    M  2012\n8    apple    M  2011\n9   orange   XS  2010\n10  orange   XS  2012\n11  orange   XS  2011\n12  orange    S  2010\n13  orange    S  2012\n14  orange    S  2011\n15  orange    M  2010\n16  orange    M  2012\n17  orange    M  2011\n</code></pre> <p>Get observations that only occur in the data:</p> <pre><code>&gt;&gt;&gt; df.expand(['type','size'])\n     type size\n0   apple   XS\n1  orange    S\n2   apple    M\n3  orange    M\n&gt;&gt;&gt; df.expand(['type','size','year'])\n     type size  year\n0   apple   XS  2010\n1  orange    S  2010\n2   apple    M  2012\n3  orange    S  2011\n4  orange    M  2012\n</code></pre> <p>Expand the DataFrame to include new observations:</p> <pre><code>&gt;&gt;&gt; df.expand('type','size',{'new_year':range(2010,2014)})\n      type size  new_year\n0    apple   XS      2010\n1    apple   XS      2011\n2    apple   XS      2012\n3    apple   XS      2013\n4    apple    S      2010\n5    apple    S      2011\n6    apple    S      2012\n7    apple    S      2013\n8    apple    M      2010\n9    apple    M      2011\n10   apple    M      2012\n11   apple    M      2013\n12  orange   XS      2010\n13  orange   XS      2011\n14  orange   XS      2012\n15  orange   XS      2013\n16  orange    S      2010\n17  orange    S      2011\n18  orange    S      2012\n19  orange    S      2013\n20  orange    M      2010\n21  orange    M      2011\n22  orange    M      2012\n23  orange    M      2013\n</code></pre> <p>Filter for missing observations:</p> <pre><code>&gt;&gt;&gt; combo = df.expand('type','size','year')\n&gt;&gt;&gt; anti_join = df.merge(combo, how='right', indicator=True)\n&gt;&gt;&gt; anti_join.query(\"_merge=='right_only'\").drop(columns=\"_merge\")\n      type  year size\n1    apple  2012   XS\n2    apple  2011   XS\n3    apple  2010    S\n4    apple  2012    S\n5    apple  2011    S\n6    apple  2010    M\n8    apple  2011    M\n9   orange  2010   XS\n10  orange  2012   XS\n11  orange  2011   XS\n14  orange  2012    S\n16  orange  2010    M\n18  orange  2011    M\n</code></pre> <p>Expand within each group, using <code>by</code>:</p> <pre><code>&gt;&gt;&gt; df.expand('year','size',by='type')\n        year size\ntype\napple   2010   XS\napple   2010    M\napple   2012   XS\napple   2012    M\norange  2010    S\norange  2010    M\norange  2011    S\norange  2011    M\norange  2012    S\norange  2012    M\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>columns</code> <code>tuple</code> <p>Specification of columns to expand. It could be column labels,  a list/tuple of column labels,  or a pandas Index/Series/DataFrame.</p> <p>It can also be a callable; the callable will be applied to the entire DataFrame. The callable should return a pandas Series/Index/DataFrame.</p> <p>It can also be a dictionary, where the values are either a 1D array or a callable that evaluates to a 1D array. The array should be unique; no check is done to verify this.</p> <code>()</code> <code>sort</code> <code>bool</code> <p>If True, sort the DataFrame.</p> <code>False</code> <code>by</code> <code>str | list</code> <p>Label or list of labels to group by.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame.</p> Source code in <code>janitor/functions/expand_grid.py</code> <pre><code>@pf.register_dataframe_method\ndef expand(\n    df: pd.DataFrame,\n    *columns: tuple,\n    sort: bool = False,\n    by: str | list = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Creates a DataFrame from a cartesian combination of all inputs.\n\n    Inspiration is from tidyr's expand() function.\n\n    expand() is often useful with\n    [pd.merge](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html)\n    to convert implicit\n    missing values to explicit missing values - similar to\n    [`complete`][janitor.functions.complete.complete].\n\n    It can also be used to figure out which combinations are missing\n    (e.g identify gaps in your DataFrame).\n\n    The variable `columns` parameter can be a column name,\n    a list of column names, a pandas Index/Series/DataFrame,\n    or a callable, which when applied to the DataFrame,\n    evaluates to a pandas Index/Series/DataFrame.\n\n    A dictionary can also be passed\n    to the variable `columns` parameter -\n    the values of the dictionary should be\n    either be a 1D array\n    or a callable that evaluates to a\n    1D array. The array should be unique;\n    no check is done to verify this.\n\n    If `by` is present, the DataFrame is *expanded* per group.\n    `by` should be a column name, or a list of column names.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; data = [{'type': 'apple', 'year': 2010, 'size': 'XS'},\n        ...         {'type': 'orange', 'year': 2010, 'size': 'S'},\n        ...         {'type': 'apple', 'year': 2012, 'size': 'M'},\n        ...         {'type': 'orange', 'year': 2010, 'size': 'S'},\n        ...         {'type': 'orange', 'year': 2011, 'size': 'S'},\n        ...         {'type': 'orange', 'year': 2012, 'size': 'M'}]\n        &gt;&gt;&gt; df = pd.DataFrame(data)\n        &gt;&gt;&gt; df\n             type  year size\n        0   apple  2010   XS\n        1  orange  2010    S\n        2   apple  2012    M\n        3  orange  2010    S\n        4  orange  2011    S\n        5  orange  2012    M\n\n        Get unique observations:\n        &gt;&gt;&gt; df.expand('type')\n             type\n        0   apple\n        1  orange\n        &gt;&gt;&gt; df.expand('size')\n          size\n        0   XS\n        1    S\n        2    M\n        &gt;&gt;&gt; df.expand('type', 'size')\n             type size\n        0   apple   XS\n        1   apple    S\n        2   apple    M\n        3  orange   XS\n        4  orange    S\n        5  orange    M\n        &gt;&gt;&gt; df.expand('type','size','year')\n              type size  year\n        0    apple   XS  2010\n        1    apple   XS  2012\n        2    apple   XS  2011\n        3    apple    S  2010\n        4    apple    S  2012\n        5    apple    S  2011\n        6    apple    M  2010\n        7    apple    M  2012\n        8    apple    M  2011\n        9   orange   XS  2010\n        10  orange   XS  2012\n        11  orange   XS  2011\n        12  orange    S  2010\n        13  orange    S  2012\n        14  orange    S  2011\n        15  orange    M  2010\n        16  orange    M  2012\n        17  orange    M  2011\n\n        Get observations that only occur in the data:\n        &gt;&gt;&gt; df.expand(['type','size'])\n             type size\n        0   apple   XS\n        1  orange    S\n        2   apple    M\n        3  orange    M\n        &gt;&gt;&gt; df.expand(['type','size','year'])\n             type size  year\n        0   apple   XS  2010\n        1  orange    S  2010\n        2   apple    M  2012\n        3  orange    S  2011\n        4  orange    M  2012\n\n        Expand the DataFrame to include new observations:\n        &gt;&gt;&gt; df.expand('type','size',{'new_year':range(2010,2014)})\n              type size  new_year\n        0    apple   XS      2010\n        1    apple   XS      2011\n        2    apple   XS      2012\n        3    apple   XS      2013\n        4    apple    S      2010\n        5    apple    S      2011\n        6    apple    S      2012\n        7    apple    S      2013\n        8    apple    M      2010\n        9    apple    M      2011\n        10   apple    M      2012\n        11   apple    M      2013\n        12  orange   XS      2010\n        13  orange   XS      2011\n        14  orange   XS      2012\n        15  orange   XS      2013\n        16  orange    S      2010\n        17  orange    S      2011\n        18  orange    S      2012\n        19  orange    S      2013\n        20  orange    M      2010\n        21  orange    M      2011\n        22  orange    M      2012\n        23  orange    M      2013\n\n        Filter for missing observations:\n        &gt;&gt;&gt; combo = df.expand('type','size','year')\n        &gt;&gt;&gt; anti_join = df.merge(combo, how='right', indicator=True)\n        &gt;&gt;&gt; anti_join.query(\"_merge=='right_only'\").drop(columns=\"_merge\")\n              type  year size\n        1    apple  2012   XS\n        2    apple  2011   XS\n        3    apple  2010    S\n        4    apple  2012    S\n        5    apple  2011    S\n        6    apple  2010    M\n        8    apple  2011    M\n        9   orange  2010   XS\n        10  orange  2012   XS\n        11  orange  2011   XS\n        14  orange  2012    S\n        16  orange  2010    M\n        18  orange  2011    M\n\n        Expand within each group, using `by`:\n        &gt;&gt;&gt; df.expand('year','size',by='type')\n                year size\n        type\n        apple   2010   XS\n        apple   2010    M\n        apple   2012   XS\n        apple   2012    M\n        orange  2010    S\n        orange  2010    M\n        orange  2011    S\n        orange  2011    M\n        orange  2012    S\n        orange  2012    M\n\n    Args:\n        df: A pandas DataFrame.\n        columns: Specification of columns to expand.\n            It could be column labels,\n             a list/tuple of column labels,\n             or a pandas Index/Series/DataFrame.\n\n            It can also be a callable;\n            the callable will be applied to the\n            entire DataFrame. The callable should\n            return a pandas Series/Index/DataFrame.\n\n            It can also be a dictionary,\n            where the values are either a 1D array\n            or a callable that evaluates to a\n            1D array.\n            The array should be unique;\n            no check is done to verify this.\n        sort: If True, sort the DataFrame.\n        by: Label or list of labels to group by.\n\n    Returns:\n        A pandas DataFrame.\n    \"\"\"  # noqa: E501\n    if by is None:\n        contents = _build_pandas_objects_for_expand(df=df, columns=columns)\n        return cartesian_product(*contents, sort=sort)\n    if not is_scalar(by) and not isinstance(by, list):\n        raise TypeError(\n            \"The argument to the by parameter \"\n            \"should be a scalar or a list; \"\n            f\"instead got {type(by).__name__}\"\n        )\n    check_column(df, column_names=by, present=True)\n    grouped = df.groupby(by=by, sort=False, dropna=False, observed=True)\n    index = grouped._grouper.result_index\n    dictionary = defaultdict(list)\n    lengths = []\n    for _, frame in grouped:\n        objects = _build_pandas_objects_for_expand(df=frame, columns=columns)\n        objects = _compute_cartesian_product(inputs=objects, sort=False)\n        length = objects[next(iter(objects))].size\n        lengths.append(length)\n        for k, v in objects.items():\n            dictionary[k].append(v)\n    dictionary = {\n        key: concat_compat(value) for key, value in dictionary.items()\n    }\n    index = index.repeat(lengths)\n    out = pd.DataFrame(data=dictionary, index=index, copy=False)\n    if sort:\n        headers = out.columns.tolist()\n        return out.sort_values(headers)\n    return out\n</code></pre>"},{"location":"api/functions/#janitor.functions.expand_grid.expand_grid","title":"<code>expand_grid(df=None, df_key=None, *, others=None)</code>","text":"<p>Creates a DataFrame from a cartesian combination of all inputs.</p> <p>Note</p> <p>This function will be deprecated in a 1.x release; use <code>cartesian_product</code> instead.</p> <p>It is not restricted to a pandas DataFrame; it can work with any list-like structure that is 1 or 2 dimensional.</p> <p>If method-chaining to a DataFrame, a string argument to <code>df_key</code> parameter must be provided.</p> <p>Data types are preserved in this function, including pandas' extension array dtypes.</p> <p>The output will always be a DataFrame, usually with a MultiIndex column, with the keys of the <code>others</code> dictionary serving as the top level columns.</p> <p>If a pandas Series/DataFrame is passed, and has a labeled index, or a MultiIndex index, the index is discarded; the final DataFrame will have a RangeIndex.</p> <p>The MultiIndexed DataFrame can be flattened using pyjanitor's <code>collapse_levels</code> method; the user can also decide to drop any of the levels, via pandas' <code>droplevel</code> method.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor as jn\n&gt;&gt;&gt; df = pd.DataFrame({\"x\": [1, 2], \"y\": [2, 1]})\n&gt;&gt;&gt; data = {\"z\": [1, 2, 3]}\n&gt;&gt;&gt; df.expand_grid(df_key=\"df\", others=data)\n  df     z\n   x  y  0\n0  1  2  1\n1  1  2  2\n2  1  2  3\n3  2  1  1\n4  2  1  2\n5  2  1  3\n</code></pre> <p><code>expand_grid</code> works with non-pandas objects:</p> <pre><code>&gt;&gt;&gt; data = {\"x\": [1, 2, 3], \"y\": [1, 2]}\n&gt;&gt;&gt; jn.expand_grid(others=data)\n   x  y\n   0  0\n0  1  1\n1  1  2\n2  2  1\n3  2  2\n4  3  1\n5  3  2\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Optional[DataFrame]</code> <p>A pandas DataFrame.</p> <code>None</code> <code>df_key</code> <code>Optional[str]</code> <p>Name of key for the dataframe. It becomes part of the column names of the dataframe.</p> <code>None</code> <code>others</code> <code>Optional[dict]</code> <p>A dictionary that contains the data to be combined with the dataframe. If no dataframe exists, all inputs in <code>others</code> will be combined to create a DataFrame.</p> <code>None</code> <p>Raises:</p> Type Description <code>KeyError</code> <p>If there is a DataFrame and <code>df_key</code> is not provided.</p> <p>Returns:</p> Type Description <code>Union[DataFrame, None]</code> <p>A pandas DataFrame of the cartesian product.</p> <code>Union[DataFrame, None]</code> <p>If <code>df</code> is not provided, and <code>others</code> is not provided,</p> <code>Union[DataFrame, None]</code> <p>None is returned.</p> Source code in <code>janitor/functions/expand_grid.py</code> <pre><code>@pf.register_dataframe_method\n@refactored_function(\n    message=(\n        \"This function will be deprecated in a 1.x release. \"\n        \"Please use `janitor.cartesian_product` instead.\"\n    )\n)\ndef expand_grid(\n    df: Optional[pd.DataFrame] = None,\n    df_key: Optional[str] = None,\n    *,\n    others: Optional[dict] = None,\n) -&gt; Union[pd.DataFrame, None]:\n    \"\"\"\n    Creates a DataFrame from a cartesian combination of all inputs.\n\n    !!!note\n\n        This function will be deprecated in a 1.x release;\n        use [`cartesian_product`][janitor.functions.expand_grid.cartesian_product]\n        instead.\n\n    It is not restricted to a pandas DataFrame;\n    it can work with any list-like structure\n    that is 1 or 2 dimensional.\n\n    If method-chaining to a DataFrame, a string argument\n    to `df_key` parameter must be provided.\n\n    Data types are preserved in this function,\n    including pandas' extension array dtypes.\n\n    The output will always be a DataFrame, usually with a MultiIndex column,\n    with the keys of the `others` dictionary serving as the top level columns.\n\n    If a pandas Series/DataFrame is passed, and has a labeled index, or\n    a MultiIndex index, the index is discarded; the final DataFrame\n    will have a RangeIndex.\n\n    The MultiIndexed DataFrame can be flattened using pyjanitor's\n    [`collapse_levels`][janitor.functions.collapse_levels.collapse_levels]\n    method; the user can also decide to drop any of the levels, via pandas'\n    `droplevel` method.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor as jn\n        &gt;&gt;&gt; df = pd.DataFrame({\"x\": [1, 2], \"y\": [2, 1]})\n        &gt;&gt;&gt; data = {\"z\": [1, 2, 3]}\n        &gt;&gt;&gt; df.expand_grid(df_key=\"df\", others=data)\n          df     z\n           x  y  0\n        0  1  2  1\n        1  1  2  2\n        2  1  2  3\n        3  2  1  1\n        4  2  1  2\n        5  2  1  3\n\n        `expand_grid` works with non-pandas objects:\n\n        &gt;&gt;&gt; data = {\"x\": [1, 2, 3], \"y\": [1, 2]}\n        &gt;&gt;&gt; jn.expand_grid(others=data)\n           x  y\n           0  0\n        0  1  1\n        1  1  2\n        2  2  1\n        3  2  2\n        4  3  1\n        5  3  2\n\n    Args:\n        df: A pandas DataFrame.\n        df_key: Name of key for the dataframe.\n            It becomes part of the column names of the dataframe.\n        others: A dictionary that contains the data\n            to be combined with the dataframe.\n            If no dataframe exists, all inputs\n            in `others` will be combined to create a DataFrame.\n\n    Raises:\n        KeyError: If there is a DataFrame and `df_key` is not provided.\n\n    Returns:\n        A pandas DataFrame of the cartesian product.\n        If `df` is not provided, and `others` is not provided,\n        None is returned.\n    \"\"\"  # noqa: E501\n\n    if df is not None:\n        check(\"df\", df, [pd.DataFrame])\n        if not df_key:\n            raise KeyError(\n                \"Using `expand_grid` as part of a \"\n                \"DataFrame method chain requires that \"\n                \"a string argument be provided for \"\n                \"the `df_key` parameter. \"\n            )\n\n        check(\"df_key\", df_key, [str])\n\n    if not others and (df is not None):\n        return df\n\n    if not others:\n        return None\n\n    check(\"others\", others, [dict])\n\n    for key in others:\n        check(\"key\", key, [str])\n\n    if df is not None:\n        others = {**{df_key: df}, **others}\n\n    others = _computations_expand_grid(others)\n    return pd.DataFrame(others, copy=False)\n</code></pre>"},{"location":"api/functions/#janitor.functions.explode_index","title":"<code>explode_index</code>","text":"<p>Implementation of the <code>explode_index</code> function.</p>"},{"location":"api/functions/#janitor.functions.explode_index.explode_index","title":"<code>explode_index(df, names_sep=None, names_pattern=None, axis='columns', level_names=None)</code>","text":"<p>Explode a single index DataFrame into a MultiIndex DataFrame.</p> <p>This method does not mutate the original DataFrame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame(\n...          {'max_speed_mean': [267.3333333333333, 50.5],\n...           'max_speed_median': [389.0, 50.5]})\n&gt;&gt;&gt; df\n   max_speed_mean  max_speed_median\n0      267.333333             389.0\n1       50.500000              50.5\n&gt;&gt;&gt; df.explode_index(names_sep='_',axis='columns')\n          max\n        speed\n         mean median\n0  267.333333  389.0\n1   50.500000   50.5\n&gt;&gt;&gt; df.explode_index(names_pattern=r\"(.+speed)_(.+)\",axis='columns')\n    max_speed\n         mean median\n0  267.333333  389.0\n1   50.500000   50.5\n&gt;&gt;&gt; df.explode_index(\n...     names_pattern=r\"(?P&lt;measurement&gt;.+speed)_(?P&lt;aggregation&gt;.+)\",\n...     axis='columns'\n... )\nmeasurement   max_speed\naggregation        mean median\n0            267.333333  389.0\n1             50.500000   50.5\n&gt;&gt;&gt; df.explode_index(\n...     names_sep='_',\n...     axis='columns',\n...     level_names = ['min or max', 'measurement','aggregation']\n... )\nmin or max          max\nmeasurement       speed\naggregation        mean median\n0            267.333333  389.0\n1             50.500000   50.5\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>names_sep</code> <code>Union[str, None]</code> <p>string or compiled regex used to split the column/index into levels.</p> <code>None</code> <code>names_pattern</code> <code>Union[str, None]</code> <p>regex to extract new levels from the column/index.</p> <code>None</code> <code>axis</code> <code>str</code> <p>'index/columns'. Determines which axis to explode.</p> <code>'columns'</code> <code>level_names</code> <code>list</code> <p>names of the levels in the MultiIndex.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with a MultiIndex.</p> Source code in <code>janitor/functions/explode_index.py</code> <pre><code>@pf.register_dataframe_method\ndef explode_index(\n    df: pd.DataFrame,\n    names_sep: Union[str, None] = None,\n    names_pattern: Union[str, None] = None,\n    axis: str = \"columns\",\n    level_names: list = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Explode a single index DataFrame into a MultiIndex DataFrame.\n\n    This method does not mutate the original DataFrame.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame(\n        ...          {'max_speed_mean': [267.3333333333333, 50.5],\n        ...           'max_speed_median': [389.0, 50.5]})\n        &gt;&gt;&gt; df\n           max_speed_mean  max_speed_median\n        0      267.333333             389.0\n        1       50.500000              50.5\n        &gt;&gt;&gt; df.explode_index(names_sep='_',axis='columns')  # doctest: +NORMALIZE_WHITESPACE\n                  max\n                speed\n                 mean median\n        0  267.333333  389.0\n        1   50.500000   50.5\n        &gt;&gt;&gt; df.explode_index(names_pattern=r\"(.+speed)_(.+)\",axis='columns') # doctest: +NORMALIZE_WHITESPACE\n            max_speed\n                 mean median\n        0  267.333333  389.0\n        1   50.500000   50.5\n        &gt;&gt;&gt; df.explode_index(\n        ...     names_pattern=r\"(?P&lt;measurement&gt;.+speed)_(?P&lt;aggregation&gt;.+)\",\n        ...     axis='columns'\n        ... ) # doctest: +NORMALIZE_WHITESPACE\n        measurement   max_speed\n        aggregation        mean median\n        0            267.333333  389.0\n        1             50.500000   50.5\n        &gt;&gt;&gt; df.explode_index(\n        ...     names_sep='_',\n        ...     axis='columns',\n        ...     level_names = ['min or max', 'measurement','aggregation']\n        ... ) # doctest: +NORMALIZE_WHITESPACE\n        min or max          max\n        measurement       speed\n        aggregation        mean median\n        0            267.333333  389.0\n        1             50.500000   50.5\n\n    Args:\n        df: A pandas DataFrame.\n        names_sep: string or compiled regex used to split the column/index into levels.\n        names_pattern: regex to extract new levels from the column/index.\n        axis: 'index/columns'. Determines which axis to explode.\n        level_names: names of the levels in the MultiIndex.\n\n    Returns:\n        A pandas DataFrame with a MultiIndex.\n    \"\"\"  # noqa: E501\n    check(\"axis\", axis, [str])\n    if axis not in {\"index\", \"columns\"}:\n        raise ValueError(\"axis should be either index or columns.\")\n    if (names_sep is None) and (names_pattern is None):\n        raise ValueError(\n            \"Provide argument for either names_sep or names_pattern.\"\n        )\n    if (names_sep is not None) and (names_pattern is not None):\n        raise ValueError(\n            \"Provide argument for either names_sep or names_pattern, not both.\"\n        )\n    if names_sep is not None:\n        check(\"names_sep\", names_sep, [str])\n    if names_pattern is not None:\n        check(\"names_pattern\", names_pattern, [str])\n    if level_names is not None:\n        check(\"level_names\", level_names, [list])\n\n    new_index = getattr(df, axis)\n    if isinstance(new_index, pd.MultiIndex):\n        return df\n    # avoid a copy - Index is immutable; a slice is safe to use.\n    df = df[:]\n    if names_sep:\n        new_index = new_index.str.split(names_sep, expand=True)\n    else:\n        named_groups = re.compile(names_pattern).groupindex\n        if named_groups and not level_names:\n            level_names = list(named_groups)\n        new_index = new_index.str.extract(names_pattern)\n        new_index = [arr.array for _, arr in new_index.items()]\n        new_index = pd.MultiIndex.from_arrays(new_index)\n    if level_names:\n        new_index.names = level_names\n\n    setattr(df, axis, new_index)\n    return df\n</code></pre>"},{"location":"api/functions/#janitor.functions.factorize_columns","title":"<code>factorize_columns</code>","text":"<p>Implementation of the <code>factorize_columns</code> function</p>"},{"location":"api/functions/#janitor.functions.factorize_columns.factorize_columns","title":"<code>factorize_columns(df, column_names, suffix='_enc', **kwargs)</code>","text":"<p>Converts labels into numerical data.</p> <p>This method will create a new column with the string <code>_enc</code> appended after the original column's name. This can be overridden with the suffix parameter.</p> <p>Internally, this method uses pandas <code>factorize</code> method. It takes in an optional suffix and keyword arguments also. An empty string as suffix will override the existing column.</p> <p>This method does not mutate the original DataFrame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"foo\": [\"b\", \"b\", \"a\", \"c\", \"b\"],\n...     \"bar\": range(4, 9),\n... })\n&gt;&gt;&gt; df\n  foo  bar\n0   b    4\n1   b    5\n2   a    6\n3   c    7\n4   b    8\n&gt;&gt;&gt; df.factorize_columns(column_names=\"foo\")\n  foo  bar  foo_enc\n0   b    4        0\n1   b    5        0\n2   a    6        1\n3   c    7        2\n4   b    8        0\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The pandas DataFrame object.</p> required <code>column_names</code> <code>Union[str, Iterable[str], Hashable]</code> <p>A column name or an iterable (list or tuple) of column names.</p> required <code>suffix</code> <code>str</code> <p>Suffix to be used for the new column. An empty string suffix means, it will override the existing column.</p> <code>'_enc'</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments. It takes any of the keyword arguments, which the pandas factorize method takes like <code>sort</code>, <code>na_sentinel</code>, <code>size_hint</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame.</p> Source code in <code>janitor/functions/factorize_columns.py</code> <pre><code>@pf.register_dataframe_method\ndef factorize_columns(\n    df: pd.DataFrame,\n    column_names: Union[str, Iterable[str], Hashable],\n    suffix: str = \"_enc\",\n    **kwargs: Any,\n) -&gt; pd.DataFrame:\n    \"\"\"Converts labels into numerical data.\n\n    This method will create a new column with the string `_enc` appended\n    after the original column's name.\n    This can be overridden with the suffix parameter.\n\n    Internally, this method uses pandas `factorize` method.\n    It takes in an optional suffix and keyword arguments also.\n    An empty string as suffix will override the existing column.\n\n    This method does not mutate the original DataFrame.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     \"foo\": [\"b\", \"b\", \"a\", \"c\", \"b\"],\n        ...     \"bar\": range(4, 9),\n        ... })\n        &gt;&gt;&gt; df\n          foo  bar\n        0   b    4\n        1   b    5\n        2   a    6\n        3   c    7\n        4   b    8\n        &gt;&gt;&gt; df.factorize_columns(column_names=\"foo\")\n          foo  bar  foo_enc\n        0   b    4        0\n        1   b    5        0\n        2   a    6        1\n        3   c    7        2\n        4   b    8        0\n\n    Args:\n        df: The pandas DataFrame object.\n        column_names: A column name or an iterable (list or tuple) of\n            column names.\n        suffix: Suffix to be used for the new column.\n            An empty string suffix means, it will override the existing column.\n        **kwargs: Keyword arguments. It takes any of the keyword arguments,\n            which the pandas factorize method takes like `sort`, `na_sentinel`,\n            `size_hint`.\n\n    Returns:\n        A pandas DataFrame.\n    \"\"\"\n    df = _factorize(df.copy(), column_names, suffix, **kwargs)\n    return df\n</code></pre>"},{"location":"api/functions/#janitor.functions.fill","title":"<code>fill</code>","text":""},{"location":"api/functions/#janitor.functions.fill.fill_direction","title":"<code>fill_direction(df, **kwargs)</code>","text":"<p>Provide a method-chainable function for filling missing values in selected columns.</p> <p>It is a wrapper for <code>pd.Series.ffill</code> and <code>pd.Series.bfill</code>, and pairs the column name with one of <code>up</code>, <code>down</code>, <code>updown</code>, and <code>downup</code>.</p> <p>Note</p> <p>This function will be deprecated in a 1.x release. Please use <code>pd.DataFrame.assign</code> instead.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor as jn\n&gt;&gt;&gt; df = pd.DataFrame(\n...    {\n...        'col1': [1, 2, 3, 4],\n...        'col2': [None, 5, 6, 7],\n...        'col3': [8, 9, 10, None],\n...        'col4': [None, None, 11, None],\n...        'col5': [None, 12, 13, None]\n...    }\n... )\n&gt;&gt;&gt; df\n   col1  col2  col3  col4  col5\n0     1   NaN   8.0   NaN   NaN\n1     2   5.0   9.0   NaN  12.0\n2     3   6.0  10.0  11.0  13.0\n3     4   7.0   NaN   NaN   NaN\n&gt;&gt;&gt; df.fill_direction(\n... col2 = 'up',\n... col3 = 'down',\n... col4 = 'downup',\n... col5 = 'updown'\n... )\n   col1  col2  col3  col4  col5\n0     1   5.0   8.0  11.0  12.0\n1     2   5.0   9.0  11.0  12.0\n2     3   6.0  10.0  11.0  13.0\n3     4   7.0  10.0  11.0  13.0\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>**kwargs</code> <code>Any</code> <p>Key - value pairs of columns and directions. Directions can be either <code>down</code>, <code>up</code>, <code>updown</code> (fill up then down) and <code>downup</code> (fill down then up).</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If direction supplied is not one of <code>down</code>, <code>up</code>, <code>updown</code>, or <code>downup</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with modified column(s).</p> Source code in <code>janitor/functions/fill.py</code> <pre><code>@pf.register_dataframe_method\n@refactored_function(\n    message=(\n        \"This function will be deprecated in a 1.x release. \"\n        \"Please use `pd.DataFrame.assign` instead.\"\n    )\n)\ndef fill_direction(df: pd.DataFrame, **kwargs: Any) -&gt; pd.DataFrame:\n    \"\"\"Provide a method-chainable function for filling missing values\n    in selected columns.\n\n    It is a wrapper for `pd.Series.ffill` and `pd.Series.bfill`,\n    and pairs the column name with one of `up`, `down`, `updown`,\n    and `downup`.\n\n    !!!note\n\n        This function will be deprecated in a 1.x release.\n        Please use `pd.DataFrame.assign` instead.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor as jn\n        &gt;&gt;&gt; df = pd.DataFrame(\n        ...    {\n        ...        'col1': [1, 2, 3, 4],\n        ...        'col2': [None, 5, 6, 7],\n        ...        'col3': [8, 9, 10, None],\n        ...        'col4': [None, None, 11, None],\n        ...        'col5': [None, 12, 13, None]\n        ...    }\n        ... )\n        &gt;&gt;&gt; df\n           col1  col2  col3  col4  col5\n        0     1   NaN   8.0   NaN   NaN\n        1     2   5.0   9.0   NaN  12.0\n        2     3   6.0  10.0  11.0  13.0\n        3     4   7.0   NaN   NaN   NaN\n        &gt;&gt;&gt; df.fill_direction(\n        ... col2 = 'up',\n        ... col3 = 'down',\n        ... col4 = 'downup',\n        ... col5 = 'updown'\n        ... )\n           col1  col2  col3  col4  col5\n        0     1   5.0   8.0  11.0  12.0\n        1     2   5.0   9.0  11.0  12.0\n        2     3   6.0  10.0  11.0  13.0\n        3     4   7.0  10.0  11.0  13.0\n\n    Args:\n        df: A pandas DataFrame.\n        **kwargs: Key - value pairs of columns and directions.\n            Directions can be either `down`, `up`, `updown`\n            (fill up then down) and `downup` (fill down then up).\n\n    Raises:\n        ValueError: If direction supplied is not one of `down`, `up`,\n            `updown`, or `downup`.\n\n    Returns:\n        A pandas DataFrame with modified column(s).\n    \"\"\"  # noqa: E501\n\n    if not kwargs:\n        return df\n\n    fill_types = {fill.name for fill in _FILLTYPE}\n    for column_name, fill_type in kwargs.items():\n        check(\"column_name\", column_name, [str])\n        check(\"fill_type\", fill_type, [str])\n        if fill_type.upper() not in fill_types:\n            raise ValueError(\n                \"fill_type should be one of up, down, updown, or downup.\"\n            )\n\n    check_column(df, kwargs)\n\n    new_values = {}\n    for column_name, fill_type in kwargs.items():\n        direction = _FILLTYPE[f\"{fill_type.upper()}\"].value\n        if len(direction) == 1:\n            direction = methodcaller(direction[0])\n            output = direction(df[column_name])\n        else:\n            direction = [methodcaller(entry) for entry in direction]\n            output = _chain_func(df[column_name], *direction)\n        new_values[column_name] = output\n\n    return df.assign(**new_values)\n</code></pre>"},{"location":"api/functions/#janitor.functions.fill.fill_empty","title":"<code>fill_empty(df, column_names, value)</code>","text":"<p>Fill <code>NaN</code> values in specified columns with a given value.</p> <p>Super sugary syntax that wraps <code>pandas.DataFrame.fillna</code>.</p> <p>This method mutates the original DataFrame.</p> <p>Note</p> <p>This function will be deprecated in a 1.x release. Please use <code>jn.impute</code> instead.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame(\n...        {\n...            'col1': [1, 2, 3],\n...            'col2': [None, 4, None ],\n...            'col3': [None, 5, 6]\n...        }\n...    )\n&gt;&gt;&gt; df\n   col1  col2  col3\n0     1   NaN   NaN\n1     2   4.0   5.0\n2     3   NaN   6.0\n&gt;&gt;&gt; df.fill_empty(column_names = 'col2', value = 0)\n   col1  col2  col3\n0     1   0.0   NaN\n1     2   4.0   5.0\n2     3   0.0   6.0\n&gt;&gt;&gt; df.fill_empty(column_names = ['col2', 'col3'], value = 0)\n   col1  col2  col3\n0     1   0.0   0.0\n1     2   4.0   5.0\n2     3   0.0   6.0\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>column_names</code> <code>Union[str, Iterable[str], Hashable]</code> <p>A column name or an iterable (list or tuple) of column names. If a single column name is passed in, then only that column will be filled; if a list or tuple is passed in, then those columns will all be filled with the same value.</p> required <code>value</code> <code>Any</code> <p>The value that replaces the <code>NaN</code> values.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with <code>NaN</code> values filled.</p> Source code in <code>janitor/functions/fill.py</code> <pre><code>@pf.register_dataframe_method\n@refactored_function(\n    message=\"This function will be deprecated in a 1.x release. \"\n    \"Kindly use `jn.impute` instead.\"\n)\n@deprecated_alias(columns=\"column_names\")\ndef fill_empty(\n    df: pd.DataFrame,\n    column_names: Union[str, Iterable[str], Hashable],\n    value: Any,\n) -&gt; pd.DataFrame:\n    \"\"\"Fill `NaN` values in specified columns with a given value.\n\n    Super sugary syntax that wraps `pandas.DataFrame.fillna`.\n\n    This method mutates the original DataFrame.\n\n    !!!note\n\n        This function will be deprecated in a 1.x release.\n        Please use [`jn.impute`][janitor.functions.impute.impute] instead.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame(\n        ...        {\n        ...            'col1': [1, 2, 3],\n        ...            'col2': [None, 4, None ],\n        ...            'col3': [None, 5, 6]\n        ...        }\n        ...    )\n        &gt;&gt;&gt; df\n           col1  col2  col3\n        0     1   NaN   NaN\n        1     2   4.0   5.0\n        2     3   NaN   6.0\n        &gt;&gt;&gt; df.fill_empty(column_names = 'col2', value = 0)\n           col1  col2  col3\n        0     1   0.0   NaN\n        1     2   4.0   5.0\n        2     3   0.0   6.0\n        &gt;&gt;&gt; df.fill_empty(column_names = ['col2', 'col3'], value = 0)\n           col1  col2  col3\n        0     1   0.0   0.0\n        1     2   4.0   5.0\n        2     3   0.0   6.0\n\n    Args:\n        df: A pandas DataFrame.\n        column_names: A column name or an iterable (list\n            or tuple) of column names. If a single column name is passed in,\n            then only that column will be filled; if a list or tuple is passed\n            in, then those columns will all be filled with the same value.\n        value: The value that replaces the `NaN` values.\n\n    Returns:\n        A pandas DataFrame with `NaN` values filled.\n    \"\"\"\n\n    check_column(df, column_names)\n    return _fill_empty(df, column_names, value=value)\n</code></pre>"},{"location":"api/functions/#janitor.functions.filter","title":"<code>filter</code>","text":""},{"location":"api/functions/#janitor.functions.filter.filter_column_isin","title":"<code>filter_column_isin(df, column_name, iterable, complement=False)</code>","text":"<p>Filter a dataframe for values in a column that exist in the given iterable.</p> <p>This method does not mutate the original DataFrame.</p> <p>Assumes exact matching; fuzzy matching not implemented.</p> <p>Examples:</p> <p>Filter the dataframe to retain rows for which <code>names</code> are exactly <code>James</code> or <code>John</code>.</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\"names\": [\"Jane\", \"Jeremy\", \"John\"], \"foo\": list(\"xyz\")})\n&gt;&gt;&gt; df\n    names foo\n0    Jane   x\n1  Jeremy   y\n2    John   z\n&gt;&gt;&gt; df.filter_column_isin(column_name=\"names\", iterable=[\"James\", \"John\"])\n  names foo\n2  John   z\n</code></pre> <p>This is the method-chaining alternative to:</p> <pre><code>df = df[df[\"names\"].isin([\"James\", \"John\"])]\n</code></pre> <p>If <code>complement=True</code>, then we will only get rows for which the names are neither <code>James</code> nor <code>John</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>column_name</code> <code>Hashable</code> <p>The column on which to filter.</p> required <code>iterable</code> <code>Iterable</code> <p>An iterable. Could be a list, tuple, another pandas Series.</p> required <code>complement</code> <code>bool</code> <p>Whether to return the complement of the selection or not.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>iterable</code> does not have a length of <code>1</code> or greater.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A filtered pandas DataFrame.</p> Source code in <code>janitor/functions/filter.py</code> <pre><code>@pf.register_dataframe_method\n@deprecated_alias(column=\"column_name\")\ndef filter_column_isin(\n    df: pd.DataFrame,\n    column_name: Hashable,\n    iterable: Iterable,\n    complement: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Filter a dataframe for values in a column that exist in the given iterable.\n\n    This method does not mutate the original DataFrame.\n\n    Assumes exact matching; fuzzy matching not implemented.\n\n    Examples:\n        Filter the dataframe to retain rows for which `names`\n        are exactly `James` or `John`.\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\"names\": [\"Jane\", \"Jeremy\", \"John\"], \"foo\": list(\"xyz\")})\n        &gt;&gt;&gt; df\n            names foo\n        0    Jane   x\n        1  Jeremy   y\n        2    John   z\n        &gt;&gt;&gt; df.filter_column_isin(column_name=\"names\", iterable=[\"James\", \"John\"])\n          names foo\n        2  John   z\n\n        This is the method-chaining alternative to:\n\n        ```python\n        df = df[df[\"names\"].isin([\"James\", \"John\"])]\n        ```\n\n        If `complement=True`, then we will only get rows for which the names\n        are neither `James` nor `John`.\n\n    Args:\n        df: A pandas DataFrame.\n        column_name: The column on which to filter.\n        iterable: An iterable. Could be a list, tuple, another pandas\n            Series.\n        complement: Whether to return the complement of the selection or\n            not.\n\n    Raises:\n        ValueError: If `iterable` does not have a length of `1`\n            or greater.\n\n    Returns:\n        A filtered pandas DataFrame.\n    \"\"\"  # noqa: E501\n    if len(iterable) == 0:\n        raise ValueError(\n            \"`iterable` kwarg must be given an iterable of length 1 \"\n            \"or greater.\"\n        )\n    criteria = df[column_name].isin(iterable)\n\n    if complement:\n        return df[~criteria]\n    return df[criteria]\n</code></pre>"},{"location":"api/functions/#janitor.functions.filter.filter_date","title":"<code>filter_date(df, column_name, start_date=None, end_date=None, years=None, months=None, days=None, column_date_options=None, format=None)</code>","text":"<p>Filter a date-based column based on certain criteria.</p> <p>This method does not mutate the original DataFrame.</p> <p>Dates may be finicky and this function builds on top of the magic from the pandas <code>to_datetime</code> function that is able to parse dates well.</p> <p>Additional options to parse the date type of your column may be found at the official pandas documentation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"a\": range(5, 9),\n...     \"dt\": [\"2021-11-12\", \"2021-12-15\", \"2022-01-03\", \"2022-01-09\"],\n... })\n&gt;&gt;&gt; df\n   a          dt\n0  5  2021-11-12\n1  6  2021-12-15\n2  7  2022-01-03\n3  8  2022-01-09\n&gt;&gt;&gt; df.filter_date(\"dt\", start_date=\"2021-12-01\", end_date=\"2022-01-05\")\n   a         dt\n1  6 2021-12-15\n2  7 2022-01-03\n&gt;&gt;&gt; df.filter_date(\"dt\", years=[2021], months=[12])\n   a         dt\n1  6 2021-12-15\n</code></pre> <p>Note</p> <p>This method will cast your column to a Timestamp!</p> <p>Note</p> <p>This only affects the format of the <code>start_date</code> and <code>end_date</code> parameters. If there's an issue with the format of the DataFrame being parsed, you would pass <code>{'format': your_format}</code> to <code>column_date_options</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The dataframe to filter on.</p> required <code>column_name</code> <code>Hashable</code> <p>The column which to apply the fraction transformation.</p> required <code>start_date</code> <code>Optional[date]</code> <p>The beginning date to use to filter the DataFrame.</p> <code>None</code> <code>end_date</code> <code>Optional[date]</code> <p>The end date to use to filter the DataFrame.</p> <code>None</code> <code>years</code> <code>Optional[List]</code> <p>The years to use to filter the DataFrame.</p> <code>None</code> <code>months</code> <code>Optional[List]</code> <p>The months to use to filter the DataFrame.</p> <code>None</code> <code>days</code> <code>Optional[List]</code> <p>The days to use to filter the DataFrame.</p> <code>None</code> <code>column_date_options</code> <code>Optional[Dict]</code> <p>Special options to use when parsing the date column in the original DataFrame. The options may be found at the official Pandas documentation.</p> <code>None</code> <code>format</code> <code>Optional[str]</code> <p>If you're using a format for <code>start_date</code> or <code>end_date</code> that is not recognized natively by pandas' <code>to_datetime</code> function, you may supply the format yourself. Python date and time formats may be found here.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A filtered pandas DataFrame.</p> Source code in <code>janitor/functions/filter.py</code> <pre><code>@pf.register_dataframe_method\n@deprecated_alias(column=\"column_name\", start=\"start_date\", end=\"end_date\")\ndef filter_date(\n    df: pd.DataFrame,\n    column_name: Hashable,\n    start_date: Optional[dt.date] = None,\n    end_date: Optional[dt.date] = None,\n    years: Optional[List] = None,\n    months: Optional[List] = None,\n    days: Optional[List] = None,\n    column_date_options: Optional[Dict] = None,\n    format: Optional[str] = None,  # skipcq: PYL-W0622\n) -&gt; pd.DataFrame:\n    \"\"\"Filter a date-based column based on certain criteria.\n\n    This method does not mutate the original DataFrame.\n\n    Dates may be finicky and this function builds on top of the *magic* from\n    the pandas `to_datetime` function that is able to parse dates well.\n\n    Additional options to parse the date type of your column may be found at\n    the official pandas [documentation][datetime].\n\n    [datetime]: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     \"a\": range(5, 9),\n        ...     \"dt\": [\"2021-11-12\", \"2021-12-15\", \"2022-01-03\", \"2022-01-09\"],\n        ... })\n        &gt;&gt;&gt; df\n           a          dt\n        0  5  2021-11-12\n        1  6  2021-12-15\n        2  7  2022-01-03\n        3  8  2022-01-09\n        &gt;&gt;&gt; df.filter_date(\"dt\", start_date=\"2021-12-01\", end_date=\"2022-01-05\")\n           a         dt\n        1  6 2021-12-15\n        2  7 2022-01-03\n        &gt;&gt;&gt; df.filter_date(\"dt\", years=[2021], months=[12])\n           a         dt\n        1  6 2021-12-15\n\n    !!!note\n\n        This method will cast your column to a Timestamp!\n\n    !!!note\n\n        This only affects the format of the `start_date` and `end_date`\n        parameters. If there's an issue with the format of the DataFrame being\n        parsed, you would pass `{'format': your_format}` to `column_date_options`.\n\n    Args:\n        df: The dataframe to filter on.\n        column_name: The column which to apply the fraction transformation.\n        start_date: The beginning date to use to filter the DataFrame.\n        end_date: The end date to use to filter the DataFrame.\n        years: The years to use to filter the DataFrame.\n        months: The months to use to filter the DataFrame.\n        days: The days to use to filter the DataFrame.\n        column_date_options: Special options to use when parsing the date\n            column in the original DataFrame. The options may be found at the\n            official Pandas documentation.\n        format: If you're using a format for `start_date` or `end_date`\n            that is not recognized natively by pandas' `to_datetime` function, you\n            may supply the format yourself. Python date and time formats may be\n            found [here](http://strftime.org/).\n\n    Returns:\n        A filtered pandas DataFrame.\n    \"\"\"  # noqa: E501\n\n    def _date_filter_conditions(conditions):\n        \"\"\"Taken from: https://stackoverflow.com/a/13616382.\"\"\"\n        return reduce(np.logical_and, conditions)\n\n    if column_date_options is None:\n        column_date_options = {}\n    df[column_name] = pd.to_datetime(df[column_name], **column_date_options)\n\n    _filter_list = []\n\n    if start_date:\n        start_date = pd.to_datetime(start_date, format=format)\n        _filter_list.append(df[column_name] &gt;= start_date)\n\n    if end_date:\n        end_date = pd.to_datetime(end_date, format=format)\n        _filter_list.append(df[column_name] &lt;= end_date)\n\n    if years:\n        _filter_list.append(df[column_name].dt.year.isin(years))\n\n    if months:\n        _filter_list.append(df[column_name].dt.month.isin(months))\n\n    if days:\n        _filter_list.append(df[column_name].dt.day.isin(days))\n\n    if start_date and end_date and start_date &gt; end_date:\n        warnings.warn(\n            f\"Your start date of {start_date} is after your end date of \"\n            f\"{end_date}. Is this intended?\"\n        )\n\n    return df.loc[_date_filter_conditions(_filter_list), :]\n</code></pre>"},{"location":"api/functions/#janitor.functions.filter.filter_on","title":"<code>filter_on(df, criteria, complement=False)</code>","text":"<p>Return a dataframe filtered on a particular criteria.</p> <p>This method does not mutate the original DataFrame.</p> <p>This is super-sugary syntax that wraps the pandas <code>.query()</code> API, enabling users to use strings to quickly specify filters for filtering their dataframe. The intent is that <code>filter_on</code> as a verb better matches the intent of a pandas user than the verb <code>query</code>.</p> <p>This is intended to be the method-chaining equivalent of the following:</p> <pre><code>df = df[df[\"score\"] &lt; 3]\n</code></pre> <p>Note</p> <p>This function will be deprecated in a 1.x release. Please use <code>pd.DataFrame.query</code> instead.</p> <p>Examples:</p> <p>Filter students who failed an exam (scored less than 50).</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"student_id\": [\"S1\", \"S2\", \"S3\"],\n...     \"score\": [40, 60, 85],\n... })\n&gt;&gt;&gt; df\n  student_id  score\n0         S1     40\n1         S2     60\n2         S3     85\n&gt;&gt;&gt; df.filter_on(\"score &lt; 50\", complement=False)\n  student_id  score\n0         S1     40\n</code></pre> <p>Credit to Brant Peterson for the name.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>criteria</code> <code>str</code> <p>A filtering criteria that returns an array or Series of booleans, on which pandas can filter on.</p> required <code>complement</code> <code>bool</code> <p>Whether to return the complement of the filter or not. If set to True, then the rows for which the criteria is False are retained instead.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A filtered pandas DataFrame.</p> Source code in <code>janitor/functions/filter.py</code> <pre><code>@pf.register_dataframe_method\n@refactored_function(\n    message=(\n        \"This function will be deprecated in a 1.x release. \"\n        \"Please use `pd.DataFrame.query` instead.\"\n    )\n)\ndef filter_on(\n    df: pd.DataFrame,\n    criteria: str,\n    complement: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Return a dataframe filtered on a particular criteria.\n\n    This method does not mutate the original DataFrame.\n\n    This is super-sugary syntax that wraps the pandas `.query()` API, enabling\n    users to use strings to quickly specify filters for filtering their\n    dataframe. The intent is that `filter_on` as a verb better matches the\n    intent of a pandas user than the verb `query`.\n\n    This is intended to be the method-chaining equivalent of the following:\n\n    ```python\n    df = df[df[\"score\"] &lt; 3]\n    ```\n\n    !!!note\n\n        This function will be deprecated in a 1.x release.\n        Please use `pd.DataFrame.query` instead.\n\n\n    Examples:\n        Filter students who failed an exam (scored less than 50).\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     \"student_id\": [\"S1\", \"S2\", \"S3\"],\n        ...     \"score\": [40, 60, 85],\n        ... })\n        &gt;&gt;&gt; df\n          student_id  score\n        0         S1     40\n        1         S2     60\n        2         S3     85\n        &gt;&gt;&gt; df.filter_on(\"score &lt; 50\", complement=False)\n          student_id  score\n        0         S1     40\n\n    Credit to Brant Peterson for the name.\n\n    Args:\n        df: A pandas DataFrame.\n        criteria: A filtering criteria that returns an array or Series of\n            booleans, on which pandas can filter on.\n        complement: Whether to return the complement of the filter or not.\n            If set to True, then the rows for which the criteria is False are\n            retained instead.\n\n    Returns:\n        A filtered pandas DataFrame.\n    \"\"\"\n\n    warnings.warn(\n        \"This function will be deprecated in a 1.x release. \"\n        \"Kindly use `pd.DataFrame.query` instead.\",\n        DeprecationWarning,\n        stacklevel=find_stack_level(),\n    )\n\n    if complement:\n        return df.query(f\"not ({criteria})\")\n    return df.query(criteria)\n</code></pre>"},{"location":"api/functions/#janitor.functions.filter.filter_string","title":"<code>filter_string(df, column_name, search_string, complement=False, case=True, flags=0, na=None, regex=True)</code>","text":"<p>Filter a string-based column according to whether it contains a substring.</p> <p>This is super sugary syntax that builds on top of <code>pandas.Series.str.contains</code>. It is meant to be the method-chaining equivalent of the following:</p> <pre><code>df = df[df[column_name].str.contains(search_string)]]\n</code></pre> <p>This method does not mutate the original DataFrame.</p> <p>Examples:</p> <p>Retain rows whose column values contain a particular substring.</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\"a\": range(3, 6), \"b\": [\"bear\", \"peeL\", \"sail\"]})\n&gt;&gt;&gt; df\n   a     b\n0  3  bear\n1  4  peeL\n2  5  sail\n&gt;&gt;&gt; df.filter_string(column_name=\"b\", search_string=\"ee\")\n   a     b\n1  4  peeL\n&gt;&gt;&gt; df.filter_string(column_name=\"b\", search_string=\"L\", case=False)\n   a     b\n1  4  peeL\n2  5  sail\n</code></pre> <p>Filter names does not contain <code>'.'</code> (disable regex mode).</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.Series([\"JoseChen\", \"Brian.Salvi\"], name=\"Name\").to_frame()\n&gt;&gt;&gt; df\n          Name\n0     JoseChen\n1  Brian.Salvi\n&gt;&gt;&gt; df.filter_string(column_name=\"Name\", search_string=\".\", regex=False, complement=True)\n       Name\n0  JoseChen\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>column_name</code> <code>Hashable</code> <p>The column to filter. The column should contain strings.</p> required <code>search_string</code> <code>str</code> <p>A regex pattern or a (sub-)string to search.</p> required <code>complement</code> <code>bool</code> <p>Whether to return the complement of the filter or not. If set to True, then the rows for which the string search fails are retained instead.</p> <code>False</code> <code>case</code> <code>bool</code> <p>If True, case sensitive.</p> <code>True</code> <code>flags</code> <code>int</code> <p>Flags to pass through to the re module, e.g. re.IGNORECASE.</p> <code>0</code> <code>na</code> <code>Any</code> <p>Fill value for missing values. The default depends on dtype of the array. For object-dtype, <code>numpy.nan</code> is used. For <code>StringDtype</code>, <code>pandas.NA</code> is used.</p> <code>None</code> <code>regex</code> <code>bool</code> <p>If True, assumes <code>search_string</code> is a regular expression. If False, treats the <code>search_string</code> as a literal string.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A filtered pandas DataFrame.</p> Source code in <code>janitor/functions/filter.py</code> <pre><code>@pf.register_dataframe_method\n@deprecated_alias(column=\"column_name\")\ndef filter_string(\n    df: pd.DataFrame,\n    column_name: Hashable,\n    search_string: str,\n    complement: bool = False,\n    case: bool = True,\n    flags: int = 0,\n    na: Any = None,\n    regex: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"Filter a string-based column according to whether it contains a substring.\n\n    This is super sugary syntax that builds on top of `pandas.Series.str.contains`.\n    It is meant to be the method-chaining equivalent of the following:\n\n    ```python\n    df = df[df[column_name].str.contains(search_string)]]\n    ```\n\n    This method does not mutate the original DataFrame.\n\n    Examples:\n        Retain rows whose column values contain a particular substring.\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\"a\": range(3, 6), \"b\": [\"bear\", \"peeL\", \"sail\"]})\n        &gt;&gt;&gt; df\n           a     b\n        0  3  bear\n        1  4  peeL\n        2  5  sail\n        &gt;&gt;&gt; df.filter_string(column_name=\"b\", search_string=\"ee\")\n           a     b\n        1  4  peeL\n        &gt;&gt;&gt; df.filter_string(column_name=\"b\", search_string=\"L\", case=False)\n           a     b\n        1  4  peeL\n        2  5  sail\n\n        Filter names does not contain `'.'` (disable regex mode).\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.Series([\"JoseChen\", \"Brian.Salvi\"], name=\"Name\").to_frame()\n        &gt;&gt;&gt; df\n                  Name\n        0     JoseChen\n        1  Brian.Salvi\n        &gt;&gt;&gt; df.filter_string(column_name=\"Name\", search_string=\".\", regex=False, complement=True)\n               Name\n        0  JoseChen\n\n    Args:\n        df: A pandas DataFrame.\n        column_name: The column to filter. The column should contain strings.\n        search_string: A regex pattern or a (sub-)string to search.\n        complement: Whether to return the complement of the filter or not. If\n            set to True, then the rows for which the string search fails are retained\n            instead.\n        case: If True, case sensitive.\n        flags: Flags to pass through to the re module, e.g. re.IGNORECASE.\n        na: Fill value for missing values. The default depends on dtype of\n            the array. For object-dtype, `numpy.nan` is used. For `StringDtype`,\n            `pandas.NA` is used.\n        regex: If True, assumes `search_string` is a regular expression. If False,\n            treats the `search_string` as a literal string.\n\n    Returns:\n        A filtered pandas DataFrame.\n    \"\"\"  # noqa: E501\n\n    criteria = df[column_name].str.contains(\n        pat=search_string,\n        case=case,\n        flags=flags,\n        na=na,\n        regex=regex,\n    )\n\n    if complement:\n        return df[~criteria]\n\n    return df[criteria]\n</code></pre>"},{"location":"api/functions/#janitor.functions.find_replace","title":"<code>find_replace</code>","text":"<p>Implementation for find_replace.</p>"},{"location":"api/functions/#janitor.functions.find_replace.find_replace","title":"<code>find_replace(df, match='exact', **mappings)</code>","text":"<p>Perform a find-and-replace action on provided columns.</p> <p>Note</p> <p>This function will be deprecated in a 1.x release. Please use <code>pd.DataFrame.replace</code> instead.</p> <p>Depending on use case, users can choose either exact, full-value matching, or regular-expression-based fuzzy matching (hence allowing substring matching in the latter case). For strings, the matching is always case sensitive.</p> <p>Examples:</p> <p>For instance, given a DataFrame containing orders at a coffee shop:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame({\n...     \"customer\": [\"Mary\", \"Tom\", \"Lila\"],\n...     \"order\": [\"ice coffee\", \"lemonade\", \"regular coffee\"]\n... })\n&gt;&gt;&gt; df\n  customer           order\n0     Mary      ice coffee\n1      Tom        lemonade\n2     Lila  regular coffee\n</code></pre> <p>Our task is to replace values <code>ice coffee</code> and <code>regular coffee</code> of the <code>order</code> column into <code>latte</code>.</p> <p>Example 1 - exact matching (functional usage):</p> <pre><code>&gt;&gt;&gt; df = find_replace(\n...     df,\n...     match=\"exact\",\n...     order={\"ice coffee\": \"latte\", \"regular coffee\": \"latte\"},\n... )\n&gt;&gt;&gt; df\n  customer     order\n0     Mary     latte\n1      Tom  lemonade\n2     Lila     latte\n</code></pre> <p>Example 1 - exact matching (method chaining):</p> <pre><code>&gt;&gt;&gt; df = df.find_replace(\n...     match=\"exact\",\n...     order={\"ice coffee\": \"latte\", \"regular coffee\": \"latte\"},\n... )\n&gt;&gt;&gt; df\n  customer     order\n0     Mary     latte\n1      Tom  lemonade\n2     Lila     latte\n</code></pre> <p>Example 2 - Regular-expression-based matching (functional usage):</p> <pre><code>&gt;&gt;&gt; df = find_replace(\n...     df,\n...     match='regex',\n...     order={'coffee$': 'latte'},\n... )\n&gt;&gt;&gt; df\n  customer     order\n0     Mary     latte\n1      Tom  lemonade\n2     Lila     latte\n</code></pre> <p>Example 2 - Regular-expression-based matching (method chaining usage):</p> <pre><code>&gt;&gt;&gt; df = df.find_replace(\n...     match='regex',\n...     order={'coffee$': 'latte'},\n... )\n&gt;&gt;&gt; df\n  customer     order\n0     Mary     latte\n1      Tom  lemonade\n2     Lila     latte\n</code></pre> <p>To perform a find and replace on the entire DataFrame, pandas' <code>df.replace()</code> function provides the appropriate functionality. You can find more detail on the replace docs.</p> <p>This function only works with column names that have no spaces or punctuation in them. For example, a column name <code>item_name</code> would work with <code>find_replace</code>, because it is a contiguous string that can be parsed correctly, but <code>item name</code> would not be parsed correctly by the Python interpreter.</p> <p>If you have column names that might not be compatible, we recommend calling on <code>clean_names()</code> as the first method call. If, for whatever reason, that is not possible, then <code>_find_replace</code> is available as a function that you can do a pandas pipe call on.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>match</code> <code>str</code> <p>Whether or not to perform an exact match or not. Valid values are \"exact\" or \"regex\".</p> <code>'exact'</code> <code>**mappings</code> <code>Any</code> <p>keyword arguments corresponding to column names that have dictionaries passed in indicating what to find (keys) and what to replace with (values).</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with replaced values.</p> Source code in <code>janitor/functions/find_replace.py</code> <pre><code>@pf.register_dataframe_method\n@refactored_function(\n    message=(\n        \"This function will be deprecated in a 1.x release. \"\n        \"Please use `pd.DataFrame.replace` instead.\"\n    )\n)\ndef find_replace(\n    df: pd.DataFrame, match: str = \"exact\", **mappings: Any\n) -&gt; pd.DataFrame:\n    \"\"\"Perform a find-and-replace action on provided columns.\n\n    !!!note\n\n        This function will be deprecated in a 1.x release.\n        Please use `pd.DataFrame.replace` instead.\n\n    Depending on use case, users can choose either exact, full-value matching,\n    or regular-expression-based fuzzy matching\n    (hence allowing substring matching in the latter case).\n    For strings, the matching is always case sensitive.\n\n    Examples:\n        For instance, given a DataFrame containing orders at a coffee shop:\n\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     \"customer\": [\"Mary\", \"Tom\", \"Lila\"],\n        ...     \"order\": [\"ice coffee\", \"lemonade\", \"regular coffee\"]\n        ... })\n        &gt;&gt;&gt; df\n          customer           order\n        0     Mary      ice coffee\n        1      Tom        lemonade\n        2     Lila  regular coffee\n\n        Our task is to replace values `ice coffee` and `regular coffee`\n        of the `order` column into `latte`.\n\n        Example 1 - exact matching (functional usage):\n\n        &gt;&gt;&gt; df = find_replace(\n        ...     df,\n        ...     match=\"exact\",\n        ...     order={\"ice coffee\": \"latte\", \"regular coffee\": \"latte\"},\n        ... )\n        &gt;&gt;&gt; df\n          customer     order\n        0     Mary     latte\n        1      Tom  lemonade\n        2     Lila     latte\n\n        Example 1 - exact matching (method chaining):\n\n        &gt;&gt;&gt; df = df.find_replace(\n        ...     match=\"exact\",\n        ...     order={\"ice coffee\": \"latte\", \"regular coffee\": \"latte\"},\n        ... )\n        &gt;&gt;&gt; df\n          customer     order\n        0     Mary     latte\n        1      Tom  lemonade\n        2     Lila     latte\n\n        Example 2 - Regular-expression-based matching (functional usage):\n\n        &gt;&gt;&gt; df = find_replace(\n        ...     df,\n        ...     match='regex',\n        ...     order={'coffee$': 'latte'},\n        ... )\n        &gt;&gt;&gt; df\n          customer     order\n        0     Mary     latte\n        1      Tom  lemonade\n        2     Lila     latte\n\n        Example 2 - Regular-expression-based matching (method chaining usage):\n\n        &gt;&gt;&gt; df = df.find_replace(\n        ...     match='regex',\n        ...     order={'coffee$': 'latte'},\n        ... )\n        &gt;&gt;&gt; df\n          customer     order\n        0     Mary     latte\n        1      Tom  lemonade\n        2     Lila     latte\n\n    To perform a find and replace on the entire DataFrame,\n    pandas' `df.replace()` function provides the appropriate functionality.\n    You can find more detail on the [replace] docs.\n\n    [replace]: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html\n\n    This function only works with column names that have no spaces\n    or punctuation in them.\n    For example, a column name `item_name` would work with `find_replace`,\n    because it is a contiguous string that can be parsed correctly,\n    but `item name` would not be parsed correctly by the Python interpreter.\n\n    If you have column names that might not be compatible,\n    we recommend calling on [`clean_names()`][janitor.functions.clean_names.clean_names]\n    as the first method call. If, for whatever reason, that is not possible,\n    then `_find_replace` is available as a function\n    that you can do a pandas [pipe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.pipe.html) call on.\n\n    Args:\n        df: A pandas DataFrame.\n        match: Whether or not to perform an exact match or not.\n            Valid values are \"exact\" or \"regex\".\n        **mappings: keyword arguments corresponding to column names\n            that have dictionaries passed in indicating what to find (keys)\n            and what to replace with (values).\n\n    Returns:\n        A pandas DataFrame with replaced values.\n    \"\"\"  # noqa: E501\n    for column_name, mapper in mappings.items():\n        df = _find_replace(df, column_name, mapper, match=match)\n    return df\n</code></pre>"},{"location":"api/functions/#janitor.functions.flag_nulls","title":"<code>flag_nulls</code>","text":"<p>Implementation source for <code>flag_nulls</code>.</p>"},{"location":"api/functions/#janitor.functions.flag_nulls.flag_nulls","title":"<code>flag_nulls(df, column_name='null_flag', columns=None)</code>","text":"<p>Creates a new column to indicate whether you have null values in a given row.</p> <p>If the columns parameter is not set, looks across the entire DataFrame, otherwise will look only in the columns you set.</p> <p>This method does not mutate the original DataFrame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"a\": [\"w\", \"x\", None, \"z\"], \"b\": [5, None, 7, 8],\n... })\n&gt;&gt;&gt; df.flag_nulls()\n      a    b  null_flag\n0     w  5.0          0\n1     x  NaN          1\n2  None  7.0          1\n3     z  8.0          0\n&gt;&gt;&gt; df.flag_nulls(columns=\"b\")\n      a    b  null_flag\n0     w  5.0          0\n1     x  NaN          1\n2  None  7.0          0\n3     z  8.0          0\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input pandas DataFrame.</p> required <code>column_name</code> <code>Optional[Hashable]</code> <p>Name for the output column.</p> <code>'null_flag'</code> <code>columns</code> <code>Optional[Union[str, Iterable[str], Hashable]]</code> <p>List of columns to look at for finding null values. If you only want to look at one column, you can simply give its name. If set to None (default), all DataFrame columns are used.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>column_name</code> is already present in the DataFrame.</p> <code>ValueError</code> <p>If any column within <code>columns</code> is not present in the DataFrame.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Input dataframe with the null flag column.</p> Source code in <code>janitor/functions/flag_nulls.py</code> <pre><code>@pf.register_dataframe_method\ndef flag_nulls(\n    df: pd.DataFrame,\n    column_name: Optional[Hashable] = \"null_flag\",\n    columns: Optional[Union[str, Iterable[str], Hashable]] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Creates a new column to indicate whether you have null values in a given\n    row.\n\n    If the columns parameter is not set, looks across the entire\n    DataFrame, otherwise will look only in the columns you set.\n\n    This method does not mutate the original DataFrame.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     \"a\": [\"w\", \"x\", None, \"z\"], \"b\": [5, None, 7, 8],\n        ... })\n        &gt;&gt;&gt; df.flag_nulls()\n              a    b  null_flag\n        0     w  5.0          0\n        1     x  NaN          1\n        2  None  7.0          1\n        3     z  8.0          0\n        &gt;&gt;&gt; df.flag_nulls(columns=\"b\")\n              a    b  null_flag\n        0     w  5.0          0\n        1     x  NaN          1\n        2  None  7.0          0\n        3     z  8.0          0\n\n    Args:\n        df: Input pandas DataFrame.\n        column_name: Name for the output column.\n        columns: List of columns to look at for finding null values. If you\n            only want to look at one column, you can simply give its name.\n            If set to None (default), all DataFrame columns are used.\n\n    Raises:\n        ValueError: If `column_name` is already present in the\n            DataFrame.\n        ValueError: If any column within `columns` is not present in\n            the DataFrame.\n\n    Returns:\n        Input dataframe with the null flag column.\n\n    &lt;!--\n    # noqa: DAR402\n    --&gt;\n    \"\"\"\n    # Sort out columns input\n    if isinstance(columns, str):\n        columns = [columns]\n    elif columns is None:\n        columns = df.columns\n    elif not isinstance(columns, Iterable):\n        # catches other hashable types\n        columns = [columns]\n\n    # Input sanitation checks\n    check_column(df, columns)\n    check_column(df, [column_name], present=False)\n\n    # This algorithm works best for n_rows &gt;&gt; n_cols. See issue #501\n    null_array = np.zeros(len(df))\n    for col in columns:\n        null_array = np.logical_or(null_array, pd.isna(df[col]))\n\n    df = df.copy()\n    df[column_name] = null_array.astype(int)\n    return df\n</code></pre>"},{"location":"api/functions/#janitor.functions.get_dupes","title":"<code>get_dupes</code>","text":"<p>Implementation of the <code>get_dupes</code> function</p>"},{"location":"api/functions/#janitor.functions.get_dupes.get_dupes","title":"<code>get_dupes(df, column_names=None)</code>","text":"<p>Return all duplicate rows.</p> <p>This method does not mutate the original DataFrame.</p> <p>Examples:</p> <p>Method chaining syntax:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"item\": [\"shoe\", \"shoe\", \"bag\", \"shoe\", \"bag\"],\n...     \"quantity\": [100, 100, 75, 200, 75],\n... })\n&gt;&gt;&gt; df\n   item  quantity\n0  shoe       100\n1  shoe       100\n2   bag        75\n3  shoe       200\n4   bag        75\n&gt;&gt;&gt; df.get_dupes()\n   item  quantity\n0  shoe       100\n1  shoe       100\n2   bag        75\n4   bag        75\n</code></pre> <p>Optional <code>column_names</code> usage:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"item\": [\"shoe\", \"shoe\", \"bag\", \"shoe\", \"bag\"],\n...     \"quantity\": [100, 100, 75, 200, 75],\n... })\n&gt;&gt;&gt; df\n   item  quantity\n0  shoe       100\n1  shoe       100\n2   bag        75\n3  shoe       200\n4   bag        75\n&gt;&gt;&gt; df.get_dupes(column_names=[\"item\"])\n   item  quantity\n0  shoe       100\n1  shoe       100\n2   bag        75\n3  shoe       200\n4   bag        75\n&gt;&gt;&gt; df.get_dupes(column_names=[\"quantity\"])\n   item  quantity\n0  shoe       100\n1  shoe       100\n2   bag        75\n4   bag        75\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The pandas DataFrame object.</p> required <code>column_names</code> <code>Optional[Union[str, Iterable[str], Hashable]]</code> <p>A column name or an iterable (list or tuple) of column names. Following pandas API, this only considers certain columns for identifying duplicates. Defaults to using all columns.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The duplicate rows, as a pandas DataFrame.</p> Source code in <code>janitor/functions/get_dupes.py</code> <pre><code>@pf.register_dataframe_method\n@deprecated_alias(columns=\"column_names\")\ndef get_dupes(\n    df: pd.DataFrame,\n    column_names: Optional[Union[str, Iterable[str], Hashable]] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Return all duplicate rows.\n\n    This method does not mutate the original DataFrame.\n\n    Examples:\n        Method chaining syntax:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     \"item\": [\"shoe\", \"shoe\", \"bag\", \"shoe\", \"bag\"],\n        ...     \"quantity\": [100, 100, 75, 200, 75],\n        ... })\n        &gt;&gt;&gt; df\n           item  quantity\n        0  shoe       100\n        1  shoe       100\n        2   bag        75\n        3  shoe       200\n        4   bag        75\n        &gt;&gt;&gt; df.get_dupes()\n           item  quantity\n        0  shoe       100\n        1  shoe       100\n        2   bag        75\n        4   bag        75\n\n        Optional `column_names` usage:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     \"item\": [\"shoe\", \"shoe\", \"bag\", \"shoe\", \"bag\"],\n        ...     \"quantity\": [100, 100, 75, 200, 75],\n        ... })\n        &gt;&gt;&gt; df\n           item  quantity\n        0  shoe       100\n        1  shoe       100\n        2   bag        75\n        3  shoe       200\n        4   bag        75\n        &gt;&gt;&gt; df.get_dupes(column_names=[\"item\"])\n           item  quantity\n        0  shoe       100\n        1  shoe       100\n        2   bag        75\n        3  shoe       200\n        4   bag        75\n        &gt;&gt;&gt; df.get_dupes(column_names=[\"quantity\"])\n           item  quantity\n        0  shoe       100\n        1  shoe       100\n        2   bag        75\n        4   bag        75\n\n    Args:\n        df: The pandas DataFrame object.\n        column_names: A column name or an iterable\n            (list or tuple) of column names. Following pandas API, this only\n            considers certain columns for identifying duplicates. Defaults\n            to using all columns.\n\n    Returns:\n        The duplicate rows, as a pandas DataFrame.\n    \"\"\"\n    return df.loc[df.duplicated(subset=column_names, keep=False)]\n</code></pre>"},{"location":"api/functions/#janitor.functions.groupby_agg","title":"<code>groupby_agg</code>","text":"<p>Implementation source for <code>groupby_agg</code>.</p>"},{"location":"api/functions/#janitor.functions.groupby_agg.groupby_agg","title":"<code>groupby_agg(df, by, new_column_name, agg_column_name, agg, dropna=True)</code>","text":"<p>Shortcut for assigning a groupby-transform to a new column.</p> <p>This method does not mutate the original DataFrame.</p> <p>Intended to be the method-chaining equivalent of:</p> <pre><code>df = df.assign(...=df.groupby(...)[...].transform(...))\n</code></pre> <p>Note</p> <p>This function will be deprecated in a 1.x release. Please use <code>jn.transform_column</code> instead.</p> <p>Examples:</p> <p>Basic usage.</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"item\": [\"shoe\", \"shoe\", \"bag\", \"shoe\", \"bag\"],\n...     \"quantity\": [100, 120, 75, 200, 25],\n... })\n&gt;&gt;&gt; df.groupby_agg(\n...     by=\"item\",\n...     agg=\"mean\",\n...     agg_column_name=\"quantity\",\n...     new_column_name=\"avg_quantity\",\n... )\n   item  quantity  avg_quantity\n0  shoe       100         140.0\n1  shoe       120         140.0\n2   bag        75          50.0\n3  shoe       200         140.0\n4   bag        25          50.0\n</code></pre> <p>Set <code>dropna=False</code> to compute the aggregation, treating the null values in the <code>by</code> column as an isolated \"group\".</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"x\": [\"a\", \"a\", None, \"b\"], \"y\": [9, 9, 9, 9],\n... })\n&gt;&gt;&gt; df.groupby_agg(\n...     by=\"x\",\n...     agg=\"count\",\n...     agg_column_name=\"y\",\n...     new_column_name=\"y_count\",\n...     dropna=False,\n... )\n      x  y  y_count\n0     a  9        2\n1     a  9        2\n2  None  9        1\n3     b  9        1\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>by</code> <code>Union[List, Callable, str]</code> <p>Column(s) to groupby on, will be passed into <code>DataFrame.groupby</code>.</p> required <code>new_column_name</code> <code>str</code> <p>Name of the aggregation output column.</p> required <code>agg_column_name</code> <code>str</code> <p>Name of the column to aggregate over.</p> required <code>agg</code> <code>Union[Callable, str]</code> <p>How to aggregate.</p> required <code>dropna</code> <code>bool</code> <p>Whether or not to include null values, if present in the <code>by</code> column(s). Default is True (null values in <code>by</code> are assigned NaN in the new column).</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame.</p> Source code in <code>janitor/functions/groupby_agg.py</code> <pre><code>@pf.register_dataframe_method\n@deprecated_alias(new_column=\"new_column_name\", agg_column=\"agg_column_name\")\n@refactored_function(\n    message=(\n        \"This function will be deprecated in a 1.x release. \"\n        \"Please use `janitor.transform_column` instead.\"\n    )\n)\ndef groupby_agg(\n    df: pd.DataFrame,\n    by: Union[List, Callable, str],\n    new_column_name: str,\n    agg_column_name: str,\n    agg: Union[Callable, str],\n    dropna: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"Shortcut for assigning a groupby-transform to a new column.\n\n    This method does not mutate the original DataFrame.\n\n    Intended to be the method-chaining equivalent of:\n\n    ```python\n    df = df.assign(...=df.groupby(...)[...].transform(...))\n    ```\n\n    !!!note\n\n        This function will be deprecated in a 1.x release.\n        Please use\n        [`jn.transform_column`][janitor.functions.transform_columns.transform_column]\n        instead.\n\n    Examples:\n        Basic usage.\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     \"item\": [\"shoe\", \"shoe\", \"bag\", \"shoe\", \"bag\"],\n        ...     \"quantity\": [100, 120, 75, 200, 25],\n        ... })\n        &gt;&gt;&gt; df.groupby_agg(\n        ...     by=\"item\",\n        ...     agg=\"mean\",\n        ...     agg_column_name=\"quantity\",\n        ...     new_column_name=\"avg_quantity\",\n        ... )\n           item  quantity  avg_quantity\n        0  shoe       100         140.0\n        1  shoe       120         140.0\n        2   bag        75          50.0\n        3  shoe       200         140.0\n        4   bag        25          50.0\n\n        Set `dropna=False` to compute the aggregation, treating the null\n        values in the `by` column as an isolated \"group\".\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     \"x\": [\"a\", \"a\", None, \"b\"], \"y\": [9, 9, 9, 9],\n        ... })\n        &gt;&gt;&gt; df.groupby_agg(\n        ...     by=\"x\",\n        ...     agg=\"count\",\n        ...     agg_column_name=\"y\",\n        ...     new_column_name=\"y_count\",\n        ...     dropna=False,\n        ... )\n              x  y  y_count\n        0     a  9        2\n        1     a  9        2\n        2  None  9        1\n        3     b  9        1\n\n    Args:\n        df: A pandas DataFrame.\n        by: Column(s) to groupby on, will be passed into `DataFrame.groupby`.\n        new_column_name: Name of the aggregation output column.\n        agg_column_name: Name of the column to aggregate over.\n        agg: How to aggregate.\n        dropna: Whether or not to include null values, if present in the\n            `by` column(s). Default is True (null values in `by` are assigned NaN in\n            the new column).\n\n    Returns:\n        A pandas DataFrame.\n    \"\"\"  # noqa: E501\n\n    return df.assign(\n        **{\n            new_column_name: df.groupby(by, dropna=dropna)[\n                agg_column_name\n            ].transform(agg),\n        }\n    )\n</code></pre>"},{"location":"api/functions/#janitor.functions.groupby_topk","title":"<code>groupby_topk</code>","text":"<p>Implementation of the <code>groupby_topk</code> function</p>"},{"location":"api/functions/#janitor.functions.groupby_topk.groupby_topk","title":"<code>groupby_topk(df, by, column, k, dropna=True, ascending=True, ignore_index=True)</code>","text":"<p>Return top <code>k</code> rows from a groupby of a set of columns.</p> <p>Returns a DataFrame that has the top <code>k</code> values per <code>column</code>, grouped by <code>by</code>. Under the hood it uses <code>nlargest/nsmallest</code>, for numeric columns, which avoids sorting the entire dataframe, and is usually more performant. For non-numeric columns, <code>pd.sort_values</code> is used. No sorting is done to the <code>by</code> column(s); the order is maintained in the final output.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame(\n...     {\n...         \"age\": [20, 23, 22, 43, 21],\n...         \"id\": [1, 4, 6, 2, 5],\n...         \"result\": [\"pass\", \"pass\", \"fail\", \"pass\", \"fail\"],\n...     }\n... )\n&gt;&gt;&gt; df\n   age  id result\n0   20   1   pass\n1   23   4   pass\n2   22   6   fail\n3   43   2   pass\n4   21   5   fail\n</code></pre> <p>Ascending top 3:</p> <pre><code>&gt;&gt;&gt; df.groupby_topk(by=\"result\", column=\"age\", k=3)\n   age  id result\n0   20   1   pass\n1   23   4   pass\n2   43   2   pass\n3   21   5   fail\n4   22   6   fail\n</code></pre> <p>Descending top 2:</p> <pre><code>&gt;&gt;&gt; df.groupby_topk(\n...     by=\"result\", column=\"age\", k=2, ascending=False, ignore_index=False\n... )\n   age  id result\n3   43   2   pass\n1   23   4   pass\n2   22   6   fail\n4   21   5   fail\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>by</code> <code>Union[list, Hashable]</code> <p>Column name(s) to group input DataFrame <code>df</code> by.</p> required <code>column</code> <code>Hashable</code> <p>Name of the column that determines <code>k</code> rows to return.</p> required <code>k</code> <code>int</code> <p>Number of top rows to return for each group.</p> required <code>dropna</code> <code>bool</code> <p>If <code>True</code>, and <code>NA</code> values exist in <code>by</code>, the <code>NA</code> values are not used in the groupby computation to get the relevant <code>k</code> rows. If <code>False</code>, and <code>NA</code> values exist in <code>by</code>, then the <code>NA</code> values are used in the groupby computation to get the relevant <code>k</code> rows.</p> <code>True</code> <code>ascending</code> <code>bool</code> <p>If <code>True</code>, the smallest top <code>k</code> rows, determined by <code>column</code> are returned; if <code>False, the largest top</code>k<code>rows, determined by</code>column` are returned.</p> <code>True</code> <code>ignore_index</code> <code>bool</code> <p>If <code>True</code>, the original index is ignored. If <code>False</code>, the original index for the top <code>k</code> rows is retained.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>k</code> is less than 1.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with top <code>k</code> rows per <code>column</code>, grouped by <code>by</code>.</p> Source code in <code>janitor/functions/groupby_topk.py</code> <pre><code>@pf.register_dataframe_method\n@deprecated_alias(groupby_column_name=\"by\", sort_column_name=\"column\")\ndef groupby_topk(\n    df: pd.DataFrame,\n    by: Union[list, Hashable],\n    column: Hashable,\n    k: int,\n    dropna: bool = True,\n    ascending: bool = True,\n    ignore_index: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"Return top `k` rows from a groupby of a set of columns.\n\n    Returns a DataFrame that has the top `k` values per `column`,\n    grouped by `by`. Under the hood it uses `nlargest/nsmallest`,\n    for numeric columns, which avoids sorting the entire dataframe,\n    and is usually more performant. For non-numeric columns, `pd.sort_values`\n    is used.\n    No sorting is done to the `by` column(s); the order is maintained\n    in the final output.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame(\n        ...     {\n        ...         \"age\": [20, 23, 22, 43, 21],\n        ...         \"id\": [1, 4, 6, 2, 5],\n        ...         \"result\": [\"pass\", \"pass\", \"fail\", \"pass\", \"fail\"],\n        ...     }\n        ... )\n        &gt;&gt;&gt; df\n           age  id result\n        0   20   1   pass\n        1   23   4   pass\n        2   22   6   fail\n        3   43   2   pass\n        4   21   5   fail\n\n        Ascending top 3:\n\n        &gt;&gt;&gt; df.groupby_topk(by=\"result\", column=\"age\", k=3)\n           age  id result\n        0   20   1   pass\n        1   23   4   pass\n        2   43   2   pass\n        3   21   5   fail\n        4   22   6   fail\n\n        Descending top 2:\n\n        &gt;&gt;&gt; df.groupby_topk(\n        ...     by=\"result\", column=\"age\", k=2, ascending=False, ignore_index=False\n        ... )\n           age  id result\n        3   43   2   pass\n        1   23   4   pass\n        2   22   6   fail\n        4   21   5   fail\n\n    Args:\n        df: A pandas DataFrame.\n        by: Column name(s) to group input DataFrame `df` by.\n        column: Name of the column that determines `k` rows\n            to return.\n        k: Number of top rows to return for each group.\n        dropna: If `True`, and `NA` values exist in `by`, the `NA`\n            values are not used in the groupby computation to get the relevant\n            `k` rows. If `False`, and `NA` values exist in `by`, then the `NA`\n            values are used in the groupby computation to get the relevant\n            `k` rows.\n        ascending: If `True`, the smallest top `k` rows,\n            determined by `column` are returned; if `False, the largest top `k`\n            rows, determined by `column` are returned.\n        ignore_index: If `True`, the original index is ignored.\n            If `False`, the original index for the top `k` rows is retained.\n\n    Raises:\n        ValueError: If `k` is less than 1.\n\n    Returns:\n        A pandas DataFrame with top `k` rows per `column`, grouped by `by`.\n    \"\"\"  # noqa: E501\n\n    if isinstance(by, Hashable):\n        by = [by]\n\n    check(\"by\", by, [Hashable, list])\n\n    check_column(df, [column])\n    check_column(df, by)\n\n    if k &lt; 1:\n        raise ValueError(\n            \"Numbers of rows per group \"\n            \"to be returned must be greater than 0.\"\n        )\n\n    indices = df.groupby(by=by, dropna=dropna, sort=False, observed=True)\n    indices = indices[column]\n\n    try:\n        if ascending:\n            indices = indices.nsmallest(n=k)\n        else:\n            indices = indices.nlargest(n=k)\n    except TypeError:\n        indices = indices.apply(\n            lambda d: d.sort_values(ascending=ascending).head(k)\n        )\n\n    indices = indices.index.get_level_values(-1)\n    if ignore_index:\n        return df.loc[indices].reset_index(drop=True)\n    return df.loc[indices]\n</code></pre>"},{"location":"api/functions/#janitor.functions.impute","title":"<code>impute</code>","text":"<p>Implementation of <code>impute</code> function</p>"},{"location":"api/functions/#janitor.functions.impute.impute","title":"<code>impute(df, column_names, value=None, statistic_column_name=None)</code>","text":"<p>Method-chainable imputation of values in a column.</p> <p>This method does not mutate the original DataFrame.</p> <p>Underneath the hood, this function calls the <code>.fillna()</code> method available to every <code>pandas.Series</code> object.</p> <p>Either one of <code>value</code> or <code>statistic_column_name</code> should be provided.</p> <p>If <code>value</code> is provided, then all null values in the selected column will take on the value provided.</p> <p>If <code>statistic_column_name</code> is provided, then all null values in the selected column(s) will take on the summary statistic value of other non-null values.</p> <p>Column selection in <code>column_names</code> is possible using the <code>select</code> syntax.</p> <p>Currently supported statistics include:</p> <ul> <li><code>mean</code> (also aliased by <code>average</code>)</li> <li><code>median</code></li> <li><code>mode</code></li> <li><code>minimum</code> (also aliased by <code>min</code>)</li> <li><code>maximum</code> (also aliased by <code>max</code>)</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"a\": [1, 2, 3],\n...     \"sales\": np.nan,\n...     \"score\": [np.nan, 3, 2],\n... })\n&gt;&gt;&gt; df\n   a  sales  score\n0  1    NaN    NaN\n1  2    NaN    3.0\n2  3    NaN    2.0\n</code></pre> <p>Imputing null values with 0 (using the <code>value</code> parameter):</p> <pre><code>&gt;&gt;&gt; df.impute(column_names=\"sales\", value=0.0)\n   a  sales  score\n0  1    0.0    NaN\n1  2    0.0    3.0\n2  3    0.0    2.0\n</code></pre> <p>Imputing null values with median (using the <code>statistic_column_name</code> parameter):</p> <pre><code>&gt;&gt;&gt; df.impute(column_names=\"score\", statistic_column_name=\"median\")\n   a  sales  score\n0  1    NaN    2.5\n1  2    NaN    3.0\n2  3    NaN    2.0\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>column_names</code> <code>Any</code> <p>The name of the column(s) on which to impute values.</p> required <code>value</code> <code>Optional[Any]</code> <p>The value used for imputation, passed into <code>.fillna</code> method of the underlying pandas Series.</p> <code>None</code> <code>statistic_column_name</code> <code>Optional[str]</code> <p>The column statistic to impute.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both <code>value</code> and <code>statistic_column_name</code> are provided.</p> <code>KeyError</code> <p>If <code>statistic_column_name</code> is not one of <code>mean</code>, <code>average</code>, <code>median</code>, <code>mode</code>, <code>minimum</code>, <code>min</code>, <code>maximum</code>, or <code>max</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>An imputed pandas DataFrame.</p> Source code in <code>janitor/functions/impute.py</code> <pre><code>@pf.register_dataframe_method\n@deprecated_alias(column=\"column_name\")\n@deprecated_alias(column_name=\"column_names\")\n@deprecated_alias(statistic=\"statistic_column_name\")\ndef impute(\n    df: pd.DataFrame,\n    column_names: Any,\n    value: Optional[Any] = None,\n    statistic_column_name: Optional[str] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Method-chainable imputation of values in a column.\n\n    This method does not mutate the original DataFrame.\n\n    Underneath the hood, this function calls the `.fillna()` method available\n    to every `pandas.Series` object.\n\n    Either one of `value` or `statistic_column_name` should be provided.\n\n    If `value` is provided, then all null values in the selected column will\n    take on the value provided.\n\n    If `statistic_column_name` is provided, then all null values in the\n    selected column(s) will take on the summary statistic value\n    of other non-null values.\n\n    Column selection in `column_names` is possible using the\n    [`select`][janitor.functions.select.select] syntax.\n\n    Currently supported statistics include:\n\n    - `mean` (also aliased by `average`)\n    - `median`\n    - `mode`\n    - `minimum` (also aliased by `min`)\n    - `maximum` (also aliased by `max`)\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     \"a\": [1, 2, 3],\n        ...     \"sales\": np.nan,\n        ...     \"score\": [np.nan, 3, 2],\n        ... })\n        &gt;&gt;&gt; df\n           a  sales  score\n        0  1    NaN    NaN\n        1  2    NaN    3.0\n        2  3    NaN    2.0\n\n        Imputing null values with 0 (using the `value` parameter):\n\n        &gt;&gt;&gt; df.impute(column_names=\"sales\", value=0.0)\n           a  sales  score\n        0  1    0.0    NaN\n        1  2    0.0    3.0\n        2  3    0.0    2.0\n\n        Imputing null values with median (using the `statistic_column_name`\n        parameter):\n\n        &gt;&gt;&gt; df.impute(column_names=\"score\", statistic_column_name=\"median\")\n           a  sales  score\n        0  1    NaN    2.5\n        1  2    NaN    3.0\n        2  3    NaN    2.0\n\n    Args:\n        df: A pandas DataFrame.\n        column_names: The name of the column(s) on which to impute values.\n        value: The value used for imputation, passed into `.fillna` method\n            of the underlying pandas Series.\n        statistic_column_name: The column statistic to impute.\n\n    Raises:\n        ValueError: If both `value` and `statistic_column_name` are\n            provided.\n        KeyError: If `statistic_column_name` is not one of `mean`,\n            `average`, `median`, `mode`, `minimum`, `min`, `maximum`, or\n            `max`.\n\n    Returns:\n        An imputed pandas DataFrame.\n    \"\"\"\n    # Firstly, we check that only one of `value` or `statistic` are provided.\n    if (value is None) and (statistic_column_name is None):\n        raise ValueError(\"Kindly specify a value or a statistic_column_name\")\n\n    if value is not None and statistic_column_name is not None:\n        raise ValueError(\n            \"Only one of `value` or `statistic_column_name` should be \"\n            \"provided.\"\n        )\n\n    column_names = get_index_labels([column_names], df, axis=\"columns\")\n\n    if value is not None:\n        value = dict(product(column_names, [value]))\n\n    else:\n        # If statistic is provided, then we compute\n        # the relevant summary statistic\n        # from the other data.\n        funcs = {\n            \"mean\": \"mean\",\n            \"average\": \"mean\",  # aliased\n            \"median\": \"median\",\n            \"mode\": \"mode\",\n            \"minimum\": \"min\",\n            \"min\": \"min\",  # aliased\n            \"maximum\": \"max\",\n            \"max\": \"max\",  # aliased\n        }\n        # Check that the statistic keyword argument is one of the approved.\n        if statistic_column_name not in funcs:\n            raise KeyError(\n                f\"`statistic_column_name` must be one of {funcs.keys()}.\"\n            )\n\n        value = dict(product(column_names, [funcs[statistic_column_name]]))\n\n        value = df.agg(value)\n\n        # special treatment for mode\n        if statistic_column_name == \"mode\":\n            value = {key: val.at[0] for key, val in value.items()}\n\n    return df.fillna(value=value)\n</code></pre>"},{"location":"api/functions/#janitor.functions.jitter","title":"<code>jitter</code>","text":"<p>Implementation of the <code>jitter</code> function.</p>"},{"location":"api/functions/#janitor.functions.jitter.jitter","title":"<code>jitter(df, column_name, dest_column_name, scale, clip=None, random_state=None)</code>","text":"<p>Adds Gaussian noise (jitter) to the values of a column.</p> <p>A new column will be created containing the values of the original column with Gaussian noise added. For each value in the column, a Gaussian distribution is created having a location (mean) equal to the value and a scale (standard deviation) equal to <code>scale</code>. A random value is then sampled from this distribution, which is the jittered value. If a tuple is supplied for <code>clip</code>, then any values of the new column less than <code>clip[0]</code> will be set to <code>clip[0]</code>, and any values greater than <code>clip[1]</code> will be set to <code>clip[1]</code>. Additionally, if a numeric value is supplied for <code>random_state</code>, this value will be used to set the random seed used for sampling. NaN values are ignored in this method.</p> <p>This method mutates the original DataFrame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\"a\": [3, 4, 5, np.nan]})\n&gt;&gt;&gt; df\n     a\n0  3.0\n1  4.0\n2  5.0\n3  NaN\n&gt;&gt;&gt; df.jitter(\"a\", dest_column_name=\"a_jit\", scale=1, random_state=42)\n     a     a_jit\n0  3.0  3.496714\n1  4.0  3.861736\n2  5.0  5.647689\n3  NaN       NaN\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>column_name</code> <code>Hashable</code> <p>Name of the column containing values to add Gaussian jitter to.</p> required <code>dest_column_name</code> <code>str</code> <p>The name of the new column containing the jittered values that will be created.</p> required <code>scale</code> <code>number</code> <p>A positive value multiplied by the original column value to determine the scale (standard deviation) of the Gaussian distribution to sample from. (A value of zero results in no jittering.)</p> required <code>clip</code> <code>Optional[Iterable[number]]</code> <p>An iterable of two values (minimum and maximum) to clip the jittered values to, default to None.</p> <code>None</code> <code>random_state</code> <code>Optional[number]</code> <p>An integer or 1-d array value used to set the random seed, default to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>column_name</code> is not numeric.</p> <code>ValueError</code> <p>If <code>scale</code> is not a numerical value greater than <code>0</code>.</p> <code>ValueError</code> <p>If <code>clip</code> is not an iterable of length <code>2</code>.</p> <code>ValueError</code> <p>If <code>clip[0]</code> is greater than <code>clip[1]</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with a new column containing Gaussian-jittered values from another column.</p> Source code in <code>janitor/functions/jitter.py</code> <pre><code>@pf.register_dataframe_method\ndef jitter(\n    df: pd.DataFrame,\n    column_name: Hashable,\n    dest_column_name: str,\n    scale: np.number,\n    clip: Optional[Iterable[np.number]] = None,\n    random_state: Optional[np.number] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Adds Gaussian noise (jitter) to the values of a column.\n\n    A new column will be created containing the values of the original column\n    with Gaussian noise added.\n    For each value in the column, a Gaussian distribution is created\n    having a location (mean) equal to the value\n    and a scale (standard deviation) equal to `scale`.\n    A random value is then sampled from this distribution,\n    which is the jittered value.\n    If a tuple is supplied for `clip`,\n    then any values of the new column less than `clip[0]`\n    will be set to `clip[0]`,\n    and any values greater than `clip[1]` will be set to `clip[1]`.\n    Additionally, if a numeric value is supplied for `random_state`,\n    this value will be used to set the random seed used for sampling.\n    NaN values are ignored in this method.\n\n    This method mutates the original DataFrame.\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\"a\": [3, 4, 5, np.nan]})\n        &gt;&gt;&gt; df\n             a\n        0  3.0\n        1  4.0\n        2  5.0\n        3  NaN\n        &gt;&gt;&gt; df.jitter(\"a\", dest_column_name=\"a_jit\", scale=1, random_state=42)\n             a     a_jit\n        0  3.0  3.496714\n        1  4.0  3.861736\n        2  5.0  5.647689\n        3  NaN       NaN\n\n    Args:\n        df: A pandas DataFrame.\n        column_name: Name of the column containing\n            values to add Gaussian jitter to.\n        dest_column_name: The name of the new column containing the\n            jittered values that will be created.\n        scale: A positive value multiplied by the original\n            column value to determine the scale (standard deviation) of the\n            Gaussian distribution to sample from. (A value of zero results in\n            no jittering.)\n        clip: An iterable of two values (minimum and maximum) to clip\n            the jittered values to, default to None.\n        random_state: An integer or 1-d array value used to set the random\n            seed, default to None.\n\n    Raises:\n        TypeError: If `column_name` is not numeric.\n        ValueError: If `scale` is not a numerical value\n            greater than `0`.\n        ValueError: If `clip` is not an iterable of length `2`.\n        ValueError: If `clip[0]` is greater than `clip[1]`.\n\n    Returns:\n        A pandas DataFrame with a new column containing\n            Gaussian-jittered values from another column.\n    \"\"\"\n\n    # Check types\n    check(\"scale\", scale, [int, float])\n\n    # Check that `column_name` is a numeric column\n    if not np.issubdtype(df[column_name].dtype, np.number):\n        raise TypeError(f\"{column_name} must be a numeric column.\")\n\n    if scale &lt;= 0:\n        raise ValueError(\"`scale` must be a numeric value greater than 0.\")\n    values = df[column_name]\n    if random_state is not None:\n        np.random.seed(random_state)\n    result = np.random.normal(loc=values, scale=scale)\n    if clip:\n        # Ensure `clip` has length 2\n        if len(clip) != 2:\n            raise ValueError(\"`clip` must be an iterable of length 2.\")\n        # Ensure the values in `clip` are ordered as min, max\n        if clip[1] &lt; clip[0]:\n            raise ValueError(\n                \"`clip[0]` must be less than or equal to `clip[1]`.\"\n            )\n        result = np.clip(result, *clip)\n    df[dest_column_name] = result\n\n    return df\n</code></pre>"},{"location":"api/functions/#janitor.functions.join_apply","title":"<code>join_apply</code>","text":"<p>Implementation of the <code>join_apply</code> function</p>"},{"location":"api/functions/#janitor.functions.join_apply.join_apply","title":"<code>join_apply(df, func, new_column_name)</code>","text":"<p>Join the result of applying a function across dataframe rows.</p> <p>This method does not mutate the original DataFrame.</p> <p>This is a convenience function that allows us to apply arbitrary functions that take any combination of information from any of the columns. The only requirement is that the function signature takes in a row from the DataFrame.</p> <p>Examples:</p> <p>Sum the result of two columns into a new column.</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\"a\":[1, 2, 3], \"b\": [2, 3, 4]})\n&gt;&gt;&gt; df\n   a  b\n0  1  2\n1  2  3\n2  3  4\n&gt;&gt;&gt; df.join_apply(\n...     func=lambda x: 2 * x[\"a\"] + x[\"b\"],\n...     new_column_name=\"2a+b\",\n... )\n   a  b  2a+b\n0  1  2     4\n1  2  3     7\n2  3  4    10\n</code></pre> <p>Incorporating conditionals in <code>func</code>.</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [20, 30, 40]})\n&gt;&gt;&gt; df\n   a   b\n0  1  20\n1  2  30\n2  3  40\n&gt;&gt;&gt; def take_a_if_even(x):\n...     if x[\"a\"] % 2 == 0:\n...         return x[\"a\"]\n...     else:\n...         return x[\"b\"]\n&gt;&gt;&gt; df.join_apply(take_a_if_even, \"a_if_even\")\n   a   b  a_if_even\n0  1  20         20\n1  2  30          2\n2  3  40         40\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>func</code> <code>Callable</code> <p>A function that is applied elementwise across all rows of the DataFrame.</p> required <code>new_column_name</code> <code>str</code> <p>Name of the resulting column.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with new column appended.</p> Source code in <code>janitor/functions/join_apply.py</code> <pre><code>@pf.register_dataframe_method\ndef join_apply(\n    df: pd.DataFrame,\n    func: Callable,\n    new_column_name: str,\n) -&gt; pd.DataFrame:\n    \"\"\"Join the result of applying a function across dataframe rows.\n\n    This method does not mutate the original DataFrame.\n\n    This is a convenience function that allows us to apply arbitrary functions\n    that take any combination of information from any of the columns. The only\n    requirement is that the function signature takes in a row from the\n    DataFrame.\n\n    Examples:\n        Sum the result of two columns into a new column.\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\"a\":[1, 2, 3], \"b\": [2, 3, 4]})\n        &gt;&gt;&gt; df\n           a  b\n        0  1  2\n        1  2  3\n        2  3  4\n        &gt;&gt;&gt; df.join_apply(\n        ...     func=lambda x: 2 * x[\"a\"] + x[\"b\"],\n        ...     new_column_name=\"2a+b\",\n        ... )\n           a  b  2a+b\n        0  1  2     4\n        1  2  3     7\n        2  3  4    10\n\n        Incorporating conditionals in `func`.\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [20, 30, 40]})\n        &gt;&gt;&gt; df\n           a   b\n        0  1  20\n        1  2  30\n        2  3  40\n        &gt;&gt;&gt; def take_a_if_even(x):\n        ...     if x[\"a\"] % 2 == 0:\n        ...         return x[\"a\"]\n        ...     else:\n        ...         return x[\"b\"]\n        &gt;&gt;&gt; df.join_apply(take_a_if_even, \"a_if_even\")\n           a   b  a_if_even\n        0  1  20         20\n        1  2  30          2\n        2  3  40         40\n\n    Args:\n        df: A pandas DataFrame.\n        func: A function that is applied elementwise across all rows of the\n            DataFrame.\n        new_column_name: Name of the resulting column.\n\n    Returns:\n        A pandas DataFrame with new column appended.\n    \"\"\"  # noqa: E501\n    df = df.copy().join(df.apply(func, axis=1).rename(new_column_name))\n    return df\n</code></pre>"},{"location":"api/functions/#janitor.functions.label_encode","title":"<code>label_encode</code>","text":"<p>Implementation of <code>label_encode</code> function</p>"},{"location":"api/functions/#janitor.functions.label_encode.label_encode","title":"<code>label_encode(df, column_names)</code>","text":"<p>Convert labels into numerical data.</p> <p>This method will create a new column with the string <code>_enc</code> appended after the original column's name. Consider this to be syntactic sugar. This function uses the <code>factorize</code> pandas function under the hood.</p> <p>This method behaves differently from <code>encode_categorical</code>. This method creates a new column of numeric data. <code>encode_categorical</code> replaces the dtype of the original column with a categorical dtype.</p> <p>This method mutates the original DataFrame.</p> <p>Note</p> <p>This function will be deprecated in a 1.x release. Please use <code>factorize_columns</code> instead.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"foo\": [\"b\", \"b\", \"a\", \"c\", \"b\"],\n...     \"bar\": range(4, 9),\n... })\n&gt;&gt;&gt; df\n  foo  bar\n0   b    4\n1   b    5\n2   a    6\n3   c    7\n4   b    8\n&gt;&gt;&gt; df.label_encode(column_names=\"foo\")\n  foo  bar  foo_enc\n0   b    4        0\n1   b    5        0\n2   a    6        1\n3   c    7        2\n4   b    8        0\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The pandas DataFrame object.</p> required <code>column_names</code> <code>Union[str, Iterable[str], Hashable]</code> <p>A column name or an iterable (list or tuple) of column names.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame.</p> Source code in <code>janitor/functions/label_encode.py</code> <pre><code>@pf.register_dataframe_method\n@refactored_function(\n    message=(\n        \"This function will be deprecated in a 1.x release. \"\n        \"Please use `janitor.factorize_columns` instead.\"\n    )\n)\n@deprecated_alias(columns=\"column_names\")\ndef label_encode(\n    df: pd.DataFrame,\n    column_names: Union[str, Iterable[str], Hashable],\n) -&gt; pd.DataFrame:\n    \"\"\"Convert labels into numerical data.\n\n    This method will create a new column with the string `_enc` appended\n    after the original column's name.\n    Consider this to be syntactic sugar.\n    This function uses the `factorize` pandas function under the hood.\n\n    This method behaves differently from\n    [`encode_categorical`][janitor.functions.encode_categorical.encode_categorical].\n    This method creates a new column of numeric data.\n    [`encode_categorical`][janitor.functions.encode_categorical.encode_categorical]\n    replaces the dtype of the original column with a *categorical* dtype.\n\n    This method mutates the original DataFrame.\n\n    !!!note\n\n        This function will be deprecated in a 1.x release.\n        Please use [`factorize_columns`][janitor.functions.factorize_columns.factorize_columns]\n        instead.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     \"foo\": [\"b\", \"b\", \"a\", \"c\", \"b\"],\n        ...     \"bar\": range(4, 9),\n        ... })\n        &gt;&gt;&gt; df\n          foo  bar\n        0   b    4\n        1   b    5\n        2   a    6\n        3   c    7\n        4   b    8\n        &gt;&gt;&gt; df.label_encode(column_names=\"foo\")\n          foo  bar  foo_enc\n        0   b    4        0\n        1   b    5        0\n        2   a    6        1\n        3   c    7        2\n        4   b    8        0\n\n    Args:\n        df: The pandas DataFrame object.\n        column_names: A column name or an iterable (list\n            or tuple) of column names.\n\n    Returns:\n        A pandas DataFrame.\n    \"\"\"  # noqa: E501\n    warnings.warn(\n        \"`label_encode` will be deprecated in a 1.x release. \"\n        \"Please use `factorize_columns` instead.\"\n    )\n    df = _factorize(df, column_names, \"_enc\")\n    return df\n</code></pre>"},{"location":"api/functions/#janitor.functions.limit_column_characters","title":"<code>limit_column_characters</code>","text":"<p>Implementation of limit_column_characters.</p>"},{"location":"api/functions/#janitor.functions.limit_column_characters.limit_column_characters","title":"<code>limit_column_characters(df, column_length, col_separator='_')</code>","text":"<p>Truncate column sizes to a specific length.</p> <p>This method mutates the original DataFrame.</p> <p>Method chaining will truncate all columns to a given length and append a given separator character with the index of duplicate columns, except for the first distinct column name.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; data_dict = {\n...     \"really_long_name\": [9, 8, 7],\n...     \"another_really_long_name\": [2, 4, 6],\n...     \"another_really_longer_name\": list(\"xyz\"),\n...     \"this_is_getting_out_of_hand\": list(\"pqr\"),\n... }\n&gt;&gt;&gt; df = pd.DataFrame(data_dict)\n&gt;&gt;&gt; df\n   really_long_name  another_really_long_name another_really_longer_name this_is_getting_out_of_hand\n0                 9                         2                          x                           p\n1                 8                         4                          y                           q\n2                 7                         6                          z                           r\n&gt;&gt;&gt; df.limit_column_characters(7)\n   really_  another another_1 this_is\n0        9        2         x       p\n1        8        4         y       q\n2        7        6         z       r\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>column_length</code> <code>int</code> <p>Character length for which to truncate all columns. The column separator value and number for duplicate column name does not contribute. Therefore, if all columns are truncated to 10 characters, the first distinct column will be 10 characters and the remaining will be 12 characters (assuming a column separator of one character).</p> required <code>col_separator</code> <code>str</code> <p>The separator to use for counting distinct column values, for example, <code>'_'</code> or <code>'.'</code>. Supply an empty string (i.e. <code>''</code>) to remove the separator.</p> <code>'_'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with truncated column lengths.</p> Source code in <code>janitor/functions/limit_column_characters.py</code> <pre><code>@pf.register_dataframe_method\ndef limit_column_characters(\n    df: pd.DataFrame,\n    column_length: int,\n    col_separator: str = \"_\",\n) -&gt; pd.DataFrame:\n    \"\"\"Truncate column sizes to a specific length.\n\n    This method mutates the original DataFrame.\n\n    Method chaining will truncate all columns to a given length and append\n    a given separator character with the index of duplicate columns, except\n    for the first distinct column name.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; data_dict = {\n        ...     \"really_long_name\": [9, 8, 7],\n        ...     \"another_really_long_name\": [2, 4, 6],\n        ...     \"another_really_longer_name\": list(\"xyz\"),\n        ...     \"this_is_getting_out_of_hand\": list(\"pqr\"),\n        ... }\n        &gt;&gt;&gt; df = pd.DataFrame(data_dict)\n        &gt;&gt;&gt; df  # doctest: +SKIP\n           really_long_name  another_really_long_name another_really_longer_name this_is_getting_out_of_hand\n        0                 9                         2                          x                           p\n        1                 8                         4                          y                           q\n        2                 7                         6                          z                           r\n        &gt;&gt;&gt; df.limit_column_characters(7)\n           really_  another another_1 this_is\n        0        9        2         x       p\n        1        8        4         y       q\n        2        7        6         z       r\n\n    Args:\n        df: A pandas DataFrame.\n        column_length: Character length for which to truncate all columns.\n            The column separator value and number for duplicate column name does\n            not contribute. Therefore, if all columns are truncated to 10\n            characters, the first distinct column will be 10 characters and the\n            remaining will be 12 characters (assuming a column separator of one\n            character).\n        col_separator: The separator to use for counting distinct column\n            values, for example, `'_'` or `'.'`.\n            Supply an empty string (i.e. `''`) to remove the separator.\n\n    Returns:\n        A pandas DataFrame with truncated column lengths.\n    \"\"\"  # noqa: E501\n\n    check(\"column_length\", column_length, [int])\n    check(\"col_separator\", col_separator, [str])\n\n    col_names = df.columns\n    col_names = [col_name[:column_length] for col_name in col_names]\n\n    col_name_set = set(col_names)\n    col_name_count = {}\n\n    # If no columns are duplicates, we can skip the loops below.\n    if len(col_name_set) == len(col_names):\n        df.columns = col_names\n        return df\n\n    for col_name_to_check in col_name_set:\n        count = 0\n        for idx, col_name in enumerate(col_names):\n            if col_name_to_check == col_name:\n                col_name_count[idx] = count\n                count += 1\n\n    final_col_names = []\n    for idx, col_name in enumerate(col_names):\n        if col_name_count[idx] &gt; 0:\n            col_name_to_append = (\n                col_name + col_separator + str(col_name_count[idx])\n            )\n            final_col_names.append(col_name_to_append)\n        else:\n            final_col_names.append(col_name)\n\n    df.columns = final_col_names\n    return df\n</code></pre>"},{"location":"api/functions/#janitor.functions.min_max_scale","title":"<code>min_max_scale</code>","text":""},{"location":"api/functions/#janitor.functions.min_max_scale.min_max_scale","title":"<code>min_max_scale(df, feature_range=(0, 1), column_name=None, jointly=False)</code>","text":"<p>Scales DataFrame to between a minimum and maximum value.</p> <p>One can optionally set a new target minimum and maximum value using the <code>feature_range</code> keyword argument.</p> <p>If <code>column_name</code> is specified, then only that column(s) of data is scaled. Otherwise, the entire dataframe is scaled. If <code>jointly</code> is <code>True</code>, the <code>column_names</code> provided entire dataframe will be regnozied as the one to jointly scale. Otherwise, each column of data will be scaled separately.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({'a':[1, 2], 'b':[0, 1]})\n&gt;&gt;&gt; df.min_max_scale()\n     a    b\n0  0.0  0.0\n1  1.0  1.0\n&gt;&gt;&gt; df.min_max_scale(jointly=True)\n     a    b\n0  0.5  0.0\n1  1.0  0.5\n</code></pre> <p>Setting custom minimum and maximum.</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({'a':[1, 2], 'b':[0, 1]})\n&gt;&gt;&gt; df.min_max_scale(feature_range=(0, 100))\n       a      b\n0    0.0    0.0\n1  100.0  100.0\n&gt;&gt;&gt; df.min_max_scale(feature_range=(0, 100), jointly=True)\n       a     b\n0   50.0   0.0\n1  100.0  50.0\n</code></pre> <p>Apply min-max to the selected columns.</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({'a':[1, 2], 'b':[0, 1], 'c': [1, 0]})\n&gt;&gt;&gt; df.min_max_scale(\n...     feature_range=(0, 100),\n...     column_name=[\"a\", \"c\"],\n... )\n       a  b      c\n0    0.0  0  100.0\n1  100.0  1    0.0\n&gt;&gt;&gt; df.min_max_scale(\n...     feature_range=(0, 100),\n...     column_name=[\"a\", \"c\"],\n...     jointly=True,\n... )\n       a  b     c\n0   50.0  0  50.0\n1  100.0  1   0.0\n&gt;&gt;&gt; df.min_max_scale(feature_range=(0, 100), column_name='a')\n       a  b  c\n0    0.0  0  1\n1  100.0  1  0\n</code></pre> <p>The aforementioned example might be applied to something like scaling the isoelectric points of amino acids. While technically they range from approx 3-10, we can also think of them on the pH scale which ranges from 1 to 14. Hence, 3 gets scaled not to 0 but approx. 0.15 instead, while 10 gets scaled to approx. 0.69 instead.</p> <p>Version Changed</p> <ul> <li>0.24.0<ul> <li>Deleted <code>old_min</code>, <code>old_max</code>, <code>new_min</code>, and <code>new_max</code> options.</li> <li>Added <code>feature_range</code>, and <code>jointly</code> options.</li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>feature_range</code> <code>tuple[int | float, int | float]</code> <p>Desired range of transformed data.</p> <code>(0, 1)</code> <code>column_name</code> <code>str | int | list[str | int] | Index</code> <p>The column on which to perform scaling.</p> <code>None</code> <code>jointly</code> <code>bool</code> <p>Scale the entire data if True.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>feature_range</code> isn't tuple type.</p> <code>ValueError</code> <p>If the length of <code>feature_range</code> isn't equal to two.</p> <code>ValueError</code> <p>If the element of <code>feature_range</code> isn't number type.</p> <code>ValueError</code> <p>If <code>feature_range[1]</code> &lt;= <code>feature_range[0]</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with scaled data.</p> Source code in <code>janitor/functions/min_max_scale.py</code> <pre><code>@pf.register_dataframe_method\n@deprecated_kwargs(\n    \"old_min\",\n    \"old_max\",\n    \"new_min\",\n    \"new_max\",\n    message=(\n        \"The keyword argument {argument!r} of {func_name!r} is deprecated. \"\n        \"Please use 'feature_range' instead.\"\n    ),\n)\n@deprecated_alias(col_name=\"column_name\")\ndef min_max_scale(\n    df: pd.DataFrame,\n    feature_range: tuple[int | float, int | float] = (0, 1),\n    column_name: str | int | list[str | int] | pd.Index = None,\n    jointly: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Scales DataFrame to between a minimum and maximum value.\n\n    One can optionally set a new target **minimum** and **maximum** value\n    using the `feature_range` keyword argument.\n\n    If `column_name` is specified, then only that column(s) of data is scaled.\n    Otherwise, the entire dataframe is scaled.\n    If `jointly` is `True`, the `column_names` provided entire dataframe will\n    be regnozied as the one to jointly scale. Otherwise, each column of data\n    will be scaled separately.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({'a':[1, 2], 'b':[0, 1]})\n        &gt;&gt;&gt; df.min_max_scale()\n             a    b\n        0  0.0  0.0\n        1  1.0  1.0\n        &gt;&gt;&gt; df.min_max_scale(jointly=True)\n             a    b\n        0  0.5  0.0\n        1  1.0  0.5\n\n        Setting custom minimum and maximum.\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({'a':[1, 2], 'b':[0, 1]})\n        &gt;&gt;&gt; df.min_max_scale(feature_range=(0, 100))\n               a      b\n        0    0.0    0.0\n        1  100.0  100.0\n        &gt;&gt;&gt; df.min_max_scale(feature_range=(0, 100), jointly=True)\n               a     b\n        0   50.0   0.0\n        1  100.0  50.0\n\n        Apply min-max to the selected columns.\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({'a':[1, 2], 'b':[0, 1], 'c': [1, 0]})\n        &gt;&gt;&gt; df.min_max_scale(\n        ...     feature_range=(0, 100),\n        ...     column_name=[\"a\", \"c\"],\n        ... )\n               a  b      c\n        0    0.0  0  100.0\n        1  100.0  1    0.0\n        &gt;&gt;&gt; df.min_max_scale(\n        ...     feature_range=(0, 100),\n        ...     column_name=[\"a\", \"c\"],\n        ...     jointly=True,\n        ... )\n               a  b     c\n        0   50.0  0  50.0\n        1  100.0  1   0.0\n        &gt;&gt;&gt; df.min_max_scale(feature_range=(0, 100), column_name='a')\n               a  b  c\n        0    0.0  0  1\n        1  100.0  1  0\n\n        The aforementioned example might be applied to something like scaling the\n        isoelectric points of amino acids. While technically they range from\n        approx 3-10, we can also think of them on the pH scale which ranges from\n        1 to 14. Hence, 3 gets scaled not to 0 but approx. 0.15 instead, while 10\n        gets scaled to approx. 0.69 instead.\n\n    !!! summary \"Version Changed\"\n\n        - 0.24.0\n            - Deleted `old_min`, `old_max`, `new_min`, and `new_max` options.\n            - Added `feature_range`, and `jointly` options.\n\n    Args:\n        df: A pandas DataFrame.\n        feature_range: Desired range of transformed data.\n        column_name: The column on which to perform scaling.\n        jointly: Scale the entire data if True.\n\n    Raises:\n        ValueError: If `feature_range` isn't tuple type.\n        ValueError: If the length of `feature_range` isn't equal to two.\n        ValueError: If the element of `feature_range` isn't number type.\n        ValueError: If `feature_range[1]` &lt;= `feature_range[0]`.\n\n    Returns:\n        A pandas DataFrame with scaled data.\n    \"\"\"  # noqa: E501\n\n    if not (\n        isinstance(feature_range, (tuple, list))\n        and len(feature_range) == 2\n        and all((isinstance(i, (int, float))) for i in feature_range)\n        and feature_range[1] &gt; feature_range[0]\n    ):\n        raise ValueError(\n            \"`feature_range` should be a range type contains number element, \"\n            \"the first element must be greater than the second one\"\n        )\n\n    if column_name is not None:\n        df = df.copy()  # Avoid to change the original DataFrame.\n\n        old_feature_range = df[column_name].pipe(_min_max_value, jointly)\n        df[column_name] = df[column_name].pipe(\n            _apply_min_max,\n            *old_feature_range,\n            *feature_range,\n        )\n    else:\n        old_feature_range = df.pipe(_min_max_value, jointly)\n        df = df.pipe(\n            _apply_min_max,\n            *old_feature_range,\n            *feature_range,\n        )\n\n    return df\n</code></pre>"},{"location":"api/functions/#janitor.functions.move","title":"<code>move</code>","text":"<p>Implementation of move.</p>"},{"location":"api/functions/#janitor.functions.move.move","title":"<code>move(df, source, target=None, position='before', axis=0)</code>","text":"<p>Changes rows or columns positions in the dataframe.</p> <p>It uses the <code>select</code> syntax, making it easy to move blocks of rows or columns at once.</p> <p>This operation does not reset the index of the dataframe. User must explicitly do so.</p> <p>The dataframe must have unique column names or indices.</p> <p>Examples:</p> <p>Move a row:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\"a\": [2, 4, 6, 8], \"b\": list(\"wxyz\")})\n&gt;&gt;&gt; df\n   a  b\n0  2  w\n1  4  x\n2  6  y\n3  8  z\n&gt;&gt;&gt; df.move(source=0, target=3, position=\"before\", axis=0)\n   a  b\n1  4  x\n2  6  y\n0  2  w\n3  8  z\n</code></pre> <p>Move a column:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; data = [{\"a\": 1, \"b\": 1, \"c\": 1,\n...          \"d\": \"a\", \"e\": \"a\",\"f\": \"a\"}]\n&gt;&gt;&gt; df = pd.DataFrame(data)\n&gt;&gt;&gt; df\n   a  b  c  d  e  f\n0  1  1  1  a  a  a\n&gt;&gt;&gt; df.move(source=\"a\", target=\"c\", position=\"after\", axis=1)\n   b  c  a  d  e  f\n0  1  1  1  a  a  a\n&gt;&gt;&gt; df.move(source=\"f\", target=\"b\", position=\"before\", axis=1)\n   a  f  b  c  d  e\n0  1  a  1  1  a  a\n&gt;&gt;&gt; df.move(source=\"a\", target=None, position=\"after\", axis=1)\n   b  c  d  e  f  a\n0  1  1  a  a  a  1\n</code></pre> <p>Move columns:</p> <pre><code>&gt;&gt;&gt; from pandas.api.types import is_numeric_dtype, is_string_dtype\n&gt;&gt;&gt; df.move(source=is_string_dtype, target=None, position=\"before\", axis=1)\n   d  e  f  a  b  c\n0  a  a  a  1  1  1\n&gt;&gt;&gt; df.move(source=is_numeric_dtype, target=None, position=\"after\", axis=1)\n   d  e  f  a  b  c\n0  a  a  a  1  1  1\n&gt;&gt;&gt; df.move(source = [\"d\", \"f\"], target=is_numeric_dtype, position=\"before\", axis=1)\n   d  f  a  b  c  e\n0  a  a  1  1  1  a\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The pandas DataFrame object.</p> required <code>source</code> <code>Any</code> <p>Columns or rows to move.</p> required <code>target</code> <code>Any</code> <p>Columns or rows to move adjacent to. If <code>None</code> and <code>position == 'before'</code>, <code>source</code> is moved to the beginning; if <code>position == 'after'</code>, <code>source</code> is moved to the end.</p> <code>None</code> <code>position</code> <code>str</code> <p>Specifies the destination of the columns/rows. Values can be either <code>before</code> or <code>after</code>; defaults to <code>before</code>.</p> <code>'before'</code> <code>axis</code> <code>int</code> <p>Axis along which the function is applied. 0 to move along the index, 1 to move along the columns.</p> <code>0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>axis</code> is not <code>0</code> or <code>1</code>.</p> <code>ValueError</code> <p>If <code>position</code> is not <code>before</code> or <code>after</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The dataframe with the Series moved.</p> Source code in <code>janitor/functions/move.py</code> <pre><code>@pf.register_dataframe_method\ndef move(\n    df: pd.DataFrame,\n    source: Any,\n    target: Any = None,\n    position: str = \"before\",\n    axis: int = 0,\n) -&gt; pd.DataFrame:\n    \"\"\"Changes rows or columns positions in the dataframe.\n\n    It uses the\n    [`select`][janitor.functions.select.select] syntax,\n    making it easy to move blocks of rows or columns at once.\n\n    This operation does not reset the index of the dataframe. User must\n    explicitly do so.\n\n    The dataframe must have unique column names or indices.\n\n    Examples:\n        Move a row:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\"a\": [2, 4, 6, 8], \"b\": list(\"wxyz\")})\n        &gt;&gt;&gt; df\n           a  b\n        0  2  w\n        1  4  x\n        2  6  y\n        3  8  z\n        &gt;&gt;&gt; df.move(source=0, target=3, position=\"before\", axis=0)\n           a  b\n        1  4  x\n        2  6  y\n        0  2  w\n        3  8  z\n\n        Move a column:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; data = [{\"a\": 1, \"b\": 1, \"c\": 1,\n        ...          \"d\": \"a\", \"e\": \"a\",\"f\": \"a\"}]\n        &gt;&gt;&gt; df = pd.DataFrame(data)\n        &gt;&gt;&gt; df\n           a  b  c  d  e  f\n        0  1  1  1  a  a  a\n        &gt;&gt;&gt; df.move(source=\"a\", target=\"c\", position=\"after\", axis=1)\n           b  c  a  d  e  f\n        0  1  1  1  a  a  a\n        &gt;&gt;&gt; df.move(source=\"f\", target=\"b\", position=\"before\", axis=1)\n           a  f  b  c  d  e\n        0  1  a  1  1  a  a\n        &gt;&gt;&gt; df.move(source=\"a\", target=None, position=\"after\", axis=1)\n           b  c  d  e  f  a\n        0  1  1  a  a  a  1\n\n        Move columns:\n        &gt;&gt;&gt; from pandas.api.types import is_numeric_dtype, is_string_dtype\n        &gt;&gt;&gt; df.move(source=is_string_dtype, target=None, position=\"before\", axis=1)\n           d  e  f  a  b  c\n        0  a  a  a  1  1  1\n        &gt;&gt;&gt; df.move(source=is_numeric_dtype, target=None, position=\"after\", axis=1)\n           d  e  f  a  b  c\n        0  a  a  a  1  1  1\n        &gt;&gt;&gt; df.move(source = [\"d\", \"f\"], target=is_numeric_dtype, position=\"before\", axis=1)\n           d  f  a  b  c  e\n        0  a  a  1  1  1  a\n\n    Args:\n        df: The pandas DataFrame object.\n        source: Columns or rows to move.\n        target: Columns or rows to move adjacent to.\n            If `None` and `position == 'before'`, `source`\n            is moved to the beginning; if `position == 'after'`,\n            `source` is moved to the end.\n        position: Specifies the destination of the columns/rows.\n            Values can be either `before` or `after`; defaults to `before`.\n        axis: Axis along which the function is applied. 0 to move along\n            the index, 1 to move along the columns.\n\n    Raises:\n        ValueError: If `axis` is not `0` or `1`.\n        ValueError: If `position` is not `before` or `after`.\n\n    Returns:\n        The dataframe with the Series moved.\n    \"\"\"  # noqa: E501\n    if axis not in [0, 1]:\n        raise ValueError(f\"Invalid axis '{axis}'. Can only be 0 or 1.\")\n\n    if position not in [\"before\", \"after\"]:\n        raise ValueError(\n            f\"Invalid position '{position}'. Can only be 'before' or 'after'.\"\n        )\n\n    mapping = {0: \"index\", 1: \"columns\"}\n    names = getattr(df, mapping[axis])\n\n    assert names.is_unique\n\n    index = np.arange(names.size)\n    source = _select_index([source], df, mapping[axis])\n    source = _index_converter(source, index)\n    if target is None:\n        if position == \"after\":\n            target = np.array([names.size])\n        else:\n            target = np.array([0])\n    else:\n        target = _select_index([target], df, mapping[axis])\n        target = _index_converter(target, index)\n    index = np.delete(index, source)\n\n    if position == \"before\":\n        position = index.searchsorted(target[0])\n    else:\n        position = index.searchsorted(target[-1]) + 1\n    start = index[:position]\n    end = index[position:]\n    position = np.concatenate([start, source, end])\n\n    return df.iloc(axis=axis)[position]\n</code></pre>"},{"location":"api/functions/#janitor.functions.pivot","title":"<code>pivot</code>","text":""},{"location":"api/functions/#janitor.functions.pivot.pivot_longer","title":"<code>pivot_longer(df, index=None, column_names=None, names_to=None, values_to='value', column_level=None, names_sep=None, names_pattern=None, names_transform=None, dropna=False, sort_by_appearance=False, ignore_index=True)</code>","text":"<p>Unpivots a DataFrame from wide to long format.</p> <p>This method does not mutate the original DataFrame.</p> <p>It is modeled after the <code>pivot_longer</code> function in R's tidyr package, and also takes inspiration from R's data.table package.</p> <p>This function is useful to massage a DataFrame into a format where one or more columns are considered measured variables, and all other columns are considered as identifier variables.</p> <p>All measured variables are unpivoted (and typically duplicated) along the row axis.</p> <p>Column selection in <code>index</code> and <code>column_names</code> is possible using the <code>select</code> syntax.</p> <p>For more granular control on the unpivoting, have a look at <code>pivot_longer_spec</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame(\n...     {\n...         \"Sepal.Length\": [5.1, 5.9],\n...         \"Sepal.Width\": [3.5, 3.0],\n...         \"Petal.Length\": [1.4, 5.1],\n...         \"Petal.Width\": [0.2, 1.8],\n...         \"Species\": [\"setosa\", \"virginica\"],\n...     }\n... )\n&gt;&gt;&gt; df\n   Sepal.Length  Sepal.Width  Petal.Length  Petal.Width    Species\n0           5.1          3.5           1.4          0.2     setosa\n1           5.9          3.0           5.1          1.8  virginica\n</code></pre> <p>Replicate pandas' melt:</p> <pre><code>&gt;&gt;&gt; df.pivot_longer(index = 'Species')\n     Species      variable  value\n0     setosa  Sepal.Length    5.1\n1  virginica  Sepal.Length    5.9\n2     setosa   Sepal.Width    3.5\n3  virginica   Sepal.Width    3.0\n4     setosa  Petal.Length    1.4\n5  virginica  Petal.Length    5.1\n6     setosa   Petal.Width    0.2\n7  virginica   Petal.Width    1.8\n</code></pre> <p>Convenient, flexible column selection in the <code>index</code> via the <code>select</code> syntax:</p> <pre><code>&gt;&gt;&gt; from pandas.api.types import is_string_dtype\n&gt;&gt;&gt; df.pivot_longer(index = is_string_dtype)\n     Species      variable  value\n0     setosa  Sepal.Length    5.1\n1  virginica  Sepal.Length    5.9\n2     setosa   Sepal.Width    3.5\n3  virginica   Sepal.Width    3.0\n4     setosa  Petal.Length    1.4\n5  virginica  Petal.Length    5.1\n6     setosa   Petal.Width    0.2\n7  virginica   Petal.Width    1.8\n</code></pre> <p>Split the column labels into individual columns:</p> <pre><code>&gt;&gt;&gt; df.pivot_longer(\n...     index = 'Species',\n...     names_to = ('part', 'dimension'),\n...     names_sep = '.',\n...     sort_by_appearance = True,\n... )\n     Species   part dimension  value\n0     setosa  Sepal    Length    5.1\n1     setosa  Sepal     Width    3.5\n2     setosa  Petal    Length    1.4\n3     setosa  Petal     Width    0.2\n4  virginica  Sepal    Length    5.9\n5  virginica  Sepal     Width    3.0\n6  virginica  Petal    Length    5.1\n7  virginica  Petal     Width    1.8\n</code></pre> <p>Retain parts of the column names as headers:</p> <pre><code>&gt;&gt;&gt; df.pivot_longer(\n...     index = 'Species',\n...     names_to = ('part', '.value'),\n...     names_sep = '.',\n...     sort_by_appearance = True,\n... )\n     Species   part  Length  Width\n0     setosa  Sepal     5.1    3.5\n1     setosa  Petal     1.4    0.2\n2  virginica  Sepal     5.9    3.0\n3  virginica  Petal     5.1    1.8\n</code></pre> <p>Split the column labels based on regex:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame({\"id\": [1], \"new_sp_m5564\": [2], \"newrel_f65\": [3]})\n&gt;&gt;&gt; df\n   id  new_sp_m5564  newrel_f65\n0   1             2           3\n&gt;&gt;&gt; df.pivot_longer(\n...     index = 'id',\n...     names_to = ('diagnosis', 'gender', 'age'),\n...     names_pattern = r\"new_?(.+)_(.)(\\d+)\",\n... )\n   id diagnosis gender   age  value\n0   1        sp      m  5564      2\n1   1       rel      f    65      3\n</code></pre> <p>Split the column labels for the above dataframe using named groups in <code>names_pattern</code>:</p> <pre><code>&gt;&gt;&gt; df.pivot_longer(\n...     index = 'id',\n...     names_pattern = r\"new_?(?P&lt;diagnosis&gt;.+)_(?P&lt;gender&gt;.)(?P&lt;age&gt;\\d+)\",\n... )\n    id diagnosis gender   age  value\n0   1        sp      m  5564      2\n1   1       rel      f    65      3\n</code></pre> <p>Convert the dtypes of specific columns with <code>names_transform</code>:</p> <pre><code>&gt;&gt;&gt; result = (df\n...          .pivot_longer(\n...              index = 'id',\n...              names_to = ('diagnosis', 'gender', 'age'),\n...              names_pattern = r\"new_?(.+)_(.)(\\d+)\",\n...              names_transform = {'gender': 'category', 'age':'int'})\n... )\n&gt;&gt;&gt; result.dtypes\nid           int64\ndiagnosis   object\ngender    category\nage          int64\nvalue        int64\ndtype: object\n</code></pre> <p>Use multiple <code>.value</code> to reshape the dataframe:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame(\n...     [\n...         {\n...             \"x_1_mean\": 10,\n...             \"x_2_mean\": 20,\n...             \"y_1_mean\": 30,\n...             \"y_2_mean\": 40,\n...             \"unit\": 50,\n...         }\n...     ]\n... )\n&gt;&gt;&gt; df\n   x_1_mean  x_2_mean  y_1_mean  y_2_mean  unit\n0        10        20        30        40    50\n&gt;&gt;&gt; df.pivot_longer(\n...     index=\"unit\",\n...     names_to=(\".value\", \"time\", \".value\"),\n...     names_pattern=r\"(x|y)_([0-9])(_mean)\",\n... )\n   unit time  x_mean  y_mean\n0    50    1      10      30\n1    50    2      20      40\n</code></pre> <p>Replicate the above with named groups in <code>names_pattern</code> - use <code>_</code> instead of <code>.value</code>:</p> <pre><code>&gt;&gt;&gt; df.pivot_longer(\n...     index=\"unit\",\n...     names_pattern=r\"(?P&lt;_&gt;x|y)_(?P&lt;time&gt;[0-9])(?P&lt;__&gt;_mean)\",\n... )\n   unit time  x_mean  y_mean\n0    50    1      10      30\n1    50    2      20      40\n</code></pre> <p>Convenient, flexible column selection in the <code>column_names</code> via the <code>select</code> syntax:</p> <pre><code>&gt;&gt;&gt; df.pivot_longer(\n...     column_names=\"*mean\",\n...     names_to=(\".value\", \"time\", \".value\"),\n...     names_pattern=r\"(x|y)_([0-9])(_mean)\",\n... )\n   unit time  x_mean  y_mean\n0    50    1      10      30\n1    50    2      20      40\n</code></pre> <pre><code>&gt;&gt;&gt; df.pivot_longer(\n...     column_names=slice(\"x_1_mean\", \"y_2_mean\"),\n...     names_to=(\".value\", \"time\", \".value\"),\n...     names_pattern=r\"(x|y)_([0-9])(_mean)\",\n... )\n   unit time  x_mean  y_mean\n0    50    1      10      30\n1    50    2      20      40\n</code></pre> <p>Reshape the dataframe by passing a sequence to <code>names_pattern</code>:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame({'hr1': [514, 573],\n...                    'hr2': [545, 526],\n...                    'team': ['Red Sox', 'Yankees'],\n...                    'year1': [2007, 2007],\n...                    'year2': [2008, 2008]})\n&gt;&gt;&gt; df\n   hr1  hr2     team  year1  year2\n0  514  545  Red Sox   2007   2008\n1  573  526  Yankees   2007   2008\n&gt;&gt;&gt; df.pivot_longer(\n...     index = 'team',\n...     names_to = ['year', 'hr'],\n...     names_pattern = ['year', 'hr']\n... )\n      team   hr  year\n0  Red Sox  514  2007\n1  Yankees  573  2007\n2  Red Sox  545  2008\n3  Yankees  526  2008\n</code></pre> <p>Reshape the above dataframe by passing a dictionary to <code>names_pattern</code>:</p> <pre><code>&gt;&gt;&gt; df.pivot_longer(\n...     index = 'team',\n...     names_pattern = {\"year\":\"year\", \"hr\":\"hr\"}\n... )\n      team   hr  year\n0  Red Sox  514  2007\n1  Yankees  573  2007\n2  Red Sox  545  2008\n3  Yankees  526  2008\n</code></pre> <p>Multiple values_to:</p> <pre><code>&gt;&gt;&gt; df = pd.DataFrame(\n...         {\n...             \"City\": [\"Houston\", \"Austin\", \"Hoover\"],\n...             \"State\": [\"Texas\", \"Texas\", \"Alabama\"],\n...             \"Name\": [\"Aria\", \"Penelope\", \"Niko\"],\n...             \"Mango\": [4, 10, 90],\n...             \"Orange\": [10, 8, 14],\n...             \"Watermelon\": [40, 99, 43],\n...             \"Gin\": [16, 200, 34],\n...             \"Vodka\": [20, 33, 18],\n...         },\n...     )\n&gt;&gt;&gt; df\n      City    State      Name  Mango  Orange  Watermelon  Gin  Vodka\n0  Houston    Texas      Aria      4      10          40   16     20\n1   Austin    Texas  Penelope     10       8          99  200     33\n2   Hoover  Alabama      Niko     90      14          43   34     18\n&gt;&gt;&gt; df.pivot_longer(\n...         index=[\"City\", \"State\"],\n...         column_names=slice(\"Mango\", \"Vodka\"),\n...         names_to=(\"Fruit\", \"Drink\"),\n...         values_to=(\"Pounds\", \"Ounces\"),\n...         names_pattern=[\"M|O|W\", \"G|V\"],\n...     )\n      City    State       Fruit  Drink  Pounds  Ounces\n0  Houston    Texas       Mango    Gin       4    16.0\n1   Austin    Texas       Mango    Gin      10   200.0\n2   Hoover  Alabama       Mango    Gin      90    34.0\n3  Houston    Texas      Orange  Vodka      10    20.0\n4   Austin    Texas      Orange  Vodka       8    33.0\n5   Hoover  Alabama      Orange  Vodka      14    18.0\n6  Houston    Texas  Watermelon   None      40     NaN\n7   Austin    Texas  Watermelon   None      99     NaN\n8   Hoover  Alabama  Watermelon   None      43     NaN\n</code></pre> <p>Replicate the above transformation with a nested dictionary passed to <code>names_pattern</code> - the outer keys in the <code>names_pattern</code> dictionary are passed to <code>names_to</code>, while the inner keys are passed to <code>values_to</code>:</p> <pre><code>&gt;&gt;&gt; df.pivot_longer(\n...     index=[\"City\", \"State\"],\n...     column_names=slice(\"Mango\", \"Vodka\"),\n...     names_pattern={\n...         \"Fruit\": {\"Pounds\": \"M|O|W\"},\n...         \"Drink\": {\"Ounces\": \"G|V\"},\n...     },\n... )\n      City    State       Fruit  Drink  Pounds  Ounces\n0  Houston    Texas       Mango    Gin       4    16.0\n1   Austin    Texas       Mango    Gin      10   200.0\n2   Hoover  Alabama       Mango    Gin      90    34.0\n3  Houston    Texas      Orange  Vodka      10    20.0\n4   Austin    Texas      Orange  Vodka       8    33.0\n5   Hoover  Alabama      Orange  Vodka      14    18.0\n6  Houston    Texas  Watermelon   None      40     NaN\n7   Austin    Texas  Watermelon   None      99     NaN\n8   Hoover  Alabama  Watermelon   None      43     NaN\n</code></pre> <p>Version Changed</p> <ul> <li>0.24.0<ul> <li>Added <code>dropna</code> parameter.</li> </ul> </li> <li>0.24.1<ul> <li><code>names_pattern</code> can accept a dictionary.</li> <li>named groups supported in <code>names_pattern</code>.</li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>index</code> <code>list | tuple | str | Pattern</code> <p>Name(s) of columns to use as identifier variables. Should be either a single column name, or a list/tuple of column names. <code>index</code> should be a list of tuples if the columns are a MultiIndex.</p> <code>None</code> <code>column_names</code> <code>list | tuple | str | Pattern</code> <p>Name(s) of columns to unpivot. Should be either a single column name or a list/tuple of column names. <code>column_names</code> should be a list of tuples if the columns are a MultiIndex.</p> <code>None</code> <code>names_to</code> <code>list | tuple | str</code> <p>Name of new column as a string that will contain what were previously the column names in <code>column_names</code>. The default is <code>variable</code> if no value is provided. It can also be a list/tuple of strings that will serve as new column names, if <code>name_sep</code> or <code>names_pattern</code> is provided. If <code>.value</code> is in <code>names_to</code>, new column names will be extracted from part of the existing column names and overrides <code>values_to</code>.</p> <code>None</code> <code>values_to</code> <code>str</code> <p>Name of new column as a string that will contain what were previously the values of the columns in <code>column_names</code>. values_to can also be a list/tuple and requires that names_pattern is also a list/tuple.</p> <code>'value'</code> <code>column_level</code> <code>int | str</code> <p>If columns are a MultiIndex, then use this level to unpivot the DataFrame. Provided for compatibility with pandas' melt, and applies only if neither <code>names_sep</code> nor <code>names_pattern</code> is provided.</p> <code>None</code> <code>names_sep</code> <code>str | Pattern</code> <p>Determines how the column name is broken up, if <code>names_to</code> contains multiple values. It takes the same specification as pandas' <code>str.split</code> method, and can be a string or regular expression. <code>names_sep</code> does not work with MultiIndex columns.</p> <code>None</code> <code>names_pattern</code> <code>list | tuple | str | Pattern</code> <p>Determines how the column name is broken up. It can be a regular expression containing matching groups. Under the hood it is processed with pandas' <code>str.extract</code> function. If it is a single regex, the number of groups must match the length of <code>names_to</code>. Named groups are supported, if <code>names_to</code> is none. <code>_</code> is used instead of <code>.value</code> as a placeholder in named groups. <code>_</code> can be overloaded for multiple <code>.value</code> calls - <code>_</code>, <code>__</code>, <code>___</code>, ... <code>names_pattern</code> can also be a list/tuple of regular expressions It can also be a list/tuple of strings; the strings will be treated as regular expressions. Under the hood it is processed with pandas' <code>str.contains</code> function. For a list/tuple of regular expressions, <code>names_to</code> must also be a list/tuple and the lengths of both arguments must match. <code>names_pattern</code> can also be a dictionary, where the keys are the new column names, while the values can be a regular expression or a string which will be evaluated as a regular expression. Alternatively, a nested dictionary can be used, where the sub key(s) are associated with <code>values_to</code>. Please have a look at the examples for usage. <code>names_pattern</code> does not work with MultiIndex columns.</p> <code>None</code> <code>names_transform</code> <code>str | Callable | dict</code> <p>Use this option to change the types of columns that have been transformed to rows. This does not applies to the values' columns. Accepts any argument that is acceptable by <code>pd.astype</code>.</p> <code>None</code> <code>dropna</code> <code>bool</code> <p>Determines whether or not to drop nulls from the values columns. Default is <code>False</code>.</p> <code>False</code> <code>sort_by_appearance</code> <code>bool</code> <p>Boolean value that determines the final look of the DataFrame. If <code>True</code>, the unpivoted DataFrame will be stacked in order of first appearance.</p> <code>False</code> <code>ignore_index</code> <code>bool</code> <p>If <code>True</code>, the original index is ignored. If <code>False</code>, the original index is retained and the index labels will be repeated as necessary.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame that has been unpivoted from wide to long format.</p> Source code in <code>janitor/functions/pivot.py</code> <pre><code>@pf.register_dataframe_method\ndef pivot_longer(\n    df: pd.DataFrame,\n    index: list | tuple | str | Pattern = None,\n    column_names: list | tuple | str | Pattern = None,\n    names_to: list | tuple | str = None,\n    values_to: str = \"value\",\n    column_level: int | str = None,\n    names_sep: str | Pattern = None,\n    names_pattern: list | tuple | str | Pattern = None,\n    names_transform: str | Callable | dict = None,\n    dropna: bool = False,\n    sort_by_appearance: bool = False,\n    ignore_index: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"Unpivots a DataFrame from *wide* to *long* format.\n\n    This method does not mutate the original DataFrame.\n\n    It is modeled after the `pivot_longer` function in R's tidyr package,\n    and also takes inspiration from R's data.table package.\n\n    This function is useful to massage a DataFrame into a format where\n    one or more columns are considered measured variables, and all other\n    columns are considered as identifier variables.\n\n    All measured variables are *unpivoted* (and typically duplicated) along the\n    row axis.\n\n    Column selection in `index` and `column_names` is possible using the\n    [`select`][janitor.functions.select.select] syntax.\n\n    For more granular control on the unpivoting, have a look at\n    [`pivot_longer_spec`][janitor.functions.pivot.pivot_longer_spec].\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame(\n        ...     {\n        ...         \"Sepal.Length\": [5.1, 5.9],\n        ...         \"Sepal.Width\": [3.5, 3.0],\n        ...         \"Petal.Length\": [1.4, 5.1],\n        ...         \"Petal.Width\": [0.2, 1.8],\n        ...         \"Species\": [\"setosa\", \"virginica\"],\n        ...     }\n        ... )\n        &gt;&gt;&gt; df\n           Sepal.Length  Sepal.Width  Petal.Length  Petal.Width    Species\n        0           5.1          3.5           1.4          0.2     setosa\n        1           5.9          3.0           5.1          1.8  virginica\n\n        Replicate pandas' melt:\n        &gt;&gt;&gt; df.pivot_longer(index = 'Species')\n             Species      variable  value\n        0     setosa  Sepal.Length    5.1\n        1  virginica  Sepal.Length    5.9\n        2     setosa   Sepal.Width    3.5\n        3  virginica   Sepal.Width    3.0\n        4     setosa  Petal.Length    1.4\n        5  virginica  Petal.Length    5.1\n        6     setosa   Petal.Width    0.2\n        7  virginica   Petal.Width    1.8\n\n        Convenient, flexible column selection in the `index` via the\n        [`select`][janitor.functions.select.select] syntax:\n        &gt;&gt;&gt; from pandas.api.types import is_string_dtype\n        &gt;&gt;&gt; df.pivot_longer(index = is_string_dtype)\n             Species      variable  value\n        0     setosa  Sepal.Length    5.1\n        1  virginica  Sepal.Length    5.9\n        2     setosa   Sepal.Width    3.5\n        3  virginica   Sepal.Width    3.0\n        4     setosa  Petal.Length    1.4\n        5  virginica  Petal.Length    5.1\n        6     setosa   Petal.Width    0.2\n        7  virginica   Petal.Width    1.8\n\n        Split the column labels into individual columns:\n        &gt;&gt;&gt; df.pivot_longer(\n        ...     index = 'Species',\n        ...     names_to = ('part', 'dimension'),\n        ...     names_sep = '.',\n        ...     sort_by_appearance = True,\n        ... )\n             Species   part dimension  value\n        0     setosa  Sepal    Length    5.1\n        1     setosa  Sepal     Width    3.5\n        2     setosa  Petal    Length    1.4\n        3     setosa  Petal     Width    0.2\n        4  virginica  Sepal    Length    5.9\n        5  virginica  Sepal     Width    3.0\n        6  virginica  Petal    Length    5.1\n        7  virginica  Petal     Width    1.8\n\n        Retain parts of the column names as headers:\n        &gt;&gt;&gt; df.pivot_longer(\n        ...     index = 'Species',\n        ...     names_to = ('part', '.value'),\n        ...     names_sep = '.',\n        ...     sort_by_appearance = True,\n        ... )\n             Species   part  Length  Width\n        0     setosa  Sepal     5.1    3.5\n        1     setosa  Petal     1.4    0.2\n        2  virginica  Sepal     5.9    3.0\n        3  virginica  Petal     5.1    1.8\n\n        Split the column labels based on regex:\n        &gt;&gt;&gt; df = pd.DataFrame({\"id\": [1], \"new_sp_m5564\": [2], \"newrel_f65\": [3]})\n        &gt;&gt;&gt; df\n           id  new_sp_m5564  newrel_f65\n        0   1             2           3\n        &gt;&gt;&gt; df.pivot_longer(\n        ...     index = 'id',\n        ...     names_to = ('diagnosis', 'gender', 'age'),\n        ...     names_pattern = r\"new_?(.+)_(.)(\\\\d+)\",\n        ... )\n           id diagnosis gender   age  value\n        0   1        sp      m  5564      2\n        1   1       rel      f    65      3\n\n        Split the column labels for the above dataframe using named groups in `names_pattern`:\n        &gt;&gt;&gt; df.pivot_longer(\n        ...     index = 'id',\n        ...     names_pattern = r\"new_?(?P&lt;diagnosis&gt;.+)_(?P&lt;gender&gt;.)(?P&lt;age&gt;\\\\d+)\",\n        ... )\n            id diagnosis gender   age  value\n        0   1        sp      m  5564      2\n        1   1       rel      f    65      3\n\n        Convert the dtypes of specific columns with `names_transform`:\n        &gt;&gt;&gt; result = (df\n        ...          .pivot_longer(\n        ...              index = 'id',\n        ...              names_to = ('diagnosis', 'gender', 'age'),\n        ...              names_pattern = r\"new_?(.+)_(.)(\\\\d+)\",\n        ...              names_transform = {'gender': 'category', 'age':'int'})\n        ... )\n        &gt;&gt;&gt; result.dtypes\n        id           int64\n        diagnosis   object\n        gender    category\n        age          int64\n        value        int64\n        dtype: object\n\n        Use multiple `.value` to reshape the dataframe:\n        &gt;&gt;&gt; df = pd.DataFrame(\n        ...     [\n        ...         {\n        ...             \"x_1_mean\": 10,\n        ...             \"x_2_mean\": 20,\n        ...             \"y_1_mean\": 30,\n        ...             \"y_2_mean\": 40,\n        ...             \"unit\": 50,\n        ...         }\n        ...     ]\n        ... )\n        &gt;&gt;&gt; df\n           x_1_mean  x_2_mean  y_1_mean  y_2_mean  unit\n        0        10        20        30        40    50\n        &gt;&gt;&gt; df.pivot_longer(\n        ...     index=\"unit\",\n        ...     names_to=(\".value\", \"time\", \".value\"),\n        ...     names_pattern=r\"(x|y)_([0-9])(_mean)\",\n        ... )\n           unit time  x_mean  y_mean\n        0    50    1      10      30\n        1    50    2      20      40\n\n        Replicate the above with named groups in `names_pattern` - use `_` instead of `.value`:\n        &gt;&gt;&gt; df.pivot_longer(\n        ...     index=\"unit\",\n        ...     names_pattern=r\"(?P&lt;_&gt;x|y)_(?P&lt;time&gt;[0-9])(?P&lt;__&gt;_mean)\",\n        ... )\n           unit time  x_mean  y_mean\n        0    50    1      10      30\n        1    50    2      20      40\n\n        Convenient, flexible column selection in the `column_names` via\n        the [`select`][janitor.functions.select.select] syntax:\n        &gt;&gt;&gt; df.pivot_longer(\n        ...     column_names=\"*mean\",\n        ...     names_to=(\".value\", \"time\", \".value\"),\n        ...     names_pattern=r\"(x|y)_([0-9])(_mean)\",\n        ... )\n           unit time  x_mean  y_mean\n        0    50    1      10      30\n        1    50    2      20      40\n\n        &gt;&gt;&gt; df.pivot_longer(\n        ...     column_names=slice(\"x_1_mean\", \"y_2_mean\"),\n        ...     names_to=(\".value\", \"time\", \".value\"),\n        ...     names_pattern=r\"(x|y)_([0-9])(_mean)\",\n        ... )\n           unit time  x_mean  y_mean\n        0    50    1      10      30\n        1    50    2      20      40\n\n        Reshape the dataframe by passing a sequence to `names_pattern`:\n        &gt;&gt;&gt; df = pd.DataFrame({'hr1': [514, 573],\n        ...                    'hr2': [545, 526],\n        ...                    'team': ['Red Sox', 'Yankees'],\n        ...                    'year1': [2007, 2007],\n        ...                    'year2': [2008, 2008]})\n        &gt;&gt;&gt; df\n           hr1  hr2     team  year1  year2\n        0  514  545  Red Sox   2007   2008\n        1  573  526  Yankees   2007   2008\n        &gt;&gt;&gt; df.pivot_longer(\n        ...     index = 'team',\n        ...     names_to = ['year', 'hr'],\n        ...     names_pattern = ['year', 'hr']\n        ... )\n              team   hr  year\n        0  Red Sox  514  2007\n        1  Yankees  573  2007\n        2  Red Sox  545  2008\n        3  Yankees  526  2008\n\n\n        Reshape the above dataframe by passing a dictionary to `names_pattern`:\n        &gt;&gt;&gt; df.pivot_longer(\n        ...     index = 'team',\n        ...     names_pattern = {\"year\":\"year\", \"hr\":\"hr\"}\n        ... )\n              team   hr  year\n        0  Red Sox  514  2007\n        1  Yankees  573  2007\n        2  Red Sox  545  2008\n        3  Yankees  526  2008\n\n        Multiple values_to:\n        &gt;&gt;&gt; df = pd.DataFrame(\n        ...         {\n        ...             \"City\": [\"Houston\", \"Austin\", \"Hoover\"],\n        ...             \"State\": [\"Texas\", \"Texas\", \"Alabama\"],\n        ...             \"Name\": [\"Aria\", \"Penelope\", \"Niko\"],\n        ...             \"Mango\": [4, 10, 90],\n        ...             \"Orange\": [10, 8, 14],\n        ...             \"Watermelon\": [40, 99, 43],\n        ...             \"Gin\": [16, 200, 34],\n        ...             \"Vodka\": [20, 33, 18],\n        ...         },\n        ...     )\n        &gt;&gt;&gt; df\n              City    State      Name  Mango  Orange  Watermelon  Gin  Vodka\n        0  Houston    Texas      Aria      4      10          40   16     20\n        1   Austin    Texas  Penelope     10       8          99  200     33\n        2   Hoover  Alabama      Niko     90      14          43   34     18\n        &gt;&gt;&gt; df.pivot_longer(\n        ...         index=[\"City\", \"State\"],\n        ...         column_names=slice(\"Mango\", \"Vodka\"),\n        ...         names_to=(\"Fruit\", \"Drink\"),\n        ...         values_to=(\"Pounds\", \"Ounces\"),\n        ...         names_pattern=[\"M|O|W\", \"G|V\"],\n        ...     )\n              City    State       Fruit  Drink  Pounds  Ounces\n        0  Houston    Texas       Mango    Gin       4    16.0\n        1   Austin    Texas       Mango    Gin      10   200.0\n        2   Hoover  Alabama       Mango    Gin      90    34.0\n        3  Houston    Texas      Orange  Vodka      10    20.0\n        4   Austin    Texas      Orange  Vodka       8    33.0\n        5   Hoover  Alabama      Orange  Vodka      14    18.0\n        6  Houston    Texas  Watermelon   None      40     NaN\n        7   Austin    Texas  Watermelon   None      99     NaN\n        8   Hoover  Alabama  Watermelon   None      43     NaN\n\n        Replicate the above transformation with a nested dictionary passed to `names_pattern`\n        - the outer keys in the `names_pattern` dictionary are passed to `names_to`,\n        while the inner keys are passed to `values_to`:\n        &gt;&gt;&gt; df.pivot_longer(\n        ...     index=[\"City\", \"State\"],\n        ...     column_names=slice(\"Mango\", \"Vodka\"),\n        ...     names_pattern={\n        ...         \"Fruit\": {\"Pounds\": \"M|O|W\"},\n        ...         \"Drink\": {\"Ounces\": \"G|V\"},\n        ...     },\n        ... )\n              City    State       Fruit  Drink  Pounds  Ounces\n        0  Houston    Texas       Mango    Gin       4    16.0\n        1   Austin    Texas       Mango    Gin      10   200.0\n        2   Hoover  Alabama       Mango    Gin      90    34.0\n        3  Houston    Texas      Orange  Vodka      10    20.0\n        4   Austin    Texas      Orange  Vodka       8    33.0\n        5   Hoover  Alabama      Orange  Vodka      14    18.0\n        6  Houston    Texas  Watermelon   None      40     NaN\n        7   Austin    Texas  Watermelon   None      99     NaN\n        8   Hoover  Alabama  Watermelon   None      43     NaN\n\n    !!! abstract \"Version Changed\"\n\n        - 0.24.0\n            - Added `dropna` parameter.\n        - 0.24.1\n            - `names_pattern` can accept a dictionary.\n            - named groups supported in `names_pattern`.\n\n    Args:\n        df: A pandas DataFrame.\n        index: Name(s) of columns to use as identifier variables.\n            Should be either a single column name, or a list/tuple of\n            column names.\n            `index` should be a list of tuples if the columns are a MultiIndex.\n        column_names: Name(s) of columns to unpivot. Should be either\n            a single column name or a list/tuple of column names.\n            `column_names` should be a list of tuples\n            if the columns are a MultiIndex.\n        names_to: Name of new column as a string that will contain\n            what were previously the column names in `column_names`.\n            The default is `variable` if no value is provided. It can\n            also be a list/tuple of strings that will serve as new column\n            names, if `name_sep` or `names_pattern` is provided.\n            If `.value` is in `names_to`, new column names will be extracted\n            from part of the existing column names and overrides `values_to`.\n        values_to: Name of new column as a string that will contain what\n            were previously the values of the columns in `column_names`.\n            values_to can also be a list/tuple\n            and requires that names_pattern is also a list/tuple.\n        column_level: If columns are a MultiIndex, then use this level to\n            unpivot the DataFrame. Provided for compatibility with pandas' melt,\n            and applies only if neither `names_sep` nor `names_pattern` is\n            provided.\n        names_sep: Determines how the column name is broken up, if\n            `names_to` contains multiple values. It takes the same\n            specification as pandas' `str.split` method, and can be a string\n            or regular expression. `names_sep` does not work with MultiIndex\n            columns.\n        names_pattern: Determines how the column name is broken up.\n            It can be a regular expression containing matching groups.\n            Under the hood it is processed with pandas' `str.extract` function.\n            If it is a single regex, the number of groups must match\n            the length of `names_to`.\n            Named groups are supported, if `names_to` is none. `_` is used\n            instead of `.value` as a placeholder in named groups.\n            `_` can be overloaded for multiple `.value`\n            calls - `_`, `__`, `___`, ...\n            `names_pattern` can also be a list/tuple of regular expressions\n            It can also be a list/tuple of strings;\n            the strings will be treated as regular expressions.\n            Under the hood it is processed with pandas' `str.contains` function.\n            For a list/tuple of regular expressions,\n            `names_to` must also be a list/tuple and the lengths of both\n            arguments must match.\n            `names_pattern` can also be a dictionary, where the keys are\n            the new column names, while the values can be a regular expression\n            or a string which will be evaluated as a regular expression.\n            Alternatively, a nested dictionary can be used, where the sub\n            key(s) are associated with `values_to`. Please have a look\n            at the examples for usage.\n            `names_pattern` does not work with MultiIndex columns.\n        names_transform: Use this option to change the types of columns that\n            have been transformed to rows. This does not applies to the values' columns.\n            Accepts any argument that is acceptable by `pd.astype`.\n        dropna: Determines whether or not to drop nulls\n            from the values columns. Default is `False`.\n        sort_by_appearance: Boolean value that determines\n            the final look of the DataFrame. If `True`, the unpivoted DataFrame\n            will be stacked in order of first appearance.\n        ignore_index: If `True`,\n            the original index is ignored. If `False`, the original index\n            is retained and the index labels will be repeated as necessary.\n\n    Returns:\n        A pandas DataFrame that has been unpivoted from wide to long\n            format.\n    \"\"\"  # noqa: E501\n\n    # this code builds on the wonderful work of @benjaminjack\u2019s PR\n    # https://github.com/benjaminjack/pyjanitor/commit/e3df817903c20dd21634461c8a92aec137963ed0\n\n    return _computations_pivot_longer(\n        df=df,\n        index=index,\n        column_names=column_names,\n        column_level=column_level,\n        names_to=names_to,\n        values_to=values_to,\n        names_sep=names_sep,\n        names_pattern=names_pattern,\n        names_transform=names_transform,\n        dropna=dropna,\n        sort_by_appearance=sort_by_appearance,\n        ignore_index=ignore_index,\n    )\n</code></pre>"},{"location":"api/functions/#janitor.functions.pivot.pivot_longer_spec","title":"<code>pivot_longer_spec(df, spec, sort_by_appearance=False, ignore_index=True, dropna=False, df_columns_is_unique=True)</code>","text":"<p>A declarative interface to pivot a DataFrame from wide to long form, where you describe how the data will be unpivoted, using a DataFrame. This gives you, the user, more control over unpivoting, where you create a \u201cspec\u201d data frame that describes exactly how data stored in the column names becomes variables. It can come in handy for situations where <code>pivot_longer</code> seems inadequate for the transformation.</p> <p>New in version 0.28.0</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame(\n...     {\n...         \"Sepal.Length\": [5.1, 5.9],\n...         \"Sepal.Width\": [3.5, 3.0],\n...         \"Petal.Length\": [1.4, 5.1],\n...         \"Petal.Width\": [0.2, 1.8],\n...         \"Species\": [\"setosa\", \"virginica\"],\n...     }\n... )\n&gt;&gt;&gt; df\n   Sepal.Length  Sepal.Width  Petal.Length  Petal.Width    Species\n0           5.1          3.5           1.4          0.2     setosa\n1           5.9          3.0           5.1          1.8  virginica\n&gt;&gt;&gt; spec = {'.name':['Sepal.Length','Petal.Length',\n...                  'Sepal.Width','Petal.Width'],\n...         '.value':['Length','Length','Width','Width'],\n...         'part':['Sepal','Petal','Sepal','Petal']}\n&gt;&gt;&gt; spec = pd.DataFrame(spec)\n&gt;&gt;&gt; spec\n          .name  .value   part\n0  Sepal.Length  Length  Sepal\n1  Petal.Length  Length  Petal\n2   Sepal.Width   Width  Sepal\n3   Petal.Width   Width  Petal\n&gt;&gt;&gt; pivot_longer_spec(df=df,spec=spec)\n     Species   part  Length  Width\n0     setosa  Sepal     5.1    3.5\n1  virginica  Sepal     5.9    3.0\n2     setosa  Petal     1.4    0.2\n3  virginica  Petal     5.1    1.8\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The source DataFrame to unpivot.</p> required <code>spec</code> <code>DataFrame</code> <p>A specification DataFrame. At a minimum, the spec DataFrame must have a '.name' and a '.value' columns. The '.name' column  should contain the columns in the source DataFrame that will be transformed to long form. The '.value' column gives the name of the column(s) that the values in the source DataFrame will go into. Additional columns in spec should be named to match columns in the long format of the dataset and contain values corresponding to columns pivoted from the wide format. Note that these additional columns should not already exist in the source DataFrame.</p> required <code>sort_by_appearance</code> <code>bool</code> <p>Boolean value that determines the final look of the DataFrame. If <code>True</code>, the unpivoted DataFrame will be stacked in order of first appearance.</p> <code>False</code> <code>ignore_index</code> <code>bool</code> <p>If <code>True</code>, the original index is ignored. If <code>False</code>, the original index is retained and the index labels will be repeated as necessary.</p> <code>True</code> <code>dropna</code> <code>bool</code> <p>Determines whether or not to drop nulls from the values columns. Default is <code>False</code>.</p> <code>False</code> <code>df_columns_is_unique</code> <code>bool</code> <p>Boolean value to indicate if the source DataFrame's columns is unique. Default is <code>True</code>.</p> <code>True</code> <p>Raises:</p> Type Description <code>KeyError</code> <p>If '.name' or '.value' is missing from the spec's columns.</p> <code>ValueError</code> <p>If the spec's columns is not unique, or the labels in spec['.name'] is not unique.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame.</p> Source code in <code>janitor/functions/pivot.py</code> <pre><code>def pivot_longer_spec(\n    df: pd.DataFrame,\n    spec: pd.DataFrame,\n    sort_by_appearance: bool = False,\n    ignore_index: bool = True,\n    dropna: bool = False,\n    df_columns_is_unique: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"A declarative interface to pivot a DataFrame from wide to long form,\n    where you describe how the data will be unpivoted,\n    using a DataFrame. This gives you, the user,\n    more control over unpivoting, where you create a \u201cspec\u201d\n    data frame that describes exactly how data stored\n    in the column names becomes variables.\n    It can come in handy for situations where\n    [`pivot_longer`][janitor.functions.pivot.pivot_longer]\n    seems inadequate for the transformation.\n\n    !!! info \"New in version 0.28.0\"\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame(\n        ...     {\n        ...         \"Sepal.Length\": [5.1, 5.9],\n        ...         \"Sepal.Width\": [3.5, 3.0],\n        ...         \"Petal.Length\": [1.4, 5.1],\n        ...         \"Petal.Width\": [0.2, 1.8],\n        ...         \"Species\": [\"setosa\", \"virginica\"],\n        ...     }\n        ... )\n        &gt;&gt;&gt; df\n           Sepal.Length  Sepal.Width  Petal.Length  Petal.Width    Species\n        0           5.1          3.5           1.4          0.2     setosa\n        1           5.9          3.0           5.1          1.8  virginica\n        &gt;&gt;&gt; spec = {'.name':['Sepal.Length','Petal.Length',\n        ...                  'Sepal.Width','Petal.Width'],\n        ...         '.value':['Length','Length','Width','Width'],\n        ...         'part':['Sepal','Petal','Sepal','Petal']}\n        &gt;&gt;&gt; spec = pd.DataFrame(spec)\n        &gt;&gt;&gt; spec\n                  .name  .value   part\n        0  Sepal.Length  Length  Sepal\n        1  Petal.Length  Length  Petal\n        2   Sepal.Width   Width  Sepal\n        3   Petal.Width   Width  Petal\n        &gt;&gt;&gt; pivot_longer_spec(df=df,spec=spec)\n             Species   part  Length  Width\n        0     setosa  Sepal     5.1    3.5\n        1  virginica  Sepal     5.9    3.0\n        2     setosa  Petal     1.4    0.2\n        3  virginica  Petal     5.1    1.8\n\n    Args:\n        df: The source DataFrame to unpivot.\n        spec: A specification DataFrame.\n            At a minimum, the spec DataFrame\n            must have a '.name' and a '.value' columns.\n            The '.name' column  should contain the\n            columns in the source DataFrame that will be\n            transformed to long form.\n            The '.value' column gives the name of the column(s)\n            that the values in the source DataFrame will go into.\n            Additional columns in spec should be named to match columns\n            in the long format of the dataset and contain values\n            corresponding to columns pivoted from the wide format.\n            Note that these additional columns should not already exist\n            in the source DataFrame.\n        sort_by_appearance: Boolean value that determines\n            the final look of the DataFrame. If `True`, the unpivoted DataFrame\n            will be stacked in order of first appearance.\n        ignore_index: If `True`,\n            the original index is ignored. If `False`, the original index\n            is retained and the index labels will be repeated as necessary.\n        dropna: Determines whether or not to drop nulls\n            from the values columns. Default is `False`.\n        df_columns_is_unique: Boolean value to indicate if the source\n            DataFrame's columns is unique. Default is `True`.\n\n    Raises:\n        KeyError: If '.name' or '.value' is missing from the spec's columns.\n        ValueError: If the spec's columns is not unique,\n            or the labels in spec['.name'] is not unique.\n\n    Returns:\n        A pandas DataFrame.\n    \"\"\"\n    check(\"spec\", spec, [pd.DataFrame])\n    if not spec.columns.is_unique:\n        raise ValueError(\"Kindly ensure the spec's columns is unique.\")\n    if \".name\" not in spec.columns:\n        raise KeyError(\n            \"Kindly ensure the spec DataFrame has a `.name` column.\"\n        )\n    if \".value\" not in spec.columns:\n        raise KeyError(\n            \"Kindly ensure the spec DataFrame has a `.value` column.\"\n        )\n    if spec.columns.tolist()[:2] != [\".name\", \".value\"]:\n        raise ValueError(\n            \"The first two columns of the spec DataFrame \"\n            \"should be '.name' and '.value', \"\n            \"with '.name' coming before '.value'.\"\n        )\n    if not spec[\".name\"].is_unique:\n        raise ValueError(\"The labels in the `.name` column should be unique.\")\n\n    exclude = df.columns.intersection(spec.columns)\n    if not exclude.empty:\n        raise ValueError(\n            f\"Labels {*exclude, } in the spec DataFrame already exist \"\n            \"as column labels in the source DataFrame. \"\n            \"Kindly ensure the spec DataFrame's columns \"\n            \"are not present in the source DataFrame.\"\n        )\n\n    check(\"dropna\", dropna, [bool])\n    check(\"sort_by_appearance\", sort_by_appearance, [bool])\n    check(\"ignore_index\", ignore_index, [bool])\n    check(\"df_columns_is_unique\", df_columns_is_unique, [bool])\n\n    index = df.columns.difference(spec[\".name\"], sort=False)\n    index = {name: df[name]._values for name in index}\n\n    df = df.loc[:, spec[\".name\"]]\n    if not df_columns_is_unique:\n        spec = pd.DataFrame({\".name\": df.columns}).merge(\n            spec, on=\".name\", how=\"inner\"\n        )\n    others = [label for label in spec if label not in {\".name\", \".value\"}]\n    return _pivot_longer_dot_value(\n        df=df,\n        spec=spec.drop(columns=\".name\"),\n        index=index,\n        others=others,\n        sort_by_appearance=sort_by_appearance,\n        ignore_index=ignore_index,\n        dropna=dropna,\n    )\n</code></pre>"},{"location":"api/functions/#janitor.functions.pivot.pivot_wider","title":"<code>pivot_wider(df, index=None, names_from=None, values_from=None, flatten_levels=True, names_sep='_', names_glue=None, reset_index=True, names_expand=False, index_expand=False)</code>","text":"<p>Reshapes data from long to wide form.</p> <p>Note</p> <p>This function will be deprecated in a 1.x release. Please use <code>pd.DataFrame.pivot</code> instead.</p> <p>The number of columns are increased, while decreasing the number of rows. It is the inverse of the <code>pivot_longer</code> method, and is a wrapper around <code>pd.DataFrame.pivot</code> method.</p> <p>This method does not mutate the original DataFrame.</p> <p>Column selection in <code>index</code>, <code>names_from</code> and <code>values_from</code> is possible using the <code>select</code> syntax.</p> <p>A ValueError is raised if the combination of the <code>index</code> and <code>names_from</code> is not unique.</p> <p>By default, values from <code>values_from</code> are always at the top level if the columns are not flattened. If flattened, the values from <code>values_from</code> are usually at the start of each label in the columns.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = [{'dep': 5.5, 'step': 1, 'a': 20, 'b': 30},\n...       {'dep': 5.5, 'step': 2, 'a': 25, 'b': 37},\n...       {'dep': 6.1, 'step': 1, 'a': 22, 'b': 19},\n...       {'dep': 6.1, 'step': 2, 'a': 18, 'b': 29}]\n&gt;&gt;&gt; df = pd.DataFrame(df)\n&gt;&gt;&gt; df\n   dep  step   a   b\n0  5.5     1  20  30\n1  5.5     2  25  37\n2  6.1     1  22  19\n3  6.1     2  18  29\n</code></pre> <p>Pivot and flatten columns:</p> <pre><code>&gt;&gt;&gt; df.pivot_wider(\n...     index = \"dep\",\n...     names_from = \"step\",\n... )\n   dep  a_1  a_2  b_1  b_2\n0  5.5   20   25   30   37\n1  6.1   22   18   19   29\n</code></pre> <p>Modify columns with <code>names_sep</code>:</p> <pre><code>&gt;&gt;&gt; df.pivot_wider(\n...     index = \"dep\",\n...     names_from = \"step\",\n...     names_sep = \"\",\n... )\n   dep  a1  a2  b1  b2\n0  5.5  20  25  30  37\n1  6.1  22  18  19  29\n</code></pre> <p>Modify columns with <code>names_glue</code>:</p> <pre><code>&gt;&gt;&gt; df.pivot_wider(\n...     index = \"dep\",\n...     names_from = \"step\",\n...     names_glue = \"{_value}_step{step}\",\n... )\n   dep  a_step1  a_step2  b_step1  b_step2\n0  5.5       20       25       30       37\n1  6.1       22       18       19       29\n</code></pre> <p>Expand columns to expose implicit missing values - this applies only to categorical columns:</p> <pre><code>&gt;&gt;&gt; weekdays = (\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")\n&gt;&gt;&gt; daily = pd.DataFrame(\n...     {\n...         \"day\": pd.Categorical(\n...             values=(\"Tue\", \"Thu\", \"Fri\", \"Mon\"), categories=weekdays\n...         ),\n...         \"value\": (2, 3, 1, 5),\n...     },\n... index=[0, 0, 0, 0],\n... )\n&gt;&gt;&gt; daily\n   day  value\n0  Tue      2\n0  Thu      3\n0  Fri      1\n0  Mon      5\n&gt;&gt;&gt; daily.pivot_wider(names_from='day', values_from='value')\n   Tue  Thu  Fri  Mon\n0    2    3    1    5\n&gt;&gt;&gt; (daily\n... .pivot_wider(\n...     names_from='day',\n...     values_from='value',\n...     names_expand=True)\n... )\n   Mon  Tue  Wed  Thu  Fri  Sat  Sun\n0    5    2  NaN    3    1  NaN  NaN\n</code></pre> <p>Expand the index to expose implicit missing values - this applies only to categorical columns:</p> <pre><code>&gt;&gt;&gt; daily = daily.assign(letter = list('ABBA'))\n&gt;&gt;&gt; daily\n   day  value letter\n0  Tue      2      A\n0  Thu      3      B\n0  Fri      1      B\n0  Mon      5      A\n&gt;&gt;&gt; daily.pivot_wider(index='day',names_from='letter',values_from='value')\n   day    A    B\n0  Tue  2.0  NaN\n1  Thu  NaN  3.0\n2  Fri  NaN  1.0\n3  Mon  5.0  NaN\n&gt;&gt;&gt; (daily\n... .pivot_wider(\n...     index='day',\n...     names_from='letter',\n...     values_from='value',\n...     index_expand=True)\n... )\n   day    A    B\n0  Mon  5.0  NaN\n1  Tue  2.0  NaN\n2  Wed  NaN  NaN\n3  Thu  NaN  3.0\n4  Fri  NaN  1.0\n5  Sat  NaN  NaN\n6  Sun  NaN  NaN\n</code></pre> <p>Version Changed</p> <ul> <li>0.24.0<ul> <li>Added <code>reset_index</code>, <code>names_expand</code> and <code>index_expand</code> parameters.</li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>index</code> <code>list | str</code> <p>Name(s) of columns to use as identifier variables. It should be either a single column name, or a list of column names. If <code>index</code> is not provided, the DataFrame's index is used.</p> <code>None</code> <code>names_from</code> <code>list | str</code> <p>Name(s) of column(s) to use to make the new DataFrame's columns. Should be either a single column name, or a list of column names.</p> <code>None</code> <code>values_from</code> <code>list | str</code> <p>Name(s) of column(s) that will be used for populating the new DataFrame's values. If <code>values_from</code> is not specified,  all remaining columns will be used.</p> <code>None</code> <code>flatten_levels</code> <code>bool</code> <p>If <code>False</code>, the DataFrame stays as a MultiIndex.</p> <code>True</code> <code>names_sep</code> <code>str</code> <p>If <code>names_from</code> or <code>values_from</code> contain multiple variables, this will be used to join the values into a single string to use as a column name. Default is <code>_</code>. Applicable only if <code>flatten_levels</code> is <code>True</code>.</p> <code>'_'</code> <code>names_glue</code> <code>str</code> <p>A string to control the output of the flattened columns. It offers more flexibility in creating custom column names, and uses python's <code>str.format_map</code> under the hood. Simply create the string template, using the column labels in <code>names_from</code>, and special <code>_value</code> as a placeholder for <code>values_from</code>. Applicable only if <code>flatten_levels</code> is <code>True</code>.</p> <code>None</code> <code>reset_index</code> <code>bool</code> <p>Determines whether to restore <code>index</code> as a column/columns. Applicable only if <code>index</code> is provided, and <code>flatten_levels</code> is <code>True</code>.</p> <code>True</code> <code>names_expand</code> <code>bool</code> <p>Expand columns to show all the categories. Applies only if <code>names_from</code> is a categorical column.</p> <code>False</code> <code>index_expand</code> <code>bool</code> <p>Expand the index to show all the categories. Applies only if <code>index</code> is a categorical column.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame that has been unpivoted from long to wide form.</p> Source code in <code>janitor/functions/pivot.py</code> <pre><code>@pf.register_dataframe_method\n@refactored_function(\n    message=(\n        \"This function will be deprecated in a 1.x release. \"\n        \"Please use `pd.DataFrame.pivot` instead.\"\n    )\n)\ndef pivot_wider(\n    df: pd.DataFrame,\n    index: list | str = None,\n    names_from: list | str = None,\n    values_from: list | str = None,\n    flatten_levels: bool = True,\n    names_sep: str = \"_\",\n    names_glue: str = None,\n    reset_index: bool = True,\n    names_expand: bool = False,\n    index_expand: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Reshapes data from *long* to *wide* form.\n\n    !!!note\n\n        This function will be deprecated in a 1.x release.\n        Please use `pd.DataFrame.pivot` instead.\n\n    The number of columns are increased, while decreasing\n    the number of rows. It is the inverse of the\n    [`pivot_longer`][janitor.functions.pivot.pivot_longer]\n    method, and is a wrapper around `pd.DataFrame.pivot` method.\n\n    This method does not mutate the original DataFrame.\n\n    Column selection in `index`, `names_from` and `values_from`\n    is possible using the\n    [`select`][janitor.functions.select.select] syntax.\n\n    A ValueError is raised if the combination\n    of the `index` and `names_from` is not unique.\n\n    By default, values from `values_from` are always\n    at the top level if the columns are not flattened.\n    If flattened, the values from `values_from` are usually\n    at the start of each label in the columns.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = [{'dep': 5.5, 'step': 1, 'a': 20, 'b': 30},\n        ...       {'dep': 5.5, 'step': 2, 'a': 25, 'b': 37},\n        ...       {'dep': 6.1, 'step': 1, 'a': 22, 'b': 19},\n        ...       {'dep': 6.1, 'step': 2, 'a': 18, 'b': 29}]\n        &gt;&gt;&gt; df = pd.DataFrame(df)\n        &gt;&gt;&gt; df\n           dep  step   a   b\n        0  5.5     1  20  30\n        1  5.5     2  25  37\n        2  6.1     1  22  19\n        3  6.1     2  18  29\n\n        Pivot and flatten columns:\n        &gt;&gt;&gt; df.pivot_wider( # doctest: +SKIP\n        ...     index = \"dep\",\n        ...     names_from = \"step\",\n        ... )\n           dep  a_1  a_2  b_1  b_2\n        0  5.5   20   25   30   37\n        1  6.1   22   18   19   29\n\n        Modify columns with `names_sep`:\n        &gt;&gt;&gt; df.pivot_wider( # doctest: +SKIP\n        ...     index = \"dep\",\n        ...     names_from = \"step\",\n        ...     names_sep = \"\",\n        ... )\n           dep  a1  a2  b1  b2\n        0  5.5  20  25  30  37\n        1  6.1  22  18  19  29\n\n        Modify columns with `names_glue`:\n        &gt;&gt;&gt; df.pivot_wider( # doctest: +SKIP\n        ...     index = \"dep\",\n        ...     names_from = \"step\",\n        ...     names_glue = \"{_value}_step{step}\",\n        ... )\n           dep  a_step1  a_step2  b_step1  b_step2\n        0  5.5       20       25       30       37\n        1  6.1       22       18       19       29\n\n        Expand columns to expose implicit missing values\n        - this applies only to categorical columns:\n        &gt;&gt;&gt; weekdays = (\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")\n        &gt;&gt;&gt; daily = pd.DataFrame(\n        ...     {\n        ...         \"day\": pd.Categorical(\n        ...             values=(\"Tue\", \"Thu\", \"Fri\", \"Mon\"), categories=weekdays\n        ...         ),\n        ...         \"value\": (2, 3, 1, 5),\n        ...     },\n        ... index=[0, 0, 0, 0],\n        ... )\n        &gt;&gt;&gt; daily\n           day  value\n        0  Tue      2\n        0  Thu      3\n        0  Fri      1\n        0  Mon      5\n        &gt;&gt;&gt; daily.pivot_wider(names_from='day', values_from='value') # doctest: +SKIP\n           Tue  Thu  Fri  Mon\n        0    2    3    1    5\n        &gt;&gt;&gt; (daily # doctest: +SKIP\n        ... .pivot_wider(\n        ...     names_from='day',\n        ...     values_from='value',\n        ...     names_expand=True)\n        ... )\n           Mon  Tue  Wed  Thu  Fri  Sat  Sun\n        0    5    2  NaN    3    1  NaN  NaN\n\n        Expand the index to expose implicit missing values\n        - this applies only to categorical columns:\n        &gt;&gt;&gt; daily = daily.assign(letter = list('ABBA'))\n        &gt;&gt;&gt; daily\n           day  value letter\n        0  Tue      2      A\n        0  Thu      3      B\n        0  Fri      1      B\n        0  Mon      5      A\n        &gt;&gt;&gt; daily.pivot_wider(index='day',names_from='letter',values_from='value') # doctest: +SKIP\n           day    A    B\n        0  Tue  2.0  NaN\n        1  Thu  NaN  3.0\n        2  Fri  NaN  1.0\n        3  Mon  5.0  NaN\n        &gt;&gt;&gt; (daily # doctest: +SKIP\n        ... .pivot_wider(\n        ...     index='day',\n        ...     names_from='letter',\n        ...     values_from='value',\n        ...     index_expand=True)\n        ... )\n           day    A    B\n        0  Mon  5.0  NaN\n        1  Tue  2.0  NaN\n        2  Wed  NaN  NaN\n        3  Thu  NaN  3.0\n        4  Fri  NaN  1.0\n        5  Sat  NaN  NaN\n        6  Sun  NaN  NaN\n\n\n    !!! abstract \"Version Changed\"\n\n        - 0.24.0\n            - Added `reset_index`, `names_expand` and `index_expand` parameters.\n\n    Args:\n        df: A pandas DataFrame.\n        index: Name(s) of columns to use as identifier variables.\n            It should be either a single column name, or a list of column names.\n            If `index` is not provided, the DataFrame's index is used.\n        names_from: Name(s) of column(s) to use to make the new\n            DataFrame's columns. Should be either a single column name,\n            or a list of column names.\n        values_from: Name(s) of column(s) that will be used for populating\n            the new DataFrame's values.\n            If `values_from` is not specified,  all remaining columns\n            will be used.\n        flatten_levels: If `False`, the DataFrame stays as a MultiIndex.\n        names_sep: If `names_from` or `values_from` contain multiple\n            variables, this will be used to join the values into a single string\n            to use as a column name. Default is `_`.\n            Applicable only if `flatten_levels` is `True`.\n        names_glue: A string to control the output of the flattened columns.\n            It offers more flexibility in creating custom column names,\n            and uses python's `str.format_map` under the hood.\n            Simply create the string template,\n            using the column labels in `names_from`,\n            and special `_value` as a placeholder for `values_from`.\n            Applicable only if `flatten_levels` is `True`.\n        reset_index: Determines whether to restore `index`\n            as a column/columns. Applicable only if `index` is provided,\n            and `flatten_levels` is `True`.\n        names_expand: Expand columns to show all the categories.\n            Applies only if `names_from` is a categorical column.\n        index_expand: Expand the index to show all the categories.\n            Applies only if `index` is a categorical column.\n\n    Returns:\n        A pandas DataFrame that has been unpivoted from long to wide form.\n    \"\"\"  # noqa: E501\n\n    # no need for an explicit copy --&gt; df = df.copy()\n    # `pd.pivot` creates one\n    return _computations_pivot_wider(\n        df,\n        index,\n        names_from,\n        values_from,\n        flatten_levels,\n        names_sep,\n        names_glue,\n        reset_index,\n        names_expand,\n        index_expand,\n    )\n</code></pre>"},{"location":"api/functions/#janitor.functions.process_text","title":"<code>process_text</code>","text":"<p>Implementation source for <code>process_text</code>.</p>"},{"location":"api/functions/#janitor.functions.process_text.process_text","title":"<code>process_text(df, column_name, string_function, **kwargs)</code>","text":"<p>Apply a Pandas string method to an existing column.</p> <p>This function aims to make string cleaning easy, while chaining, by simply passing the string method name, along with keyword arguments, if any, to the function.</p> <p>This modifies an existing column; it does not create a new column; new columns can be created via pyjanitor's <code>transform_columns</code>.</p> <p>A list of all the string methods in Pandas can be accessed here.</p> <p>Note</p> <p>This function will be deprecated in a 1.x release. Please use <code>jn.transform_column</code> instead.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; import re\n&gt;&gt;&gt; df = pd.DataFrame({\"text\": [\"Ragnar\", \"sammywemmy\", \"ginger\"],\n... \"code\": [1, 2, 3]})\n&gt;&gt;&gt; df\n         text  code\n0      Ragnar     1\n1  sammywemmy     2\n2      ginger     3\n&gt;&gt;&gt; df.process_text(column_name=\"text\", string_function=\"lower\")\n         text  code\n0      ragnar     1\n1  sammywemmy     2\n2      ginger     3\n</code></pre> <p>For string methods with parameters, simply pass the keyword arguments:</p> <pre><code>&gt;&gt;&gt; df.process_text(\n...     column_name=\"text\",\n...     string_function=\"extract\",\n...     pat=r\"(ag)\",\n...     expand=False,\n...     flags=re.IGNORECASE,\n... )\n  text  code\n0   ag     1\n1  NaN     2\n2  NaN     3\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>column_name</code> <code>str</code> <p>String column to be operated on.</p> required <code>string_function</code> <code>str</code> <p>pandas string method to be applied.</p> required <code>**kwargs</code> <code>Any</code> <p>Keyword arguments for parameters of the <code>string_function</code>.</p> <code>{}</code> <p>Raises:</p> Type Description <code>KeyError</code> <p>If <code>string_function</code> is not a Pandas string method.</p> <code>ValueError</code> <p>If the text function returns a DataFrame, instead of a Series.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with modified column.</p> Source code in <code>janitor/functions/process_text.py</code> <pre><code>@pf.register_dataframe_method\n@refactored_function(\n    message=(\n        \"This function will be deprecated in a 1.x release. \"\n        \"Please use `jn.transform_columns` instead.\"\n    )\n)\n@deprecated_alias(column=\"column_name\")\ndef process_text(\n    df: pd.DataFrame,\n    column_name: str,\n    string_function: str,\n    **kwargs: Any,\n) -&gt; pd.DataFrame:\n    \"\"\"Apply a Pandas string method to an existing column.\n\n    This function aims to make string cleaning easy, while chaining,\n    by simply passing the string method name,\n    along with keyword arguments, if any, to the function.\n\n    This modifies an existing column; it does not create a new column;\n    new columns can be created via pyjanitor's\n    [`transform_columns`][janitor.functions.transform_columns.transform_columns].\n\n    A list of all the string methods in Pandas can be accessed [here](https://pandas.pydata.org/docs/user_guide/text.html#method-summary).\n\n    !!!note\n\n        This function will be deprecated in a 1.x release.\n        Please use [`jn.transform_column`][janitor.functions.transform_columns.transform_column]\n        instead.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; import re\n        &gt;&gt;&gt; df = pd.DataFrame({\"text\": [\"Ragnar\", \"sammywemmy\", \"ginger\"],\n        ... \"code\": [1, 2, 3]})\n        &gt;&gt;&gt; df\n                 text  code\n        0      Ragnar     1\n        1  sammywemmy     2\n        2      ginger     3\n        &gt;&gt;&gt; df.process_text(column_name=\"text\", string_function=\"lower\")\n                 text  code\n        0      ragnar     1\n        1  sammywemmy     2\n        2      ginger     3\n\n        For string methods with parameters, simply pass the keyword arguments:\n\n        &gt;&gt;&gt; df.process_text(\n        ...     column_name=\"text\",\n        ...     string_function=\"extract\",\n        ...     pat=r\"(ag)\",\n        ...     expand=False,\n        ...     flags=re.IGNORECASE,\n        ... )\n          text  code\n        0   ag     1\n        1  NaN     2\n        2  NaN     3\n\n    Args:\n        df: A pandas DataFrame.\n        column_name: String column to be operated on.\n        string_function: pandas string method to be applied.\n        **kwargs: Keyword arguments for parameters of the `string_function`.\n\n    Raises:\n        KeyError: If `string_function` is not a Pandas string method.\n        ValueError: If the text function returns a DataFrame, instead of a Series.\n\n    Returns:\n        A pandas DataFrame with modified column.\n    \"\"\"  # noqa: E501\n\n    check(\"column_name\", column_name, [str])\n    check(\"string_function\", string_function, [str])\n    check_column(df, [column_name])\n\n    pandas_string_methods = [\n        func.__name__\n        for _, func in inspect.getmembers(pd.Series.str, inspect.isfunction)\n        if not func.__name__.startswith(\"_\")\n    ]\n\n    if string_function not in pandas_string_methods:\n        raise KeyError(f\"{string_function} is not a Pandas string method.\")\n\n    result = getattr(df[column_name].str, string_function)(**kwargs)\n\n    if isinstance(result, pd.DataFrame):\n        raise ValueError(\n            \"The outcome of the processed text is a DataFrame, \"\n            \"which is not supported in `process_text`.\"\n        )\n\n    return df.assign(**{column_name: result})\n</code></pre>"},{"location":"api/functions/#janitor.functions.remove_columns","title":"<code>remove_columns</code>","text":"<p>Implementation of remove_columns.</p>"},{"location":"api/functions/#janitor.functions.remove_columns.remove_columns","title":"<code>remove_columns(df, column_names)</code>","text":"<p>Remove the set of columns specified in <code>column_names</code>.</p> <p>This method does not mutate the original DataFrame.</p> <p>Intended to be the method-chaining alternative to <code>del df[col]</code>.</p> <p>Note</p> <p>This function will be deprecated in a 1.x release. Kindly use <code>pd.DataFrame.drop</code> instead.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\"a\": [2, 4, 6], \"b\": [1, 3, 5], \"c\": [7, 8, 9]})\n&gt;&gt;&gt; df\n   a  b  c\n0  2  1  7\n1  4  3  8\n2  6  5  9\n&gt;&gt;&gt; df.remove_columns(column_names=['a', 'c'])\n   b\n0  1\n1  3\n2  5\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>column_names</code> <code>Union[str, Iterable[str], Hashable]</code> <p>The columns to remove.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame.</p> Source code in <code>janitor/functions/remove_columns.py</code> <pre><code>@pf.register_dataframe_method\n@refactored_function(\n    message=(\n        \"This function will be deprecated in a 1.x release. \"\n        \"Please use `pd.DataFrame.drop` instead.\"\n    )\n)\n@deprecated_alias(columns=\"column_names\")\ndef remove_columns(\n    df: pd.DataFrame,\n    column_names: Union[str, Iterable[str], Hashable],\n) -&gt; pd.DataFrame:\n    \"\"\"Remove the set of columns specified in `column_names`.\n\n    This method does not mutate the original DataFrame.\n\n    Intended to be the method-chaining alternative to `del df[col]`.\n\n    !!!note\n\n        This function will be deprecated in a 1.x release.\n        Kindly use `pd.DataFrame.drop` instead.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\"a\": [2, 4, 6], \"b\": [1, 3, 5], \"c\": [7, 8, 9]})\n        &gt;&gt;&gt; df\n           a  b  c\n        0  2  1  7\n        1  4  3  8\n        2  6  5  9\n        &gt;&gt;&gt; df.remove_columns(column_names=['a', 'c'])\n           b\n        0  1\n        1  3\n        2  5\n\n    Args:\n        df: A pandas DataFrame.\n        column_names: The columns to remove.\n\n    Returns:\n        A pandas DataFrame.\n    \"\"\"\n\n    return df.drop(columns=column_names)\n</code></pre>"},{"location":"api/functions/#janitor.functions.remove_empty","title":"<code>remove_empty</code>","text":"<p>Implementation of remove_empty.</p>"},{"location":"api/functions/#janitor.functions.remove_empty.remove_empty","title":"<code>remove_empty(df, reset_index=True)</code>","text":"<p>Drop all rows and columns that are completely null.</p> <p>This method does not mutate the original DataFrame.</p> <p>Implementation is inspired from StackOverflow.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"a\": [1, np.nan, 2],\n...     \"b\": [3, np.nan, 4],\n...     \"c\": [np.nan, np.nan, np.nan],\n... })\n&gt;&gt;&gt; df\n     a    b   c\n0  1.0  3.0 NaN\n1  NaN  NaN NaN\n2  2.0  4.0 NaN\n&gt;&gt;&gt; df.remove_empty()\n     a    b\n0  1.0  3.0\n1  2.0  4.0\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The pandas DataFrame object.</p> required <code>reset_index</code> <code>bool</code> <p>Determines if the index is reset.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame.</p> Source code in <code>janitor/functions/remove_empty.py</code> <pre><code>@pf.register_dataframe_method\ndef remove_empty(df: pd.DataFrame, reset_index: bool = True) -&gt; pd.DataFrame:\n    \"\"\"Drop all rows and columns that are completely null.\n\n    This method does not mutate the original DataFrame.\n\n    Implementation is inspired from [StackOverflow][so].\n\n    [so]: https://stackoverflow.com/questions/38884538/python-pandas-find-all-rows-where-all-values-are-nan\n\n    Examples:\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     \"a\": [1, np.nan, 2],\n        ...     \"b\": [3, np.nan, 4],\n        ...     \"c\": [np.nan, np.nan, np.nan],\n        ... })\n        &gt;&gt;&gt; df\n             a    b   c\n        0  1.0  3.0 NaN\n        1  NaN  NaN NaN\n        2  2.0  4.0 NaN\n        &gt;&gt;&gt; df.remove_empty()\n             a    b\n        0  1.0  3.0\n        1  2.0  4.0\n\n    Args:\n        df: The pandas DataFrame object.\n        reset_index: Determines if the index is reset.\n\n    Returns:\n        A pandas DataFrame.\n    \"\"\"  # noqa: E501\n    outcome = df.isna()\n    outcome = df.loc[~outcome.all(axis=1), ~outcome.all(axis=0)]\n    if reset_index:\n        return outcome.reset_index(drop=True)\n    return outcome\n</code></pre>"},{"location":"api/functions/#janitor.functions.rename_columns","title":"<code>rename_columns</code>","text":""},{"location":"api/functions/#janitor.functions.rename_columns.rename_column","title":"<code>rename_column(df, old_column_name, new_column_name)</code>","text":"<p>Rename a column in place.</p> <p>This method does not mutate the original DataFrame.</p> <p>Note</p> <p>This function will be deprecated in a 1.x release. Please use <code>pd.DataFrame.rename</code> instead.</p> <p>This is just syntactic sugar/a convenience function for renaming one column at a time. If you are convinced that there are multiple columns in need of changing, then use the <code>pandas.DataFrame.rename</code> method.</p> <p>Examples:</p> <p>Change the name of column 'a' to 'a_new'.</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"abc\")})\n&gt;&gt;&gt; df.rename_column(old_column_name='a', new_column_name='a_new')\n   a_new  b\n0      0  a\n1      1  b\n2      2  c\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The pandas DataFrame object.</p> required <code>old_column_name</code> <code>str</code> <p>The old column name.</p> required <code>new_column_name</code> <code>str</code> <p>The new column name.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with renamed columns.</p> Source code in <code>janitor/functions/rename_columns.py</code> <pre><code>@pf.register_dataframe_method\n@refactored_function(\n    message=(\n        \"This function will be deprecated in a 1.x release. \"\n        \"Please use `pd.DataFrame.rename` instead.\"\n    )\n)\n@deprecated_alias(old=\"old_column_name\", new=\"new_column_name\")\ndef rename_column(\n    df: pd.DataFrame,\n    old_column_name: str,\n    new_column_name: str,\n) -&gt; pd.DataFrame:\n    \"\"\"Rename a column in place.\n\n    This method does not mutate the original DataFrame.\n\n    !!!note\n\n        This function will be deprecated in a 1.x release.\n        Please use `pd.DataFrame.rename` instead.\n\n    This is just syntactic sugar/a convenience function for renaming one column at a time.\n    If you are convinced that there are multiple columns in need of changing,\n    then use the `pandas.DataFrame.rename` method.\n\n    Examples:\n        Change the name of column 'a' to 'a_new'.\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"abc\")})\n        &gt;&gt;&gt; df.rename_column(old_column_name='a', new_column_name='a_new')\n           a_new  b\n        0      0  a\n        1      1  b\n        2      2  c\n\n    Args:\n        df: The pandas DataFrame object.\n        old_column_name: The old column name.\n        new_column_name: The new column name.\n\n    Returns:\n        A pandas DataFrame with renamed columns.\n    \"\"\"  # noqa: E501\n\n    check_column(df, [old_column_name])\n\n    return df.rename(columns={old_column_name: new_column_name})\n</code></pre>"},{"location":"api/functions/#janitor.functions.rename_columns.rename_columns","title":"<code>rename_columns(df, new_column_names=None, function=None)</code>","text":"<p>Rename columns.</p> <p>This method does not mutate the original DataFrame.</p> <p>Note</p> <p>This function will be deprecated in a 1.x release. Please use <code>pd.DataFrame.rename</code> instead.</p> <p>One of the <code>new_column_names</code> or <code>function</code> are a required parameter. If both are provided, then <code>new_column_names</code> takes priority and <code>function</code> is never executed.</p> <p>Examples:</p> <p>Rename columns using a dictionary which maps old names to new names.</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"xyz\")})\n&gt;&gt;&gt; df\n   a  b\n0  0  x\n1  1  y\n2  2  z\n&gt;&gt;&gt; df.rename_columns(new_column_names={\"a\": \"a_new\", \"b\": \"b_new\"})\n   a_new b_new\n0      0     x\n1      1     y\n2      2     z\n</code></pre> <p>Rename columns using a generic callable.</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"xyz\")})\n&gt;&gt;&gt; df.rename_columns(function=str.upper)\n   A  B\n0  0  x\n1  1  y\n2  2  z\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The pandas DataFrame object.</p> required <code>new_column_names</code> <code>Union[Dict, None]</code> <p>A dictionary of old and new column names.</p> <code>None</code> <code>function</code> <code>Callable</code> <p>A function which should be applied to all the columns.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both <code>new_column_names</code> and <code>function</code> are None.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with renamed columns.</p> Source code in <code>janitor/functions/rename_columns.py</code> <pre><code>@pf.register_dataframe_method\n@refactored_function(\n    message=(\n        \"This function will be deprecated in a 1.x release. \"\n        \"Please use `pd.DataFrame.rename` instead.\"\n    )\n)\ndef rename_columns(\n    df: pd.DataFrame,\n    new_column_names: Union[Dict, None] = None,\n    function: Callable = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Rename columns.\n\n    This method does not mutate the original DataFrame.\n\n    !!!note\n\n        This function will be deprecated in a 1.x release.\n        Please use `pd.DataFrame.rename` instead.\n\n    One of the `new_column_names` or `function` are a required parameter.\n    If both are provided, then `new_column_names` takes priority and `function`\n    is never executed.\n\n    Examples:\n        Rename columns using a dictionary which maps old names to new names.\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"xyz\")})\n        &gt;&gt;&gt; df\n           a  b\n        0  0  x\n        1  1  y\n        2  2  z\n        &gt;&gt;&gt; df.rename_columns(new_column_names={\"a\": \"a_new\", \"b\": \"b_new\"})\n           a_new b_new\n        0      0     x\n        1      1     y\n        2      2     z\n\n        Rename columns using a generic callable.\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\"a\": list(range(3)), \"b\": list(\"xyz\")})\n        &gt;&gt;&gt; df.rename_columns(function=str.upper)\n           A  B\n        0  0  x\n        1  1  y\n        2  2  z\n\n    Args:\n        df: The pandas DataFrame object.\n        new_column_names: A dictionary of old and new column names.\n        function: A function which should be applied to all the columns.\n\n    Raises:\n        ValueError: If both `new_column_names` and `function` are None.\n\n    Returns:\n        A pandas DataFrame with renamed columns.\n    \"\"\"  # noqa: E501\n\n    if new_column_names is None and function is None:\n        raise ValueError(\n            \"One of new_column_names or function must be provided\"\n        )\n\n    if new_column_names is not None:\n        check_column(df, new_column_names)\n        return df.rename(columns=new_column_names)\n\n    return df.rename(mapper=function, axis=\"columns\")\n</code></pre>"},{"location":"api/functions/#janitor.functions.reorder_columns","title":"<code>reorder_columns</code>","text":"<p>Implementation source for <code>reorder_columns</code>.</p>"},{"location":"api/functions/#janitor.functions.reorder_columns.reorder_columns","title":"<code>reorder_columns(df, column_order)</code>","text":"<p>Reorder DataFrame columns by specifying desired order as list of col names.</p> <p>Columns not specified retain their order and follow after the columns specified in <code>column_order</code>.</p> <p>All columns specified within the <code>column_order</code> list must be present within <code>df</code>.</p> <p>This method does not mutate the original DataFrame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\"col1\": [1, 1, 1], \"col2\": [2, 2, 2], \"col3\": [3, 3, 3]})\n&gt;&gt;&gt; df\n   col1  col2  col3\n0     1     2     3\n1     1     2     3\n2     1     2     3\n&gt;&gt;&gt; df.reorder_columns(['col3', 'col1'])\n   col3  col1  col2\n0     3     1     2\n1     3     1     2\n2     3     1     2\n</code></pre> <p>Notice that the column order of <code>df</code> is now <code>col3</code>, <code>col1</code>, <code>col2</code>.</p> <p>Internally, this function uses <code>DataFrame.reindex</code> with <code>copy=False</code> to avoid unnecessary data duplication.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p><code>DataFrame</code> to reorder</p> required <code>column_order</code> <code>Union[Iterable[str], Index, Hashable]</code> <p>A list of column names or Pandas <code>Index</code> specifying their order in the returned <code>DataFrame</code>.</p> required <p>Raises:</p> Type Description <code>IndexError</code> <p>If a column within <code>column_order</code> is not found within the DataFrame.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with reordered columns.</p> Source code in <code>janitor/functions/reorder_columns.py</code> <pre><code>@pf.register_dataframe_method\ndef reorder_columns(\n    df: pd.DataFrame, column_order: Union[Iterable[str], pd.Index, Hashable]\n) -&gt; pd.DataFrame:\n    \"\"\"Reorder DataFrame columns by specifying desired order as list of col names.\n\n    Columns not specified retain their order and follow after the columns specified\n    in `column_order`.\n\n    All columns specified within the `column_order` list must be present within `df`.\n\n    This method does not mutate the original DataFrame.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\"col1\": [1, 1, 1], \"col2\": [2, 2, 2], \"col3\": [3, 3, 3]})\n        &gt;&gt;&gt; df\n           col1  col2  col3\n        0     1     2     3\n        1     1     2     3\n        2     1     2     3\n        &gt;&gt;&gt; df.reorder_columns(['col3', 'col1'])\n           col3  col1  col2\n        0     3     1     2\n        1     3     1     2\n        2     3     1     2\n\n        Notice that the column order of `df` is now `col3`, `col1`, `col2`.\n\n    Internally, this function uses `DataFrame.reindex` with `copy=False`\n    to avoid unnecessary data duplication.\n\n    Args:\n        df: `DataFrame` to reorder\n        column_order: A list of column names or Pandas `Index`\n            specifying their order in the returned `DataFrame`.\n\n    Raises:\n        IndexError: If a column within `column_order` is not found\n            within the DataFrame.\n\n    Returns:\n        A pandas DataFrame with reordered columns.\n    \"\"\"  # noqa: E501\n    check(\"column_order\", column_order, [list, tuple, pd.Index])\n\n    if any(col not in df.columns for col in column_order):\n        raise IndexError(\n            \"One or more columns in `column_order` were not found in the \"\n            \"DataFrame.\"\n        )\n\n    # if column_order is a Pandas index, needs conversion to list:\n    column_order = list(column_order)\n\n    return df.reindex(\n        columns=(\n            column_order\n            + [col for col in df.columns if col not in column_order]\n        ),\n        copy=False,\n    )\n</code></pre>"},{"location":"api/functions/#janitor.functions.round_to_fraction","title":"<code>round_to_fraction</code>","text":"<p>Implementation of <code>round_to_fraction</code></p>"},{"location":"api/functions/#janitor.functions.round_to_fraction.round_to_fraction","title":"<code>round_to_fraction(df, column_name, denominator, digits=np.inf)</code>","text":"<p>Round all values in a column to a fraction.</p> <p>This method mutates the original DataFrame.</p> <p>Taken from the R package.</p> <p>Also, optionally round to a specified number of digits.</p> <p>Examples:</p> <p>Round numeric column to the nearest 1/4 value.</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"a1\": [1.263, 2.499, np.nan],\n...     \"a2\": [\"x\", \"y\", \"z\"],\n... })\n&gt;&gt;&gt; df\n      a1 a2\n0  1.263  x\n1  2.499  y\n2    NaN  z\n&gt;&gt;&gt; df.round_to_fraction(\"a1\", denominator=4)\n     a1 a2\n0  1.25  x\n1  2.50  y\n2   NaN  z\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>column_name</code> <code>Hashable</code> <p>Name of column to round to fraction.</p> required <code>denominator</code> <code>float</code> <p>The denominator of the fraction for rounding. Must be a positive number.</p> required <code>digits</code> <code>float</code> <p>The number of digits for rounding after rounding to the fraction. Default is np.inf (i.e. no subsequent rounding).</p> <code>inf</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>denominator</code> is not a positive number.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with a column's values rounded.</p> Source code in <code>janitor/functions/round_to_fraction.py</code> <pre><code>@pf.register_dataframe_method\n@deprecated_alias(col_name=\"column_name\")\ndef round_to_fraction(\n    df: pd.DataFrame,\n    column_name: Hashable,\n    denominator: float,\n    digits: float = np.inf,\n) -&gt; pd.DataFrame:\n    \"\"\"Round all values in a column to a fraction.\n\n    This method mutates the original DataFrame.\n\n    Taken from [the R package](https://github.com/sfirke/janitor/issues/235).\n\n    Also, optionally round to a specified number of digits.\n\n    Examples:\n        Round numeric column to the nearest 1/4 value.\n\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     \"a1\": [1.263, 2.499, np.nan],\n        ...     \"a2\": [\"x\", \"y\", \"z\"],\n        ... })\n        &gt;&gt;&gt; df\n              a1 a2\n        0  1.263  x\n        1  2.499  y\n        2    NaN  z\n        &gt;&gt;&gt; df.round_to_fraction(\"a1\", denominator=4)\n             a1 a2\n        0  1.25  x\n        1  2.50  y\n        2   NaN  z\n\n    Args:\n        df: A pandas DataFrame.\n        column_name: Name of column to round to fraction.\n        denominator: The denominator of the fraction for rounding. Must be\n            a positive number.\n        digits: The number of digits for rounding after rounding to the\n            fraction. Default is np.inf (i.e. no subsequent rounding).\n\n    Raises:\n        ValueError: If `denominator` is not a positive number.\n\n    Returns:\n        A pandas DataFrame with a column's values rounded.\n    \"\"\"\n    check_column(df, column_name)\n    check(\"denominator\", denominator, [float, int])\n    check(\"digits\", digits, [float, int])\n\n    if denominator &lt;= 0:\n        raise ValueError(\"denominator is expected to be a positive number.\")\n\n    df[column_name] = round(df[column_name] * denominator, 0) / denominator\n    if not np.isinf(digits):\n        df[column_name] = round(df[column_name], digits)\n\n    return df\n</code></pre>"},{"location":"api/functions/#janitor.functions.row_to_names","title":"<code>row_to_names</code>","text":"<p>Implementation of the <code>row_to_names</code> function.</p>"},{"location":"api/functions/#janitor.functions.row_to_names.row_to_names","title":"<code>row_to_names(df, row_numbers=0, remove_rows=False, remove_rows_above=False, reset_index=False)</code>","text":"<p>Elevates a row, or rows, to be the column names of a DataFrame.</p> <p>This method does not mutate the original DataFrame.</p> <p>Contains options to remove the elevated row from the DataFrame along with removing the rows above the selected row.</p> <p>Examples:</p> <p>Replace column names with the first row and reset the index.</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"a\": [\"nums\", 6, 9],\n...     \"b\": [\"chars\", \"x\", \"y\"],\n... })\n&gt;&gt;&gt; df\n      a      b\n0  nums  chars\n1     6      x\n2     9      y\n&gt;&gt;&gt; df.row_to_names(0, remove_rows=True, reset_index=True)\n  nums chars\n0    6     x\n1    9     y\n&gt;&gt;&gt; df.row_to_names([0,1], remove_rows=True, reset_index=True)\n  nums chars\n     6     x\n0    9     y\n</code></pre> <p>Remove rows above the elevated row and the elevated row itself.</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"a\": [\"bla1\", \"nums\", 6, 9],\n...     \"b\": [\"bla2\", \"chars\", \"x\", \"y\"],\n... })\n&gt;&gt;&gt; df\n      a      b\n0  bla1   bla2\n1  nums  chars\n2     6      x\n3     9      y\n&gt;&gt;&gt; df.row_to_names(1, remove_rows=True, remove_rows_above=True, reset_index=True)\n  nums chars\n0    6     x\n1    9     y\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>row_numbers</code> <code>int | list | slice</code> <p>Position of the row(s) containing the variable names. It can be an integer, a list or a slice. Defaults to 0 (first row).</p> <code>0</code> <code>remove_rows</code> <code>bool</code> <p>Whether the row(s) should be removed from the DataFrame.</p> <code>False</code> <code>remove_rows_above</code> <code>bool</code> <p>Whether the row(s) above the selected row should be removed from the DataFrame.</p> <code>False</code> <code>reset_index</code> <code>bool</code> <p>Whether the index should be reset on the returning DataFrame.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with set column names.</p> Source code in <code>janitor/functions/row_to_names.py</code> <pre><code>@pf.register_dataframe_method\n@deprecated_alias(row_number=\"row_numbers\", remove_row=\"remove_rows\")\ndef row_to_names(\n    df: pd.DataFrame,\n    row_numbers: int | list | slice = 0,\n    remove_rows: bool = False,\n    remove_rows_above: bool = False,\n    reset_index: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Elevates a row, or rows, to be the column names of a DataFrame.\n\n    This method does not mutate the original DataFrame.\n\n    Contains options to remove the elevated row from the DataFrame along with\n    removing the rows above the selected row.\n\n    Examples:\n        Replace column names with the first row and reset the index.\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     \"a\": [\"nums\", 6, 9],\n        ...     \"b\": [\"chars\", \"x\", \"y\"],\n        ... })\n        &gt;&gt;&gt; df\n              a      b\n        0  nums  chars\n        1     6      x\n        2     9      y\n        &gt;&gt;&gt; df.row_to_names(0, remove_rows=True, reset_index=True)\n          nums chars\n        0    6     x\n        1    9     y\n        &gt;&gt;&gt; df.row_to_names([0,1], remove_rows=True, reset_index=True)\n          nums chars\n             6     x\n        0    9     y\n\n        Remove rows above the elevated row and the elevated row itself.\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     \"a\": [\"bla1\", \"nums\", 6, 9],\n        ...     \"b\": [\"bla2\", \"chars\", \"x\", \"y\"],\n        ... })\n        &gt;&gt;&gt; df\n              a      b\n        0  bla1   bla2\n        1  nums  chars\n        2     6      x\n        3     9      y\n        &gt;&gt;&gt; df.row_to_names(1, remove_rows=True, remove_rows_above=True, reset_index=True)\n          nums chars\n        0    6     x\n        1    9     y\n\n    Args:\n        df: A pandas DataFrame.\n        row_numbers: Position of the row(s) containing the variable names.\n            It can be an integer, a list or a slice.\n            Defaults to 0 (first row).\n        remove_rows: Whether the row(s) should be removed from the DataFrame.\n        remove_rows_above: Whether the row(s) above the selected row should\n            be removed from the DataFrame.\n        reset_index: Whether the index should be reset on the returning DataFrame.\n\n    Returns:\n        A pandas DataFrame with set column names.\n    \"\"\"  # noqa: E501\n\n    return _row_to_names(\n        row_numbers,\n        df=df,\n        remove_rows=remove_rows,\n        remove_rows_above=remove_rows_above,\n        reset_index=reset_index,\n    )\n</code></pre>"},{"location":"api/functions/#janitor.functions.select","title":"<code>select</code>","text":""},{"location":"api/functions/#janitor.functions.select.DropLabel","title":"<code>DropLabel</code>  <code>dataclass</code>","text":"<p>Helper class for removing labels within the <code>select</code> syntax.</p> <p><code>label</code> can be any of the types supported in the <code>select</code>, <code>select_rows</code> and <code>select_columns</code> functions. An array of integers not matching the labels is returned.</p> <p>New in version 0.24.0</p> <p>Parameters:</p> Name Type Description Default <code>label</code> <code>Any</code> <p>Label(s) to be dropped from the index.</p> required Source code in <code>janitor/functions/select.py</code> <pre><code>@dataclass\nclass DropLabel:\n    \"\"\"Helper class for removing labels within the `select` syntax.\n\n    `label` can be any of the types supported in the `select`,\n    `select_rows` and `select_columns` functions.\n    An array of integers not matching the labels is returned.\n\n    !!! info \"New in version 0.24.0\"\n\n    Args:\n        label: Label(s) to be dropped from the index.\n    \"\"\"\n\n    label: Any\n</code></pre>"},{"location":"api/functions/#janitor.functions.select.get_columns","title":"<code>get_columns(group, label)</code>","text":"<p>Helper function for selecting columns on a grouped object, using the <code>select</code> syntax.</p> <p>New in version 0.25.0</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>DataFrameGroupBy | SeriesGroupBy</code> <p>A Pandas GroupBy object.</p> required <code>label</code> <code>Any</code> <p>column(s) to select.</p> required <p>Returns:</p> Type Description <code>DataFrameGroupBy | SeriesGroupBy</code> <p>A pandas groupby object.</p> Source code in <code>janitor/functions/select.py</code> <pre><code>def get_columns(\n    group: DataFrameGroupBy | SeriesGroupBy, label: Any\n) -&gt; DataFrameGroupBy | SeriesGroupBy:\n    \"\"\"\n    Helper function for selecting columns on a grouped object,\n    using the\n    [`select`][janitor.functions.select.select] syntax.\n\n    !!! info \"New in version 0.25.0\"\n\n    Args:\n        group: A Pandas GroupBy object.\n        label: column(s) to select.\n\n    Returns:\n        A pandas groupby object.\n    \"\"\"\n    check(\"groupby object\", group, [DataFrameGroupBy, SeriesGroupBy])\n    label = get_index_labels(label, group.obj, axis=\"columns\")\n    label = label if is_scalar(label) else list(label)\n    return group[label]\n</code></pre>"},{"location":"api/functions/#janitor.functions.select.get_index_labels","title":"<code>get_index_labels(arg, df, axis)</code>","text":"<p>Convenience function to get actual labels from column/index</p> <p>New in version 0.25.0</p> <p>Parameters:</p> Name Type Description Default <code>arg</code> <code>Any</code> <p>Valid inputs include: an exact column name to look for, a shell-style glob string (e.g. <code>*_thing_*</code>), a regular expression, a callable, or variable arguments of all the aforementioned. A sequence of booleans is also acceptable. A dictionary can be used for selection on a MultiIndex on different levels.</p> required <code>df</code> <code>DataFrame</code> <p>The pandas DataFrame object.</p> required <code>axis</code> <code>Literal['index', 'columns']</code> <p>Should be either <code>index</code> or <code>columns</code>.</p> required <p>Returns:</p> Type Description <code>Index</code> <p>A pandas Index.</p> Source code in <code>janitor/functions/select.py</code> <pre><code>def get_index_labels(\n    arg: Any, df: pd.DataFrame, axis: Literal[\"index\", \"columns\"]\n) -&gt; pd.Index:\n    \"\"\"Convenience function to get actual labels from column/index\n\n    !!! info \"New in version 0.25.0\"\n\n    Args:\n        arg: Valid inputs include: an exact column name to look for,\n            a shell-style glob string (e.g. `*_thing_*`),\n            a regular expression,\n            a callable,\n            or variable arguments of all the aforementioned.\n            A sequence of booleans is also acceptable.\n            A dictionary can be used for selection\n            on a MultiIndex on different levels.\n        df: The pandas DataFrame object.\n        axis: Should be either `index` or `columns`.\n\n    Returns:\n        A pandas Index.\n    \"\"\"\n    assert axis in {\"index\", \"columns\"}\n    index = getattr(df, axis)\n    return index[_select_index(arg, df, axis)]\n</code></pre>"},{"location":"api/functions/#janitor.functions.select.select","title":"<code>select(df, *args, index=None, columns=None, axis='columns', invert=False)</code>","text":"<p>Method-chainable selection of rows and columns.</p> <p>It accepts a string, shell-like glob strings <code>(*string*)</code>, regex, slice, array-like object, or a list of the previous options.</p> <p>Selection on a MultiIndex on a level, or multiple levels, is possible with a dictionary.</p> <p>This method does not mutate the original DataFrame.</p> <p>Selection can be inverted with the <code>DropLabel</code> class.</p> <p>Optional ability to invert selection of index/columns available as well.</p> <p>New in version 0.24.0</p> <p>Note</p> <p>The preferred option when selecting columns or rows in a Pandas DataFrame is with <code>.loc</code> or <code>.iloc</code> methods, as they are generally performant. <code>select</code> is primarily for convenience.</p> <p>Version Changed</p> <ul> <li>0.26.0<ul> <li>Added variable <code>args</code>, <code>invert</code> and <code>axis</code> parameters.</li> <li><code>rows</code> keyword deprecated in favour of <code>index</code>.</li> </ul> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n...      index=['cobra', 'viper', 'sidewinder'],\n...      columns=['max_speed', 'shield'])\n&gt;&gt;&gt; df\n            max_speed  shield\ncobra               1       2\nviper               4       5\nsidewinder          7       8\n&gt;&gt;&gt; df.select(index='cobra', columns='shield')\n       shield\ncobra       2\n</code></pre> <p>Labels can be dropped with the <code>DropLabel</code> class:</p> <pre><code>&gt;&gt;&gt; df.select(index=DropLabel('cobra'))\n            max_speed  shield\nviper               4       5\nsidewinder          7       8\n</code></pre> <p>More examples can be found in the <code>select_columns</code> section.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>*args</code> <code>tuple</code> <p>Valid inputs include: an exact index name to look for, a shell-style glob string (e.g. <code>*_thing_*</code>), a regular expression, a callable, or variable arguments of all the aforementioned. A sequence of booleans is also acceptable. A dictionary can be used for selection on a MultiIndex on different levels.</p> <code>()</code> <code>index</code> <code>Any</code> <p>Valid inputs include: an exact label to look for, a shell-style glob string (e.g. <code>*_thing_*</code>), a regular expression, a callable, or variable arguments of all the aforementioned. A sequence of booleans is also acceptable. A dictionary can be used for selection on a MultiIndex on different levels.</p> <code>None</code> <code>columns</code> <code>Any</code> <p>Valid inputs include: an exact label to look for, a shell-style glob string (e.g. <code>*_thing_*</code>), a regular expression, a callable, or variable arguments of all the aforementioned. A sequence of booleans is also acceptable. A dictionary can be used for selection on a MultiIndex on different levels.</p> <code>None</code> <code>invert</code> <code>bool</code> <p>Whether or not to invert the selection. This will result in the selection of the complement of the rows/columns provided.</p> <code>False</code> <code>axis</code> <code>str</code> <p>Whether the selection should be on the index('index'), or columns('columns'). Applicable only for the variable args parameter.</p> <code>'columns'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If args and index/columns are provided.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with the specified rows and/or columns selected.</p> Source code in <code>janitor/functions/select.py</code> <pre><code>@pf.register_dataframe_method\n@deprecated_alias(rows=\"index\")\ndef select(\n    df: pd.DataFrame,\n    *args: tuple,\n    index: Any = None,\n    columns: Any = None,\n    axis: str = \"columns\",\n    invert: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Method-chainable selection of rows and columns.\n\n    It accepts a string, shell-like glob strings `(*string*)`,\n    regex, slice, array-like object, or a list of the previous options.\n\n    Selection on a MultiIndex on a level, or multiple levels,\n    is possible with a dictionary.\n\n    This method does not mutate the original DataFrame.\n\n    Selection can be inverted with the `DropLabel` class.\n\n    Optional ability to invert selection of index/columns available as well.\n\n\n    !!! info \"New in version 0.24.0\"\n\n\n    !!!note\n\n        The preferred option when selecting columns or rows in a Pandas DataFrame\n        is with `.loc` or `.iloc` methods, as they are generally performant.\n        `select` is primarily for convenience.\n\n    !!! abstract \"Version Changed\"\n\n        - 0.26.0\n            - Added variable `args`, `invert` and `axis` parameters.\n            - `rows` keyword deprecated in favour of `index`.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame([[1, 2], [4, 5], [7, 8]],\n        ...      index=['cobra', 'viper', 'sidewinder'],\n        ...      columns=['max_speed', 'shield'])\n        &gt;&gt;&gt; df\n                    max_speed  shield\n        cobra               1       2\n        viper               4       5\n        sidewinder          7       8\n        &gt;&gt;&gt; df.select(index='cobra', columns='shield')\n               shield\n        cobra       2\n\n        Labels can be dropped with the `DropLabel` class:\n\n        &gt;&gt;&gt; df.select(index=DropLabel('cobra'))\n                    max_speed  shield\n        viper               4       5\n        sidewinder          7       8\n\n    More examples can be found in the\n    [`select_columns`][janitor.functions.select.select_columns] section.\n\n    Args:\n        df: A pandas DataFrame.\n        *args: Valid inputs include: an exact index name to look for,\n            a shell-style glob string (e.g. `*_thing_*`),\n            a regular expression,\n            a callable,\n            or variable arguments of all the aforementioned.\n            A sequence of booleans is also acceptable.\n            A dictionary can be used for selection\n            on a MultiIndex on different levels.\n        index: Valid inputs include: an exact label to look for,\n            a shell-style glob string (e.g. `*_thing_*`),\n            a regular expression,\n            a callable,\n            or variable arguments of all the aforementioned.\n            A sequence of booleans is also acceptable.\n            A dictionary can be used for selection\n            on a MultiIndex on different levels.\n        columns: Valid inputs include: an exact label to look for,\n            a shell-style glob string (e.g. `*_thing_*`),\n            a regular expression,\n            a callable,\n            or variable arguments of all the aforementioned.\n            A sequence of booleans is also acceptable.\n            A dictionary can be used for selection\n            on a MultiIndex on different levels.\n        invert: Whether or not to invert the selection.\n            This will result in the selection\n            of the complement of the rows/columns provided.\n        axis: Whether the selection should be on the index('index'),\n            or columns('columns').\n            Applicable only for the variable args parameter.\n\n    Raises:\n        ValueError: If args and index/columns are provided.\n\n    Returns:\n        A pandas DataFrame with the specified rows and/or columns selected.\n    \"\"\"  # noqa: E501\n\n    if args:\n        check(\"invert\", invert, [bool])\n        if (index is not None) or (columns is not None):\n            raise ValueError(\n                \"Either provide variable args with the axis parameter, \"\n                \"or provide arguments to the index and/or columns parameters.\"\n            )\n        if axis == \"index\":\n            return _select(df, rows=list(args), columns=columns, invert=invert)\n        if axis == \"columns\":\n            return _select(df, columns=list(args), rows=index, invert=invert)\n        raise ValueError(\"axis should be either 'index' or 'columns'.\")\n    return _select(df, rows=index, columns=columns, invert=invert)\n</code></pre>"},{"location":"api/functions/#janitor.functions.select.select_columns","title":"<code>select_columns(df, *args, invert=False)</code>","text":"<p>Method-chainable selection of columns.</p> <p>It accepts a string, shell-like glob strings <code>(*string*)</code>, regex, slice, array-like object, or a list of the previous options.</p> <p>Selection on a MultiIndex on a level, or multiple levels, is possible with a dictionary.</p> <p>This method does not mutate the original DataFrame.</p> <p>Optional ability to invert selection of columns available as well.</p> <p>Note</p> <p>The preferred option when selecting columns or rows in a Pandas DataFrame is with <code>.loc</code> or <code>.iloc</code> methods. <code>select_columns</code> is primarily for convenience.</p> <p>Note</p> <p>This function will be deprecated in a 1.x release. Please use <code>jn.select</code> instead.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; from numpy import nan\n&gt;&gt;&gt; pd.set_option(\"display.max_columns\", None)\n&gt;&gt;&gt; pd.set_option(\"display.expand_frame_repr\", False)\n&gt;&gt;&gt; pd.set_option(\"max_colwidth\", None)\n&gt;&gt;&gt; data = {'name': ['Cheetah','Owl monkey','Mountain beaver',\n...                  'Greater short-tailed shrew','Cow'],\n...         'genus': ['Acinonyx', 'Aotus', 'Aplodontia', 'Blarina', 'Bos'],\n...         'vore': ['carni', 'omni', 'herbi', 'omni', 'herbi'],\n...         'order': ['Carnivora','Primates','Rodentia','Soricomorpha','Artiodactyla'],\n...         'conservation': ['lc', nan, 'nt', 'lc', 'domesticated'],\n...         'sleep_total': [12.1, 17.0, 14.4, 14.9, 4.0],\n...         'sleep_rem': [nan, 1.8, 2.4, 2.3, 0.7],\n...         'sleep_cycle': [nan, nan, nan, 0.133333333, 0.666666667],\n...         'awake': [11.9, 7.0, 9.6, 9.1, 20.0],\n...         'brainwt': [nan, 0.0155, nan, 0.00029, 0.423],\n...         'bodywt': [50.0, 0.48, 1.35, 0.019, 600.0]}\n&gt;&gt;&gt; df = pd.DataFrame(data)\n&gt;&gt;&gt; df\n                         name       genus   vore         order  conservation  sleep_total  sleep_rem  sleep_cycle  awake  brainwt   bodywt\n0                     Cheetah    Acinonyx  carni     Carnivora            lc         12.1        NaN          NaN   11.9      NaN   50.000\n1                  Owl monkey       Aotus   omni      Primates           NaN         17.0        1.8          NaN    7.0  0.01550    0.480\n2             Mountain beaver  Aplodontia  herbi      Rodentia            nt         14.4        2.4          NaN    9.6      NaN    1.350\n3  Greater short-tailed shrew     Blarina   omni  Soricomorpha            lc         14.9        2.3     0.133333    9.1  0.00029    0.019\n4                         Cow         Bos  herbi  Artiodactyla  domesticated          4.0        0.7     0.666667   20.0  0.42300  600.000\n</code></pre> <p>Explicit label selection:</p> <pre><code>&gt;&gt;&gt; df.select_columns('name', 'order')\n                         name         order\n0                     Cheetah     Carnivora\n1                  Owl monkey      Primates\n2             Mountain beaver      Rodentia\n3  Greater short-tailed shrew  Soricomorpha\n4                         Cow  Artiodactyla\n</code></pre> <p>Selection via globbing:</p> <pre><code>&gt;&gt;&gt; df.select_columns(\"sleep*\", \"*wt\")\n   sleep_total  sleep_rem  sleep_cycle  brainwt   bodywt\n0         12.1        NaN          NaN      NaN   50.000\n1         17.0        1.8          NaN  0.01550    0.480\n2         14.4        2.4          NaN      NaN    1.350\n3         14.9        2.3     0.133333  0.00029    0.019\n4          4.0        0.7     0.666667  0.42300  600.000\n</code></pre> <p>Selection via regex:</p> <pre><code>&gt;&gt;&gt; import re\n&gt;&gt;&gt; df.select_columns(re.compile(r\"o.+er\"))\n          order  conservation\n0     Carnivora            lc\n1      Primates           NaN\n2      Rodentia            nt\n3  Soricomorpha            lc\n4  Artiodactyla  domesticated\n</code></pre> <p>Selection via slicing:</p> <pre><code>&gt;&gt;&gt; df.select_columns(slice('name','order'), slice('sleep_total','sleep_cycle'))\n                         name       genus   vore         order  sleep_total  sleep_rem  sleep_cycle\n0                     Cheetah    Acinonyx  carni     Carnivora         12.1        NaN          NaN\n1                  Owl monkey       Aotus   omni      Primates         17.0        1.8          NaN\n2             Mountain beaver  Aplodontia  herbi      Rodentia         14.4        2.4          NaN\n3  Greater short-tailed shrew     Blarina   omni  Soricomorpha         14.9        2.3     0.133333\n4                         Cow         Bos  herbi  Artiodactyla          4.0        0.7     0.666667\n</code></pre> <p>Selection via callable:</p> <pre><code>&gt;&gt;&gt; from pandas.api.types import is_numeric_dtype\n&gt;&gt;&gt; df.select_columns(is_numeric_dtype)\n   sleep_total  sleep_rem  sleep_cycle  awake  brainwt   bodywt\n0         12.1        NaN          NaN   11.9      NaN   50.000\n1         17.0        1.8          NaN    7.0  0.01550    0.480\n2         14.4        2.4          NaN    9.6      NaN    1.350\n3         14.9        2.3     0.133333    9.1  0.00029    0.019\n4          4.0        0.7     0.666667   20.0  0.42300  600.000\n&gt;&gt;&gt; df.select_columns(lambda f: f.isna().any())\n   conservation  sleep_rem  sleep_cycle  brainwt\n0            lc        NaN          NaN      NaN\n1           NaN        1.8          NaN  0.01550\n2            nt        2.4          NaN      NaN\n3            lc        2.3     0.133333  0.00029\n4  domesticated        0.7     0.666667  0.42300\n</code></pre> <p>Exclude columns with the <code>invert</code> parameter:</p> <pre><code>&gt;&gt;&gt; df.select_columns(is_numeric_dtype, invert=True)\n                         name       genus   vore         order  conservation\n0                     Cheetah    Acinonyx  carni     Carnivora            lc\n1                  Owl monkey       Aotus   omni      Primates           NaN\n2             Mountain beaver  Aplodontia  herbi      Rodentia            nt\n3  Greater short-tailed shrew     Blarina   omni  Soricomorpha            lc\n4                         Cow         Bos  herbi  Artiodactyla  domesticated\n</code></pre> <p>Exclude columns with the <code>DropLabel</code> class:</p> <pre><code>&gt;&gt;&gt; from janitor import DropLabel\n&gt;&gt;&gt; df.select_columns(DropLabel(slice(\"name\", \"awake\")), \"conservation\")\n   brainwt   bodywt  conservation\n0      NaN   50.000            lc\n1  0.01550    0.480           NaN\n2      NaN    1.350            nt\n3  0.00029    0.019            lc\n4  0.42300  600.000  domesticated\n</code></pre> <p>Selection on MultiIndex columns:</p> <pre><code>&gt;&gt;&gt; d = {'num_legs': [4, 4, 2, 2],\n...      'num_wings': [0, 0, 2, 2],\n...      'class': ['mammal', 'mammal', 'mammal', 'bird'],\n...      'animal': ['cat', 'dog', 'bat', 'penguin'],\n...      'locomotion': ['walks', 'walks', 'flies', 'walks']}\n&gt;&gt;&gt; df = pd.DataFrame(data=d)\n&gt;&gt;&gt; df = df.set_index(['class', 'animal', 'locomotion']).T\n&gt;&gt;&gt; df\nclass      mammal                bird\nanimal        cat   dog   bat penguin\nlocomotion  walks walks flies   walks\nnum_legs        4     4     2       2\nnum_wings       0     0     2       2\n</code></pre> <p>Selection with a scalar:</p> <pre><code>&gt;&gt;&gt; df.select_columns('mammal')\nclass      mammal\nanimal        cat   dog   bat\nlocomotion  walks walks flies\nnum_legs        4     4     2\nnum_wings       0     0     2\n</code></pre> <p>Selection with a tuple:</p> <pre><code>&gt;&gt;&gt; df.select_columns(('mammal','bat'))\nclass      mammal\nanimal        bat\nlocomotion  flies\nnum_legs        2\nnum_wings       2\n</code></pre> <p>Selection within a level is possible with a dictionary, where the key is either a level name or number:</p> <pre><code>&gt;&gt;&gt; df.select_columns({'animal':'cat'})\nclass      mammal\nanimal        cat\nlocomotion  walks\nnum_legs        4\nnum_wings       0\n&gt;&gt;&gt; df.select_columns({1:[\"bat\", \"cat\"]})\nclass      mammal\nanimal        bat   cat\nlocomotion  flies walks\nnum_legs        2     4\nnum_wings       2     0\n</code></pre> <p>Selection on multiple levels:</p> <pre><code>&gt;&gt;&gt; df.select_columns({\"class\":\"mammal\", \"locomotion\":\"flies\"})\nclass      mammal\nanimal        bat\nlocomotion  flies\nnum_legs        2\nnum_wings       2\n</code></pre> <p>Selection with a regex on a level:</p> <pre><code>&gt;&gt;&gt; df.select_columns({\"animal\":re.compile(\".+t$\")})\nclass      mammal\nanimal        cat   bat\nlocomotion  walks flies\nnum_legs        4     2\nnum_wings       0     2\n</code></pre> <p>Selection with a callable on a level:</p> <pre><code>&gt;&gt;&gt; df.select_columns({\"animal\":lambda f: f.str.endswith('t')})\nclass      mammal\nanimal        cat   bat\nlocomotion  walks flies\nnum_legs        4     2\nnum_wings       0     2\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>*args</code> <code>Any</code> <p>Valid inputs include: an exact column name to look for, a shell-style glob string (e.g. <code>*_thing_*</code>), a regular expression, a callable, or variable arguments of all the aforementioned. A sequence of booleans is also acceptable. A dictionary can be used for selection on a MultiIndex on different levels.</p> <code>()</code> <code>invert</code> <code>bool</code> <p>Whether or not to invert the selection. This will result in the selection of the complement of the columns provided.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with the specified columns selected.</p> Source code in <code>janitor/functions/select.py</code> <pre><code>@pf.register_dataframe_method\n@refactored_function(\n    message=(\n        \"This function will be deprecated in a 1.x release. \"\n        \"Please use `jn.select` instead.\"\n    )\n)\ndef select_columns(\n    df: pd.DataFrame,\n    *args: Any,\n    invert: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Method-chainable selection of columns.\n\n    It accepts a string, shell-like glob strings `(*string*)`,\n    regex, slice, array-like object, or a list of the previous options.\n\n    Selection on a MultiIndex on a level, or multiple levels,\n    is possible with a dictionary.\n\n    This method does not mutate the original DataFrame.\n\n    Optional ability to invert selection of columns available as well.\n\n    !!!note\n\n        The preferred option when selecting columns or rows in a Pandas DataFrame\n        is with `.loc` or `.iloc` methods.\n        `select_columns` is primarily for convenience.\n\n    !!!note\n\n        This function will be deprecated in a 1.x release.\n        Please use `jn.select` instead.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; from numpy import nan\n        &gt;&gt;&gt; pd.set_option(\"display.max_columns\", None)\n        &gt;&gt;&gt; pd.set_option(\"display.expand_frame_repr\", False)\n        &gt;&gt;&gt; pd.set_option(\"max_colwidth\", None)\n        &gt;&gt;&gt; data = {'name': ['Cheetah','Owl monkey','Mountain beaver',\n        ...                  'Greater short-tailed shrew','Cow'],\n        ...         'genus': ['Acinonyx', 'Aotus', 'Aplodontia', 'Blarina', 'Bos'],\n        ...         'vore': ['carni', 'omni', 'herbi', 'omni', 'herbi'],\n        ...         'order': ['Carnivora','Primates','Rodentia','Soricomorpha','Artiodactyla'],\n        ...         'conservation': ['lc', nan, 'nt', 'lc', 'domesticated'],\n        ...         'sleep_total': [12.1, 17.0, 14.4, 14.9, 4.0],\n        ...         'sleep_rem': [nan, 1.8, 2.4, 2.3, 0.7],\n        ...         'sleep_cycle': [nan, nan, nan, 0.133333333, 0.666666667],\n        ...         'awake': [11.9, 7.0, 9.6, 9.1, 20.0],\n        ...         'brainwt': [nan, 0.0155, nan, 0.00029, 0.423],\n        ...         'bodywt': [50.0, 0.48, 1.35, 0.019, 600.0]}\n        &gt;&gt;&gt; df = pd.DataFrame(data)\n        &gt;&gt;&gt; df\n                                 name       genus   vore         order  conservation  sleep_total  sleep_rem  sleep_cycle  awake  brainwt   bodywt\n        0                     Cheetah    Acinonyx  carni     Carnivora            lc         12.1        NaN          NaN   11.9      NaN   50.000\n        1                  Owl monkey       Aotus   omni      Primates           NaN         17.0        1.8          NaN    7.0  0.01550    0.480\n        2             Mountain beaver  Aplodontia  herbi      Rodentia            nt         14.4        2.4          NaN    9.6      NaN    1.350\n        3  Greater short-tailed shrew     Blarina   omni  Soricomorpha            lc         14.9        2.3     0.133333    9.1  0.00029    0.019\n        4                         Cow         Bos  herbi  Artiodactyla  domesticated          4.0        0.7     0.666667   20.0  0.42300  600.000\n\n        Explicit label selection:\n        &gt;&gt;&gt; df.select_columns('name', 'order')\n                                 name         order\n        0                     Cheetah     Carnivora\n        1                  Owl monkey      Primates\n        2             Mountain beaver      Rodentia\n        3  Greater short-tailed shrew  Soricomorpha\n        4                         Cow  Artiodactyla\n\n        Selection via globbing:\n        &gt;&gt;&gt; df.select_columns(\"sleep*\", \"*wt\")\n           sleep_total  sleep_rem  sleep_cycle  brainwt   bodywt\n        0         12.1        NaN          NaN      NaN   50.000\n        1         17.0        1.8          NaN  0.01550    0.480\n        2         14.4        2.4          NaN      NaN    1.350\n        3         14.9        2.3     0.133333  0.00029    0.019\n        4          4.0        0.7     0.666667  0.42300  600.000\n\n        Selection via regex:\n        &gt;&gt;&gt; import re\n        &gt;&gt;&gt; df.select_columns(re.compile(r\"o.+er\"))\n                  order  conservation\n        0     Carnivora            lc\n        1      Primates           NaN\n        2      Rodentia            nt\n        3  Soricomorpha            lc\n        4  Artiodactyla  domesticated\n\n        Selection via slicing:\n        &gt;&gt;&gt; df.select_columns(slice('name','order'), slice('sleep_total','sleep_cycle'))\n                                 name       genus   vore         order  sleep_total  sleep_rem  sleep_cycle\n        0                     Cheetah    Acinonyx  carni     Carnivora         12.1        NaN          NaN\n        1                  Owl monkey       Aotus   omni      Primates         17.0        1.8          NaN\n        2             Mountain beaver  Aplodontia  herbi      Rodentia         14.4        2.4          NaN\n        3  Greater short-tailed shrew     Blarina   omni  Soricomorpha         14.9        2.3     0.133333\n        4                         Cow         Bos  herbi  Artiodactyla          4.0        0.7     0.666667\n\n        Selection via callable:\n        &gt;&gt;&gt; from pandas.api.types import is_numeric_dtype\n        &gt;&gt;&gt; df.select_columns(is_numeric_dtype)\n           sleep_total  sleep_rem  sleep_cycle  awake  brainwt   bodywt\n        0         12.1        NaN          NaN   11.9      NaN   50.000\n        1         17.0        1.8          NaN    7.0  0.01550    0.480\n        2         14.4        2.4          NaN    9.6      NaN    1.350\n        3         14.9        2.3     0.133333    9.1  0.00029    0.019\n        4          4.0        0.7     0.666667   20.0  0.42300  600.000\n        &gt;&gt;&gt; df.select_columns(lambda f: f.isna().any())\n           conservation  sleep_rem  sleep_cycle  brainwt\n        0            lc        NaN          NaN      NaN\n        1           NaN        1.8          NaN  0.01550\n        2            nt        2.4          NaN      NaN\n        3            lc        2.3     0.133333  0.00029\n        4  domesticated        0.7     0.666667  0.42300\n\n        Exclude columns with the `invert` parameter:\n        &gt;&gt;&gt; df.select_columns(is_numeric_dtype, invert=True)\n                                 name       genus   vore         order  conservation\n        0                     Cheetah    Acinonyx  carni     Carnivora            lc\n        1                  Owl monkey       Aotus   omni      Primates           NaN\n        2             Mountain beaver  Aplodontia  herbi      Rodentia            nt\n        3  Greater short-tailed shrew     Blarina   omni  Soricomorpha            lc\n        4                         Cow         Bos  herbi  Artiodactyla  domesticated\n\n        Exclude columns with the `DropLabel` class:\n        &gt;&gt;&gt; from janitor import DropLabel\n        &gt;&gt;&gt; df.select_columns(DropLabel(slice(\"name\", \"awake\")), \"conservation\")\n           brainwt   bodywt  conservation\n        0      NaN   50.000            lc\n        1  0.01550    0.480           NaN\n        2      NaN    1.350            nt\n        3  0.00029    0.019            lc\n        4  0.42300  600.000  domesticated\n\n        Selection on MultiIndex columns:\n        &gt;&gt;&gt; d = {'num_legs': [4, 4, 2, 2],\n        ...      'num_wings': [0, 0, 2, 2],\n        ...      'class': ['mammal', 'mammal', 'mammal', 'bird'],\n        ...      'animal': ['cat', 'dog', 'bat', 'penguin'],\n        ...      'locomotion': ['walks', 'walks', 'flies', 'walks']}\n        &gt;&gt;&gt; df = pd.DataFrame(data=d)\n        &gt;&gt;&gt; df = df.set_index(['class', 'animal', 'locomotion']).T\n        &gt;&gt;&gt; df\n        class      mammal                bird\n        animal        cat   dog   bat penguin\n        locomotion  walks walks flies   walks\n        num_legs        4     4     2       2\n        num_wings       0     0     2       2\n\n        Selection with a scalar:\n        &gt;&gt;&gt; df.select_columns('mammal')\n        class      mammal\n        animal        cat   dog   bat\n        locomotion  walks walks flies\n        num_legs        4     4     2\n        num_wings       0     0     2\n\n        Selection with a tuple:\n        &gt;&gt;&gt; df.select_columns(('mammal','bat'))\n        class      mammal\n        animal        bat\n        locomotion  flies\n        num_legs        2\n        num_wings       2\n\n        Selection within a level is possible with a dictionary,\n        where the key is either a level name or number:\n        &gt;&gt;&gt; df.select_columns({'animal':'cat'})\n        class      mammal\n        animal        cat\n        locomotion  walks\n        num_legs        4\n        num_wings       0\n        &gt;&gt;&gt; df.select_columns({1:[\"bat\", \"cat\"]})\n        class      mammal\n        animal        bat   cat\n        locomotion  flies walks\n        num_legs        2     4\n        num_wings       2     0\n\n        Selection on multiple levels:\n        &gt;&gt;&gt; df.select_columns({\"class\":\"mammal\", \"locomotion\":\"flies\"})\n        class      mammal\n        animal        bat\n        locomotion  flies\n        num_legs        2\n        num_wings       2\n\n        Selection with a regex on a level:\n        &gt;&gt;&gt; df.select_columns({\"animal\":re.compile(\".+t$\")})\n        class      mammal\n        animal        cat   bat\n        locomotion  walks flies\n        num_legs        4     2\n        num_wings       0     2\n\n        Selection with a callable on a level:\n        &gt;&gt;&gt; df.select_columns({\"animal\":lambda f: f.str.endswith('t')})\n        class      mammal\n        animal        cat   bat\n        locomotion  walks flies\n        num_legs        4     2\n        num_wings       0     2\n\n    Args:\n        df: A pandas DataFrame.\n        *args: Valid inputs include: an exact column name to look for,\n            a shell-style glob string (e.g. `*_thing_*`),\n            a regular expression,\n            a callable,\n            or variable arguments of all the aforementioned.\n            A sequence of booleans is also acceptable.\n            A dictionary can be used for selection\n            on a MultiIndex on different levels.\n        invert: Whether or not to invert the selection.\n            This will result in the selection\n            of the complement of the columns provided.\n\n    Returns:\n        A pandas DataFrame with the specified columns selected.\n    \"\"\"  # noqa: E501\n\n    return _select(df, columns=list(args), invert=invert)\n</code></pre>"},{"location":"api/functions/#janitor.functions.select.select_rows","title":"<code>select_rows(df, *args, invert=False)</code>","text":"<p>Method-chainable selection of rows.</p> <p>It accepts a string, shell-like glob strings <code>(*string*)</code>, regex, slice, array-like object, or a list of the previous options.</p> <p>Selection on a MultiIndex on a level, or multiple levels, is possible with a dictionary.</p> <p>This method does not mutate the original DataFrame.</p> <p>Optional ability to invert selection of rows available as well.</p> <p>New in version 0.24.0</p> <p>Note</p> <p>The preferred option when selecting columns or rows in a Pandas DataFrame is with <code>.loc</code> or <code>.iloc</code> methods, as they are generally performant. <code>select_rows</code> is primarily for convenience.</p> <p>Note</p> <p>This function will be deprecated in a 1.x release. Please use <code>jn.select</code> instead.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = {\"col1\": [1, 2], \"foo\": [3, 4], \"col2\": [5, 6]}\n&gt;&gt;&gt; df = pd.DataFrame.from_dict(df, orient='index')\n&gt;&gt;&gt; df\n      0  1\ncol1  1  2\nfoo   3  4\ncol2  5  6\n&gt;&gt;&gt; df.select_rows(\"col*\")\n      0  1\ncol1  1  2\ncol2  5  6\n</code></pre> <p>More examples can be found in the <code>select_columns</code> section.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>*args</code> <code>Any</code> <p>Valid inputs include: an exact index name to look for, a shell-style glob string (e.g. <code>*_thing_*</code>), a regular expression, a callable, or variable arguments of all the aforementioned. A sequence of booleans is also acceptable. A dictionary can be used for selection on a MultiIndex on different levels.</p> <code>()</code> <code>invert</code> <code>bool</code> <p>Whether or not to invert the selection. This will result in the selection of the complement of the rows provided.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with the specified rows selected.</p> Source code in <code>janitor/functions/select.py</code> <pre><code>@pf.register_dataframe_method\n@refactored_function(\n    message=(\n        \"This function will be deprecated in a 1.x release. \"\n        \"Please use `jn.select` instead.\"\n    )\n)\ndef select_rows(\n    df: pd.DataFrame,\n    *args: Any,\n    invert: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Method-chainable selection of rows.\n\n    It accepts a string, shell-like glob strings `(*string*)`,\n    regex, slice, array-like object, or a list of the previous options.\n\n    Selection on a MultiIndex on a level, or multiple levels,\n    is possible with a dictionary.\n\n    This method does not mutate the original DataFrame.\n\n    Optional ability to invert selection of rows available as well.\n\n\n    !!! info \"New in version 0.24.0\"\n\n    !!!note\n\n        The preferred option when selecting columns or rows in a Pandas DataFrame\n        is with `.loc` or `.iloc` methods, as they are generally performant.\n        `select_rows` is primarily for convenience.\n\n    !!!note\n\n        This function will be deprecated in a 1.x release.\n        Please use `jn.select` instead.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = {\"col1\": [1, 2], \"foo\": [3, 4], \"col2\": [5, 6]}\n        &gt;&gt;&gt; df = pd.DataFrame.from_dict(df, orient='index')\n        &gt;&gt;&gt; df\n              0  1\n        col1  1  2\n        foo   3  4\n        col2  5  6\n        &gt;&gt;&gt; df.select_rows(\"col*\")\n              0  1\n        col1  1  2\n        col2  5  6\n\n    More examples can be found in the\n    [`select_columns`][janitor.functions.select.select_columns] section.\n\n    Args:\n        df: A pandas DataFrame.\n        *args: Valid inputs include: an exact index name to look for,\n            a shell-style glob string (e.g. `*_thing_*`),\n            a regular expression,\n            a callable,\n            or variable arguments of all the aforementioned.\n            A sequence of booleans is also acceptable.\n            A dictionary can be used for selection\n            on a MultiIndex on different levels.\n        invert: Whether or not to invert the selection.\n            This will result in the selection\n            of the complement of the rows provided.\n\n    Returns:\n        A pandas DataFrame with the specified rows selected.\n    \"\"\"  # noqa: E501\n    return _select(df, rows=list(args), invert=invert)\n</code></pre>"},{"location":"api/functions/#janitor.functions.shuffle","title":"<code>shuffle</code>","text":"<p>Implementation of <code>shuffle</code> functions.</p>"},{"location":"api/functions/#janitor.functions.shuffle.shuffle","title":"<code>shuffle(df, random_state=None, reset_index=True)</code>","text":"<p>Shuffle the rows of the DataFrame.</p> <p>This method does not mutate the original DataFrame.</p> <p>Super-sugary syntax! Underneath the hood, we use <code>df.sample(frac=1)</code>, with the option to set the random state.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"col1\": range(5),\n...     \"col2\": list(\"abcde\"),\n... })\n&gt;&gt;&gt; df\n   col1 col2\n0     0    a\n1     1    b\n2     2    c\n3     3    d\n4     4    e\n&gt;&gt;&gt; df.shuffle(random_state=42)\n   col1 col2\n0     1    b\n1     4    e\n2     2    c\n3     0    a\n4     3    d\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>random_state</code> <code>Any</code> <p>If provided, set a seed for the random number generator. Passed to <code>pd.DataFrame.sample()</code>.</p> <code>None</code> <code>reset_index</code> <code>bool</code> <p>If True, reset the dataframe index to the default RangeIndex.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A shuffled pandas DataFrame.</p> Source code in <code>janitor/functions/shuffle.py</code> <pre><code>@pf.register_dataframe_method\ndef shuffle(\n    df: pd.DataFrame, random_state: Any = None, reset_index: bool = True\n) -&gt; pd.DataFrame:\n    \"\"\"Shuffle the rows of the DataFrame.\n\n    This method does not mutate the original DataFrame.\n\n    Super-sugary syntax! Underneath the hood, we use `df.sample(frac=1)`,\n    with the option to set the random state.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     \"col1\": range(5),\n        ...     \"col2\": list(\"abcde\"),\n        ... })\n        &gt;&gt;&gt; df\n           col1 col2\n        0     0    a\n        1     1    b\n        2     2    c\n        3     3    d\n        4     4    e\n        &gt;&gt;&gt; df.shuffle(random_state=42)\n           col1 col2\n        0     1    b\n        1     4    e\n        2     2    c\n        3     0    a\n        4     3    d\n\n    Args:\n        df: A pandas DataFrame.\n        random_state: If provided, set a seed for the random number\n            generator. Passed to `pd.DataFrame.sample()`.\n        reset_index: If True, reset the dataframe index to the default\n            RangeIndex.\n\n    Returns:\n        A shuffled pandas DataFrame.\n    \"\"\"\n    result = df.sample(frac=1, random_state=random_state)\n    if reset_index:\n        result = result.reset_index(drop=True)\n    return result\n</code></pre>"},{"location":"api/functions/#janitor.functions.sort_column_value_order","title":"<code>sort_column_value_order</code>","text":"<p>Implementation of the <code>sort_column_value_order</code> function.</p>"},{"location":"api/functions/#janitor.functions.sort_column_value_order.sort_column_value_order","title":"<code>sort_column_value_order(df, column, column_value_order, columns=None)</code>","text":"<p>This function adds precedence to certain values in a specified column, then sorts based on that column and any other specified columns.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; company_sales = {\n...     \"SalesMonth\": [\"Jan\", \"Feb\", \"Feb\", \"Mar\", \"April\"],\n...     \"Company1\": [150.0, 200.0, 200.0, 300.0, 400.0],\n...     \"Company2\": [180.0, 250.0, 250.0, np.nan, 500.0],\n...     \"Company3\": [400.0, 500.0, 500.0, 600.0, 675.0],\n... }\n&gt;&gt;&gt; df = pd.DataFrame.from_dict(company_sales)\n&gt;&gt;&gt; df\n  SalesMonth  Company1  Company2  Company3\n0        Jan     150.0     180.0     400.0\n1        Feb     200.0     250.0     500.0\n2        Feb     200.0     250.0     500.0\n3        Mar     300.0       NaN     600.0\n4      April     400.0     500.0     675.0\n&gt;&gt;&gt; df.sort_column_value_order(\n...     \"SalesMonth\",\n...     {\"April\": 1, \"Mar\": 2, \"Feb\": 3, \"Jan\": 4}\n... )\n  SalesMonth  Company1  Company2  Company3\n4      April     400.0     500.0     675.0\n3        Mar     300.0       NaN     600.0\n1        Feb     200.0     250.0     500.0\n2        Feb     200.0     250.0     500.0\n0        Jan     150.0     180.0     400.0\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>pandas DataFrame that we are manipulating</p> required <code>column</code> <code>str</code> <p>This is a column name as a string we are using to specify which column to sort by</p> required <code>column_value_order</code> <code>dict</code> <p>Dictionary of values that will represent precedence of the values in the specified column</p> required <code>columns</code> <code>str</code> <p>A list of additional columns that we can sort by</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If chosen Column Name is not in Dataframe, or if <code>column_value_order</code> dictionary is empty.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A sorted pandas DataFrame.</p> Source code in <code>janitor/functions/sort_column_value_order.py</code> <pre><code>@pf.register_dataframe_method\ndef sort_column_value_order(\n    df: pd.DataFrame,\n    column: str,\n    column_value_order: dict,\n    columns: str = None,\n) -&gt; pd.DataFrame:\n    \"\"\"This function adds precedence to certain values in a specified column,\n    then sorts based on that column and any other specified columns.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; company_sales = {\n        ...     \"SalesMonth\": [\"Jan\", \"Feb\", \"Feb\", \"Mar\", \"April\"],\n        ...     \"Company1\": [150.0, 200.0, 200.0, 300.0, 400.0],\n        ...     \"Company2\": [180.0, 250.0, 250.0, np.nan, 500.0],\n        ...     \"Company3\": [400.0, 500.0, 500.0, 600.0, 675.0],\n        ... }\n        &gt;&gt;&gt; df = pd.DataFrame.from_dict(company_sales)\n        &gt;&gt;&gt; df\n          SalesMonth  Company1  Company2  Company3\n        0        Jan     150.0     180.0     400.0\n        1        Feb     200.0     250.0     500.0\n        2        Feb     200.0     250.0     500.0\n        3        Mar     300.0       NaN     600.0\n        4      April     400.0     500.0     675.0\n        &gt;&gt;&gt; df.sort_column_value_order(\n        ...     \"SalesMonth\",\n        ...     {\"April\": 1, \"Mar\": 2, \"Feb\": 3, \"Jan\": 4}\n        ... )\n          SalesMonth  Company1  Company2  Company3\n        4      April     400.0     500.0     675.0\n        3        Mar     300.0       NaN     600.0\n        1        Feb     200.0     250.0     500.0\n        2        Feb     200.0     250.0     500.0\n        0        Jan     150.0     180.0     400.0\n\n    Args:\n        df: pandas DataFrame that we are manipulating\n        column: This is a column name as a string we are using to specify\n            which column to sort by\n        column_value_order: Dictionary of values that will\n            represent precedence of the values in the specified column\n        columns: A list of additional columns that we can sort by\n\n    Raises:\n        ValueError: If chosen Column Name is not in\n            Dataframe, or if `column_value_order` dictionary is empty.\n\n    Returns:\n        A sorted pandas DataFrame.\n    \"\"\"\n    # Validation checks\n    check_column(df, column, present=True)\n    check(\"column_value_order\", column_value_order, [dict])\n    if not column_value_order:\n        raise ValueError(\"column_value_order dictionary cannot be empty\")\n\n    df = df.assign(cond_order=df[column].map(column_value_order))\n\n    sort_by = [\"cond_order\"]\n    if columns is not None:\n        sort_by = [\"cond_order\"] + columns\n\n    df = df.sort_values(sort_by).remove_columns(\"cond_order\")\n    return df\n</code></pre>"},{"location":"api/functions/#janitor.functions.sort_naturally","title":"<code>sort_naturally</code>","text":"<p>Implementation of the <code>sort_naturally</code> function.</p>"},{"location":"api/functions/#janitor.functions.sort_naturally.sort_naturally","title":"<code>sort_naturally(df, column_name, **natsorted_kwargs)</code>","text":"<p>Sort a DataFrame by a column using natural sorting.</p> <p>Natural sorting is distinct from the default lexiographical sorting provided by <code>pandas</code>. For example, given the following list of items:</p> <pre><code>[\"A1\", \"A11\", \"A3\", \"A2\", \"A10\"]\n</code></pre> <p>Lexicographical sorting would give us:</p> <pre><code>[\"A1\", \"A10\", \"A11\", \"A2\", \"A3\"]\n</code></pre> <p>By contrast, \"natural\" sorting would give us:</p> <pre><code>[\"A1\", \"A2\", \"A3\", \"A10\", \"A11\"]\n</code></pre> <p>This function thus provides natural sorting on a single column of a dataframe.</p> <p>To accomplish this, we do a natural sort on the unique values that are present in the dataframe. Then, we reconstitute the entire dataframe in the naturally sorted order.</p> <p>Natural sorting is provided by the Python package natsort.</p> <p>All keyword arguments to <code>natsort</code> should be provided after the column name to sort by is provided. They are passed through to the <code>natsorted</code> function.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame(\n...     {\n...         \"Well\": [\"A21\", \"A3\", \"A21\", \"B2\", \"B51\", \"B12\"],\n...         \"Value\": [1, 2, 13, 3, 4, 7],\n...     }\n... )\n&gt;&gt;&gt; df\n  Well  Value\n0  A21      1\n1   A3      2\n2  A21     13\n3   B2      3\n4  B51      4\n5  B12      7\n&gt;&gt;&gt; df.sort_naturally(\"Well\")\n  Well  Value\n1   A3      2\n0  A21      1\n2  A21     13\n3   B2      3\n5  B12      7\n4  B51      4\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>column_name</code> <code>str</code> <p>The column on which natural sorting should take place.</p> required <code>**natsorted_kwargs</code> <code>Any</code> <p>Keyword arguments to be passed to natsort's <code>natsorted</code> function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A sorted pandas DataFrame.</p> Source code in <code>janitor/functions/sort_naturally.py</code> <pre><code>@pf.register_dataframe_method\ndef sort_naturally(\n    df: pd.DataFrame, column_name: str, **natsorted_kwargs: Any\n) -&gt; pd.DataFrame:\n    \"\"\"Sort a DataFrame by a column using *natural* sorting.\n\n    Natural sorting is distinct from\n    the default lexiographical sorting provided by `pandas`.\n    For example, given the following list of items:\n\n    ```python\n    [\"A1\", \"A11\", \"A3\", \"A2\", \"A10\"]\n    ```\n\n    Lexicographical sorting would give us:\n\n    ```python\n    [\"A1\", \"A10\", \"A11\", \"A2\", \"A3\"]\n    ```\n\n    By contrast, \"natural\" sorting would give us:\n\n    ```python\n    [\"A1\", \"A2\", \"A3\", \"A10\", \"A11\"]\n    ```\n\n    This function thus provides *natural* sorting\n    on a single column of a dataframe.\n\n    To accomplish this, we do a natural sort\n    on the unique values that are present in the dataframe.\n    Then, we reconstitute the entire dataframe\n    in the naturally sorted order.\n\n    Natural sorting is provided by the Python package\n    [natsort](https://natsort.readthedocs.io/en/master/index.html).\n\n    All keyword arguments to `natsort` should be provided\n    after the column name to sort by is provided.\n    They are passed through to the `natsorted` function.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame(\n        ...     {\n        ...         \"Well\": [\"A21\", \"A3\", \"A21\", \"B2\", \"B51\", \"B12\"],\n        ...         \"Value\": [1, 2, 13, 3, 4, 7],\n        ...     }\n        ... )\n        &gt;&gt;&gt; df\n          Well  Value\n        0  A21      1\n        1   A3      2\n        2  A21     13\n        3   B2      3\n        4  B51      4\n        5  B12      7\n        &gt;&gt;&gt; df.sort_naturally(\"Well\")\n          Well  Value\n        1   A3      2\n        0  A21      1\n        2  A21     13\n        3   B2      3\n        5  B12      7\n        4  B51      4\n\n    Args:\n        df: A pandas DataFrame.\n        column_name: The column on which natural sorting should take place.\n        **natsorted_kwargs: Keyword arguments to be passed\n            to natsort's `natsorted` function.\n\n    Returns:\n        A sorted pandas DataFrame.\n    \"\"\"\n    new_order = index_natsorted(df[column_name], **natsorted_kwargs)\n    return df.iloc[new_order, :]\n</code></pre>"},{"location":"api/functions/#janitor.functions.take_first","title":"<code>take_first</code>","text":"<p>Implementation of take_first function.</p>"},{"location":"api/functions/#janitor.functions.take_first.take_first","title":"<code>take_first(df, subset, by, ascending=True)</code>","text":"<p>Take the first row within each group specified by <code>subset</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\"a\": [\"x\", \"x\", \"y\", \"y\"], \"b\": [0, 1, 2, 3]})\n&gt;&gt;&gt; df\n   a  b\n0  x  0\n1  x  1\n2  y  2\n3  y  3\n&gt;&gt;&gt; df.take_first(subset=\"a\", by=\"b\")\n   a  b\n0  x  0\n2  y  2\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>subset</code> <code>Union[Hashable, Iterable[Hashable]]</code> <p>Column(s) defining the group.</p> required <code>by</code> <code>Hashable</code> <p>Column to sort by.</p> required <code>ascending</code> <code>bool</code> <p>Whether or not to sort in ascending order, <code>bool</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame.</p> Source code in <code>janitor/functions/take_first.py</code> <pre><code>@pf.register_dataframe_method\ndef take_first(\n    df: pd.DataFrame,\n    subset: Union[Hashable, Iterable[Hashable]],\n    by: Hashable,\n    ascending: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"Take the first row within each group specified by `subset`.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\"a\": [\"x\", \"x\", \"y\", \"y\"], \"b\": [0, 1, 2, 3]})\n        &gt;&gt;&gt; df\n           a  b\n        0  x  0\n        1  x  1\n        2  y  2\n        3  y  3\n        &gt;&gt;&gt; df.take_first(subset=\"a\", by=\"b\")\n           a  b\n        0  x  0\n        2  y  2\n\n    Args:\n        df: A pandas DataFrame.\n        subset: Column(s) defining the group.\n        by: Column to sort by.\n        ascending: Whether or not to sort in ascending order, `bool`.\n\n    Returns:\n        A pandas DataFrame.\n    \"\"\"\n    result = df.sort_values(by=by, ascending=ascending).drop_duplicates(\n        subset=subset, keep=\"first\"\n    )\n\n    return result\n</code></pre>"},{"location":"api/functions/#janitor.functions.then","title":"<code>then</code>","text":"<p>Implementation source for <code>then</code>.</p>"},{"location":"api/functions/#janitor.functions.then.then","title":"<code>then(df, func)</code>","text":"<p>Add an arbitrary function to run in the <code>pyjanitor</code> method chain.</p> <p>This method does not mutate the original DataFrame.</p> <p>Note</p> <p>This function will be deprecated in a 1.x release. Please use <code>pd.DataFrame.pipe</code> instead.</p> <p>Examples:</p> <p>A trivial example using a lambda <code>func</code>.</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; (pd.DataFrame({\"a\": [1, 2, 3], \"b\": [7, 8, 9]})\n...  .then(lambda df: df * 2))\n   a   b\n0  2  14\n1  4  16\n2  6  18\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>func</code> <code>Callable</code> <p>A function you would like to run in the method chain. It should take one parameter and return one parameter, each being the DataFrame object. After that, do whatever you want in the middle. Go crazy.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame.</p> Source code in <code>janitor/functions/then.py</code> <pre><code>@pf.register_dataframe_method\n@refactored_function(\n    message=\"This function will be deprecated in a 1.x release. \"\n    \"Kindly use `pd.DataFrame.pipe` instead.\"\n)\ndef then(df: pd.DataFrame, func: Callable) -&gt; pd.DataFrame:\n    \"\"\"Add an arbitrary function to run in the `pyjanitor` method chain.\n\n    This method does not mutate the original DataFrame.\n\n    !!!note\n\n        This function will be deprecated in a 1.x release.\n        Please use `pd.DataFrame.pipe` instead.\n\n    Examples:\n        A trivial example using a lambda `func`.\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; (pd.DataFrame({\"a\": [1, 2, 3], \"b\": [7, 8, 9]})\n        ...  .then(lambda df: df * 2))\n           a   b\n        0  2  14\n        1  4  16\n        2  6  18\n\n    Args:\n        df: A pandas DataFrame.\n        func: A function you would like to run in the method chain.\n            It should take one parameter and return one parameter, each being\n            the DataFrame object. After that, do whatever you want in the\n            middle. Go crazy.\n\n    Returns:\n        A pandas DataFrame.\n    \"\"\"\n    df = func(df)\n    return df\n</code></pre>"},{"location":"api/functions/#janitor.functions.to_datetime","title":"<code>to_datetime</code>","text":"<p>Implementation source for <code>to_datetime</code>.</p>"},{"location":"api/functions/#janitor.functions.to_datetime.to_datetime","title":"<code>to_datetime(df, column_name, **kwargs)</code>","text":"<p>Convert column to a datetime type, in-place.</p> <p>Intended to be the method-chaining equivalent of:</p> <pre><code>df[column_name] = pd.to_datetime(df[column_name], **kwargs)\n</code></pre> <p>This method mutates the original DataFrame.</p> <p>Note</p> <p>This function will be deprecated in a 1.x release. Please use <code>jn.transform_column</code> instead.</p> <p>Examples:</p> <p>Converting a string column to datetime type with custom format.</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({'date': ['20200101', '20200202', '20200303']})\n&gt;&gt;&gt; df\n       date\n0  20200101\n1  20200202\n2  20200303\n&gt;&gt;&gt; df.to_datetime('date', format='%Y%m%d')\n        date\n0 2020-01-01\n1 2020-02-02\n2 2020-03-03\n</code></pre> <p>Read the pandas documentation for <code>to_datetime</code> for more information.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>column_name</code> <code>Hashable</code> <p>Column name.</p> required <code>**kwargs</code> <code>Any</code> <p>Provide any kwargs that <code>pd.to_datetime</code> can take.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with updated datetime data.</p> Source code in <code>janitor/functions/to_datetime.py</code> <pre><code>@pf.register_dataframe_method\n@deprecated_alias(column=\"column_name\")\n@refactored_function(\n    message=(\n        \"This function will be deprecated in a 1.x release. \"\n        \"Please use `jn.transform_columns` instead.\"\n    )\n)\ndef to_datetime(\n    df: pd.DataFrame, column_name: Hashable, **kwargs: Any\n) -&gt; pd.DataFrame:\n    \"\"\"Convert column to a datetime type, in-place.\n\n    Intended to be the method-chaining equivalent of:\n\n    ```python\n    df[column_name] = pd.to_datetime(df[column_name], **kwargs)\n    ```\n\n    This method mutates the original DataFrame.\n\n    !!!note\n\n        This function will be deprecated in a 1.x release.\n        Please use [`jn.transform_column`][janitor.functions.transform_columns.transform_column]\n        instead.\n\n    Examples:\n        Converting a string column to datetime type with custom format.\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({'date': ['20200101', '20200202', '20200303']})\n        &gt;&gt;&gt; df\n               date\n        0  20200101\n        1  20200202\n        2  20200303\n        &gt;&gt;&gt; df.to_datetime('date', format='%Y%m%d')\n                date\n        0 2020-01-01\n        1 2020-02-02\n        2 2020-03-03\n\n    Read the pandas documentation for [`to_datetime`][pd_docs] for more information.\n\n    [pd_docs]: https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html\n\n    Args:\n        df: A pandas DataFrame.\n        column_name: Column name.\n        **kwargs: Provide any kwargs that `pd.to_datetime` can take.\n\n    Returns:\n        A pandas DataFrame with updated datetime data.\n    \"\"\"  # noqa: E501\n    df[column_name] = pd.to_datetime(df[column_name], **kwargs)\n\n    return df\n</code></pre>"},{"location":"api/functions/#janitor.functions.toset","title":"<code>toset</code>","text":"<p>Implementation of the <code>toset</code> function.</p>"},{"location":"api/functions/#janitor.functions.toset.toset","title":"<code>toset(series)</code>","text":"<p>Return a set of the values.</p> <p>Note</p> <p>This function will be deprecated in a 1.x release. Please use <code>set(df[column])</code> instead.</p> <p>These are each a scalar type, which is a Python scalar (for str, int, float) or a pandas scalar (for Timestamp/Timedelta/Interval/Period)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; s = pd.Series([1, 2, 3, 5, 5], index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n&gt;&gt;&gt; s\na    1\nb    2\nc    3\nd    5\ne    5\ndtype: int64\n&gt;&gt;&gt; s.toset()\n{1, 2, 3, 5}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>series</code> <code>Series</code> <p>A pandas series.</p> required <p>Returns:</p> Type Description <code>Set</code> <p>A set of values.</p> Source code in <code>janitor/functions/toset.py</code> <pre><code>@pf.register_series_method\n@refactored_function(\n    message=(\n        \"This function will be deprecated in a 1.x release. \"\n        \"Please use `set(df[column])` instead.\"\n    )\n)\ndef toset(series: pd.Series) -&gt; Set:\n    \"\"\"Return a set of the values.\n\n    !!!note\n\n        This function will be deprecated in a 1.x release.\n        Please use `set(df[column])` instead.\n\n    These are each a scalar type, which is a Python scalar\n    (for str, int, float) or a pandas scalar\n    (for Timestamp/Timedelta/Interval/Period)\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; s = pd.Series([1, 2, 3, 5, 5], index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\n        &gt;&gt;&gt; s\n        a    1\n        b    2\n        c    3\n        d    5\n        e    5\n        dtype: int64\n        &gt;&gt;&gt; s.toset()\n        {1, 2, 3, 5}\n\n    Args:\n        series: A pandas series.\n\n    Returns:\n        A set of values.\n    \"\"\"\n\n    return set(series.tolist())\n</code></pre>"},{"location":"api/functions/#janitor.functions.transform_columns","title":"<code>transform_columns</code>","text":""},{"location":"api/functions/#janitor.functions.transform_columns.transform_column","title":"<code>transform_column(df, column_name, function, dest_column_name=None, elementwise=True)</code>","text":"<p>Transform the given column using the provided function.</p> <p>Meant to be the method-chaining equivalent of: <pre><code>df[dest_column_name] = df[column_name].apply(function)\n</code></pre></p> <p>Functions can be applied in one of two ways:</p> <ul> <li>Element-wise (default; <code>elementwise=True</code>). Then, the individual column elements will be passed in as the first argument of <code>function</code>.</li> <li>Column-wise (<code>elementwise=False</code>). Then, <code>function</code> is expected to take in a pandas Series and return a sequence that is of identical length to the original.</li> </ul> <p>If <code>dest_column_name</code> is provided, then the transformation result is stored in that column. Otherwise, the transformed result is stored under the name of the original column.</p> <p>This method does not mutate the original DataFrame.</p> <p>Examples:</p> <p>Transform a column in-place with an element-wise function.</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"a\": [2, 3, 4],\n...     \"b\": [\"area\", \"pyjanitor\", \"grapefruit\"],\n... })\n&gt;&gt;&gt; df\n   a           b\n0  2        area\n1  3   pyjanitor\n2  4  grapefruit\n&gt;&gt;&gt; df.transform_column(\n...     column_name=\"a\",\n...     function=lambda x: x**2 - 1,\n... )\n    a           b\n0   3        area\n1   8   pyjanitor\n2  15  grapefruit\n</code></pre> <p>Examples:</p> <p>Transform a column in-place with an column-wise function.</p> <pre><code>&gt;&gt;&gt; df.transform_column(\n...     column_name=\"b\",\n...     function=lambda srs: srs.str[:5],\n...     elementwise=False,\n... )\n   a      b\n0  2   area\n1  3  pyjan\n2  4  grape\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>column_name</code> <code>Hashable</code> <p>The column to transform.</p> required <code>function</code> <code>Callable</code> <p>A function to apply on the column.</p> required <code>dest_column_name</code> <code>Optional[str]</code> <p>The column name to store the transformation result in. Defaults to None, which will result in the original column name being overwritten. If a name is provided here, then a new column with the transformed values will be created.</p> <code>None</code> <code>elementwise</code> <code>bool</code> <p>Whether to apply the function elementwise or not. If <code>elementwise</code> is True, then the function's first argument should be the data type of each datum in the column of data, and should return a transformed datum. If <code>elementwise</code> is False, then the function's should expect a pandas Series passed into it, and return a pandas Series.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with a transformed column.</p> Source code in <code>janitor/functions/transform_columns.py</code> <pre><code>@pf.register_dataframe_method\n@deprecated_alias(col_name=\"column_name\", dest_col_name=\"dest_column_name\")\ndef transform_column(\n    df: pd.DataFrame,\n    column_name: Hashable,\n    function: Callable,\n    dest_column_name: Optional[str] = None,\n    elementwise: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"Transform the given column using the provided function.\n\n    Meant to be the method-chaining equivalent of:\n    ```python\n    df[dest_column_name] = df[column_name].apply(function)\n    ```\n\n    Functions can be applied in one of two ways:\n\n    - **Element-wise** (default; `elementwise=True`). Then, the individual\n    column elements will be passed in as the first argument of `function`.\n    - **Column-wise** (`elementwise=False`). Then, `function` is expected to\n    take in a pandas Series and return a sequence that is of identical length\n    to the original.\n\n    If `dest_column_name` is provided, then the transformation result is stored\n    in that column. Otherwise, the transformed result is stored under the name\n    of the original column.\n\n    This method does not mutate the original DataFrame.\n\n    Examples:\n        Transform a column in-place with an element-wise function.\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     \"a\": [2, 3, 4],\n        ...     \"b\": [\"area\", \"pyjanitor\", \"grapefruit\"],\n        ... })\n        &gt;&gt;&gt; df\n           a           b\n        0  2        area\n        1  3   pyjanitor\n        2  4  grapefruit\n        &gt;&gt;&gt; df.transform_column(\n        ...     column_name=\"a\",\n        ...     function=lambda x: x**2 - 1,\n        ... )\n            a           b\n        0   3        area\n        1   8   pyjanitor\n        2  15  grapefruit\n\n    Examples:\n        Transform a column in-place with an column-wise function.\n\n        &gt;&gt;&gt; df.transform_column(\n        ...     column_name=\"b\",\n        ...     function=lambda srs: srs.str[:5],\n        ...     elementwise=False,\n        ... )\n           a      b\n        0  2   area\n        1  3  pyjan\n        2  4  grape\n\n    Args:\n        df: A pandas DataFrame.\n        column_name: The column to transform.\n        function: A function to apply on the column.\n        dest_column_name: The column name to store the transformation result\n            in. Defaults to None, which will result in the original column\n            name being overwritten. If a name is provided here, then a new\n            column with the transformed values will be created.\n        elementwise: Whether to apply the function elementwise or not.\n            If `elementwise` is True, then the function's first argument\n            should be the data type of each datum in the column of data,\n            and should return a transformed datum.\n            If `elementwise` is False, then the function's should expect\n            a pandas Series passed into it, and return a pandas Series.\n\n    Returns:\n        A pandas DataFrame with a transformed column.\n    \"\"\"\n    check_column(df, column_name)\n\n    if dest_column_name is None:\n        dest_column_name = column_name\n    elif dest_column_name != column_name:\n        # If `dest_column_name` is provided and equals `column_name`, then we\n        # assume that the user's intent is to perform an in-place\n        # transformation (Same behaviour as when `dest_column_name` = None).\n        # Otherwise we throw an error if `dest_column_name` already exists in\n        # df.\n        check_column(df, dest_column_name, present=False)\n\n    result = _get_transform_column_result(\n        df[column_name],\n        function,\n        elementwise,\n    )\n\n    return df.assign(**{dest_column_name: result})\n</code></pre>"},{"location":"api/functions/#janitor.functions.transform_columns.transform_columns","title":"<code>transform_columns(df, column_names, function, suffix=None, elementwise=True, new_column_names=None)</code>","text":"<p>Transform multiple columns through the same transformation.</p> <p>This method does not mutate the original DataFrame.</p> <p>Super syntactic sugar! Essentially wraps <code>transform_column</code> and calls it repeatedly over all column names provided.</p> <p>User can optionally supply either a suffix to create a new set of columns with the specified suffix, or provide a dictionary mapping each original column name in <code>column_names</code> to its corresponding new column name. Note that all column names must be strings.</p> <p>Examples:</p> <p>log10 transform a list of columns, replacing original columns.</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"col1\": [5, 10, 15],\n...     \"col2\": [3, 6, 9],\n...     \"col3\": [10, 100, 1_000],\n... })\n&gt;&gt;&gt; df\n   col1  col2  col3\n0     5     3    10\n1    10     6   100\n2    15     9  1000\n&gt;&gt;&gt; df.transform_columns([\"col1\", \"col2\", \"col3\"], np.log10)\n       col1      col2  col3\n0  0.698970  0.477121   1.0\n1  1.000000  0.778151   2.0\n2  1.176091  0.954243   3.0\n</code></pre> <p>Using the <code>suffix</code> parameter to create new columns.</p> <pre><code>&gt;&gt;&gt; df.transform_columns([\"col1\", \"col3\"], np.log10, suffix=\"_log\")\n   col1  col2  col3  col1_log  col3_log\n0     5     3    10  0.698970       1.0\n1    10     6   100  1.000000       2.0\n2    15     9  1000  1.176091       3.0\n</code></pre> <p>Using the <code>new_column_names</code> parameter to create new columns.</p> <pre><code>&gt;&gt;&gt; df.transform_columns(\n...     [\"col1\", \"col3\"],\n...     np.log10,\n...     new_column_names={\"col1\": \"transform1\"},\n... )\n   col1  col2  col3  transform1\n0     5     3   1.0    0.698970\n1    10     6   2.0    1.000000\n2    15     9   3.0    1.176091\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame.</p> required <code>column_names</code> <code>Union[List[str], Tuple[str]]</code> <p>An iterable of columns to transform.</p> required <code>function</code> <code>Callable</code> <p>A function to apply on each column.</p> required <code>suffix</code> <code>Optional[str]</code> <p>Suffix to use when creating new columns to hold the transformed values.</p> <code>None</code> <code>elementwise</code> <code>bool</code> <p>Passed on to <code>transform_column</code>; whether or not to apply the transformation function elementwise (True) or columnwise (False).</p> <code>True</code> <code>new_column_names</code> <code>Optional[Dict[str, str]]</code> <p>An explicit mapping of old column names in <code>column_names</code> to new column names. If any column specified in <code>column_names</code> is not a key in this dictionary, the transformation will happen in-place for that column.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both <code>suffix</code> and <code>new_column_names</code> are specified.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with transformed columns.</p> Source code in <code>janitor/functions/transform_columns.py</code> <pre><code>@pf.register_dataframe_method\n@deprecated_alias(columns=\"column_names\", new_names=\"new_column_names\")\ndef transform_columns(\n    df: pd.DataFrame,\n    column_names: Union[List[str], Tuple[str]],\n    function: Callable,\n    suffix: Optional[str] = None,\n    elementwise: bool = True,\n    new_column_names: Optional[Dict[str, str]] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Transform multiple columns through the same transformation.\n\n    This method does not mutate the original DataFrame.\n\n    Super syntactic sugar!\n    Essentially wraps [`transform_column`][janitor.functions.transform_columns.transform_column]\n    and calls it repeatedly over all column names provided.\n\n    User can optionally supply either a suffix to create a new set of columns\n    with the specified suffix, or provide a dictionary mapping each original\n    column name in `column_names` to its corresponding new column name.\n    Note that all column names must be strings.\n\n    Examples:\n        log10 transform a list of columns, replacing original columns.\n\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     \"col1\": [5, 10, 15],\n        ...     \"col2\": [3, 6, 9],\n        ...     \"col3\": [10, 100, 1_000],\n        ... })\n        &gt;&gt;&gt; df\n           col1  col2  col3\n        0     5     3    10\n        1    10     6   100\n        2    15     9  1000\n        &gt;&gt;&gt; df.transform_columns([\"col1\", \"col2\", \"col3\"], np.log10)\n               col1      col2  col3\n        0  0.698970  0.477121   1.0\n        1  1.000000  0.778151   2.0\n        2  1.176091  0.954243   3.0\n\n        Using the `suffix` parameter to create new columns.\n\n        &gt;&gt;&gt; df.transform_columns([\"col1\", \"col3\"], np.log10, suffix=\"_log\")\n           col1  col2  col3  col1_log  col3_log\n        0     5     3    10  0.698970       1.0\n        1    10     6   100  1.000000       2.0\n        2    15     9  1000  1.176091       3.0\n\n        Using the `new_column_names` parameter to create new columns.\n\n        &gt;&gt;&gt; df.transform_columns(\n        ...     [\"col1\", \"col3\"],\n        ...     np.log10,\n        ...     new_column_names={\"col1\": \"transform1\"},\n        ... )\n           col1  col2  col3  transform1\n        0     5     3   1.0    0.698970\n        1    10     6   2.0    1.000000\n        2    15     9   3.0    1.176091\n\n    Args:\n        df: A pandas DataFrame.\n        column_names: An iterable of columns to transform.\n        function: A function to apply on each column.\n        suffix: Suffix to use when creating new columns to hold\n            the transformed values.\n        elementwise: Passed on to [`transform_column`][janitor.functions.transform_columns.transform_column]; whether or not\n            to apply the transformation function elementwise (True)\n            or columnwise (False).\n        new_column_names: An explicit mapping of old column names in\n            `column_names` to new column names. If any column specified in\n            `column_names` is not a key in this dictionary, the transformation\n            will happen in-place for that column.\n\n    Raises:\n        ValueError: If both `suffix` and `new_column_names` are specified.\n\n    Returns:\n        A pandas DataFrame with transformed columns.\n    \"\"\"  # noqa: E501\n    check(\"column_names\", column_names, [list, tuple])\n    check_column(df, column_names)\n\n    if suffix is not None and new_column_names is not None:\n        raise ValueError(\n            \"Only one of `suffix` or `new_column_names` should be specified.\"\n        )\n\n    if suffix:\n        check(\"suffix\", suffix, [str])\n        dest_column_names = {col: col + suffix for col in column_names}\n    elif new_column_names:\n        check(\"new_column_names\", new_column_names, [dict])\n        dest_column_names = {\n            col: new_column_names.get(col, col) for col in column_names\n        }\n    else:\n        dest_column_names = dict(zip(column_names, column_names))\n\n    results = {}\n    for old_col, new_col in dest_column_names.items():\n        if old_col != new_col:\n            check_column(df, new_col, present=False)\n        results[new_col] = _get_transform_column_result(\n            df[old_col],\n            function,\n            elementwise=elementwise,\n        )\n\n    return df.assign(**results)\n</code></pre>"},{"location":"api/functions/#janitor.functions.truncate_datetime","title":"<code>truncate_datetime</code>","text":"<p>Implementation of the <code>truncate_datetime</code> family of functions.</p>"},{"location":"api/functions/#janitor.functions.truncate_datetime.truncate_datetime_dataframe","title":"<code>truncate_datetime_dataframe(df, datepart)</code>","text":"<p>Truncate times down to a user-specified precision of year, month, day, hour, minute, or second.</p> <p>This method does not mutate the original DataFrame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"foo\": [\"xxxx\", \"yyyy\", \"zzzz\"],\n...     \"dt\": pd.date_range(\"2020-03-11\", periods=3, freq=\"15H\"),\n... })\n&gt;&gt;&gt; df\n    foo                  dt\n0  xxxx 2020-03-11 00:00:00\n1  yyyy 2020-03-11 15:00:00\n2  zzzz 2020-03-12 06:00:00\n&gt;&gt;&gt; df.truncate_datetime_dataframe(\"day\")\n    foo         dt\n0  xxxx 2020-03-11\n1  yyyy 2020-03-11\n2  zzzz 2020-03-12\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The pandas DataFrame on which to truncate datetime.</p> required <code>datepart</code> <code>str</code> <p>Truncation precision, YEAR, MONTH, DAY, HOUR, MINUTE, SECOND. (String is automagically capitalized)</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid <code>datepart</code> precision is passed in.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame with all valid datetimes truncated down to the specified precision.</p> Source code in <code>janitor/functions/truncate_datetime.py</code> <pre><code>@pf.register_dataframe_method\ndef truncate_datetime_dataframe(\n    df: pd.DataFrame,\n    datepart: str,\n) -&gt; pd.DataFrame:\n    \"\"\"Truncate times down to a user-specified precision of\n    year, month, day, hour, minute, or second.\n\n    This method does not mutate the original DataFrame.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     \"foo\": [\"xxxx\", \"yyyy\", \"zzzz\"],\n        ...     \"dt\": pd.date_range(\"2020-03-11\", periods=3, freq=\"15H\"),\n        ... })\n        &gt;&gt;&gt; df\n            foo                  dt\n        0  xxxx 2020-03-11 00:00:00\n        1  yyyy 2020-03-11 15:00:00\n        2  zzzz 2020-03-12 06:00:00\n        &gt;&gt;&gt; df.truncate_datetime_dataframe(\"day\")\n            foo         dt\n        0  xxxx 2020-03-11\n        1  yyyy 2020-03-11\n        2  zzzz 2020-03-12\n\n    Args:\n        df: The pandas DataFrame on which to truncate datetime.\n        datepart: Truncation precision, YEAR, MONTH, DAY,\n            HOUR, MINUTE, SECOND. (String is automagically\n            capitalized)\n\n    Raises:\n        ValueError: If an invalid `datepart` precision is passed in.\n\n    Returns:\n        A pandas DataFrame with all valid datetimes truncated down\n            to the specified precision.\n    \"\"\"\n    # idea from Stack Overflow\n    # https://stackoverflow.com/a/28783971/7175713\n    # https://numpy.org/doc/stable/reference/arrays.datetime.html\n    ACCEPTABLE_DATEPARTS = {\n        \"YEAR\": \"datetime64[Y]\",\n        \"MONTH\": \"datetime64[M]\",\n        \"DAY\": \"datetime64[D]\",\n        \"HOUR\": \"datetime64[h]\",\n        \"MINUTE\": \"datetime64[m]\",\n        \"SECOND\": \"datetime64[s]\",\n    }\n    datepart = datepart.upper()\n    if datepart not in ACCEPTABLE_DATEPARTS:\n        raise ValueError(\n            \"Received an invalid `datepart` precision. \"\n            f\"Please enter any one of {ACCEPTABLE_DATEPARTS}.\"\n        )\n\n    dictionary = {}\n\n    for label, series in df.items():\n        if is_datetime64_any_dtype(series):\n            dtype = ACCEPTABLE_DATEPARTS[datepart]\n            # TODO: add branch for pyarrow arrays\n            series = np.array(series._values, dtype=dtype)\n        dictionary[label] = series\n\n    return pd.DataFrame(dictionary)\n</code></pre>"},{"location":"api/functions/#janitor.functions.update_where","title":"<code>update_where</code>","text":"<p>Function for updating values based on other column values.</p>"},{"location":"api/functions/#janitor.functions.update_where.update_where","title":"<code>update_where(df, conditions, target_column_name, target_val)</code>","text":"<p>Add multiple conditions to update a column in the dataframe.</p> <p>This method does not mutate the original DataFrame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; data = {\n...    \"a\": [1, 2, 3, 4],\n...    \"b\": [5, 6, 7, 8],\n...    \"c\": [0, 0, 0, 0],\n... }\n&gt;&gt;&gt; df = pd.DataFrame(data)\n&gt;&gt;&gt; df\n   a  b  c\n0  1  5  0\n1  2  6  0\n2  3  7  0\n3  4  8  0\n&gt;&gt;&gt; df.update_where(\n...    conditions = (df.a &gt; 2) &amp; (df.b &lt; 8),\n...    target_column_name = 'c',\n...    target_val = 10\n... )\n   a  b   c\n0  1  5   0\n1  2  6   0\n2  3  7  10\n3  4  8   0\n&gt;&gt;&gt; df.update_where( # supports pandas *query* style string expressions\n...    conditions = \"a &gt; 2 and b &lt; 8\",\n...    target_column_name = 'c',\n...    target_val = 10\n... )\n   a  b   c\n0  1  5   0\n1  2  6   0\n2  3  7  10\n3  4  8   0\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The pandas DataFrame object.</p> required <code>conditions</code> <code>Any</code> <p>Conditions used to update a target column and target value.</p> required <code>target_column_name</code> <code>Hashable</code> <p>Column to be updated. If column does not exist in DataFrame, a new column will be created; note that entries that do not get set in the new column will be null.</p> required <code>target_val</code> <code>Any</code> <p>Value to be updated.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>conditions</code> does not return a boolean array-like data structure.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame.</p> Source code in <code>janitor/functions/update_where.py</code> <pre><code>@pf.register_dataframe_method\n@deprecated_alias(target_col=\"target_column_name\")\ndef update_where(\n    df: pd.DataFrame,\n    conditions: Any,\n    target_column_name: Hashable,\n    target_val: Any,\n) -&gt; pd.DataFrame:\n    \"\"\"Add multiple conditions to update a column in the dataframe.\n\n    This method does not mutate the original DataFrame.\n\n    Examples:\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; data = {\n        ...    \"a\": [1, 2, 3, 4],\n        ...    \"b\": [5, 6, 7, 8],\n        ...    \"c\": [0, 0, 0, 0],\n        ... }\n        &gt;&gt;&gt; df = pd.DataFrame(data)\n        &gt;&gt;&gt; df\n           a  b  c\n        0  1  5  0\n        1  2  6  0\n        2  3  7  0\n        3  4  8  0\n        &gt;&gt;&gt; df.update_where(\n        ...    conditions = (df.a &gt; 2) &amp; (df.b &lt; 8),\n        ...    target_column_name = 'c',\n        ...    target_val = 10\n        ... )\n           a  b   c\n        0  1  5   0\n        1  2  6   0\n        2  3  7  10\n        3  4  8   0\n        &gt;&gt;&gt; df.update_where( # supports pandas *query* style string expressions\n        ...    conditions = \"a &gt; 2 and b &lt; 8\",\n        ...    target_column_name = 'c',\n        ...    target_val = 10\n        ... )\n           a  b   c\n        0  1  5   0\n        1  2  6   0\n        2  3  7  10\n        3  4  8   0\n\n    Args:\n        df: The pandas DataFrame object.\n        conditions: Conditions used to update a target column\n            and target value.\n        target_column_name: Column to be updated. If column does not exist\n            in DataFrame, a new column will be created; note that entries\n            that do not get set in the new column will be null.\n        target_val: Value to be updated.\n\n    Raises:\n        ValueError: If `conditions` does not return a boolean array-like\n            data structure.\n\n    Returns:\n        A pandas DataFrame.\n    \"\"\"\n\n    df = df.copy()\n\n    # use query mode if a string expression is passed\n    if isinstance(conditions, str):\n        conditions = df.eval(conditions)\n\n    if not is_bool_dtype(conditions):\n        raise ValueError(\n            \"\"\"\n            Kindly ensure that `conditions` passed\n            evaluates to a Boolean dtype.\n            \"\"\"\n        )\n\n    df.loc[conditions, target_column_name] = target_val\n\n    return df\n</code></pre>"},{"location":"api/functions/#janitor.functions.utils","title":"<code>utils</code>","text":"<p>Utility functions for all of the functions submodule.</p>"},{"location":"api/functions/#janitor.functions.utils.patterns","title":"<code>patterns(regex_pattern)</code>","text":"<p>This function converts a string into a compiled regular expression.</p> <p>It can be used to select columns in the index or columns_names arguments of <code>pivot_longer</code> function.</p> <p>Warning</p> <p>This function is deprecated. Kindly use <code>re.compile</code> instead.</p> <p>Parameters:</p> Name Type Description Default <code>regex_pattern</code> <code>Union[str, Pattern]</code> <p>String to be converted to compiled regular expression.</p> required <p>Returns:</p> Type Description <code>Pattern</code> <p>A compile regular expression from provided <code>regex_pattern</code>.</p> Source code in <code>janitor/functions/utils.py</code> <pre><code>def patterns(regex_pattern: Union[str, Pattern]) -&gt; Pattern:\n    \"\"\"This function converts a string into a compiled regular expression.\n\n    It can be used to select columns in the index or columns_names\n    arguments of `pivot_longer` function.\n\n    !!!warning\n\n        This function is deprecated. Kindly use `re.compile` instead.\n\n    Args:\n        regex_pattern: String to be converted to compiled regular\n            expression.\n\n    Returns:\n        A compile regular expression from provided `regex_pattern`.\n    \"\"\"\n    warnings.warn(\n        \"This function is deprecated. Kindly use `re.compile` instead.\",\n        DeprecationWarning,\n        stacklevel=find_stack_level(),\n    )\n    check(\"regular expression\", regex_pattern, [str, Pattern])\n\n    return re.compile(regex_pattern)\n</code></pre>"},{"location":"api/functions/#janitor.functions.utils.unionize_dataframe_categories","title":"<code>unionize_dataframe_categories(*dataframes, column_names=None)</code>","text":"<p>Given a group of dataframes which contain some categorical columns, for each categorical column present, find all the possible categories across all the dataframes which have that column. Update each dataframes' corresponding column with a new categorical object that contains the original data but has labels for all the possible categories from all dataframes. This is useful when concatenating a list of dataframes which all have the same categorical columns into one dataframe.</p> <p>If, for a given categorical column, all input dataframes do not have at least one instance of all the possible categories, Pandas will change the output dtype of that column from <code>category</code> to <code>object</code>, losing out on dramatic speed gains you get from the former format.</p> <p>Examples:</p> <p>Usage example for concatenation of categorical column-containing dataframes:</p> <p>Instead of:</p> <pre><code>concatenated_df = pd.concat([df1, df2, df3], ignore_index=True)\n</code></pre> <p>which in your case has resulted in <code>category</code> -&gt; <code>object</code> conversion, use:</p> <pre><code>unionized_dataframes = unionize_dataframe_categories(df1, df2, df2)\nconcatenated_df = pd.concat(unionized_dataframes, ignore_index=True)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>*dataframes</code> <code>Any</code> <p>The dataframes you wish to unionize the categorical objects for.</p> <code>()</code> <code>column_names</code> <code>Optional[Iterable[CategoricalDtype]]</code> <p>If supplied, only unionize this subset of columns.</p> <code>None</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If any of the inputs are not pandas DataFrames.</p> <p>Returns:</p> Type Description <code>List[DataFrame]</code> <p>A list of the category-unioned dataframes in the same order they were provided.</p> Source code in <code>janitor/functions/utils.py</code> <pre><code>def unionize_dataframe_categories(\n    *dataframes: Any,\n    column_names: Optional[Iterable[pd.CategoricalDtype]] = None,\n) -&gt; List[pd.DataFrame]:\n    \"\"\"\n    Given a group of dataframes which contain some categorical columns, for\n    each categorical column present, find all the possible categories across\n    all the dataframes which have that column.\n    Update each dataframes' corresponding column with a new categorical object\n    that contains the original data\n    but has labels for all the possible categories from all dataframes.\n    This is useful when concatenating a list of dataframes which all have the\n    same categorical columns into one dataframe.\n\n    If, for a given categorical column, all input dataframes do not have at\n    least one instance of all the possible categories,\n    Pandas will change the output dtype of that column from `category` to\n    `object`, losing out on dramatic speed gains you get from the former\n    format.\n\n    Examples:\n        Usage example for concatenation of categorical column-containing\n        dataframes:\n\n        Instead of:\n\n        ```python\n        concatenated_df = pd.concat([df1, df2, df3], ignore_index=True)\n        ```\n\n        which in your case has resulted in `category` -&gt; `object` conversion,\n        use:\n\n        ```python\n        unionized_dataframes = unionize_dataframe_categories(df1, df2, df2)\n        concatenated_df = pd.concat(unionized_dataframes, ignore_index=True)\n        ```\n\n    Args:\n        *dataframes: The dataframes you wish to unionize the categorical\n            objects for.\n        column_names: If supplied, only unionize this subset of columns.\n\n    Raises:\n        TypeError: If any of the inputs are not pandas DataFrames.\n\n    Returns:\n        A list of the category-unioned dataframes in the same order they\n            were provided.\n    \"\"\"\n\n    if any(not isinstance(df, pd.DataFrame) for df in dataframes):\n        raise TypeError(\"Inputs must all be dataframes.\")\n\n    if column_names is None:\n        # Find all columns across all dataframes that are categorical\n\n        column_names = set()\n\n        for dataframe in dataframes:\n            column_names = column_names.union(\n                [\n                    column_name\n                    for column_name in dataframe.columns\n                    if isinstance(\n                        dataframe[column_name].dtype, pd.CategoricalDtype\n                    )\n                ]\n            )\n\n    else:\n        column_names = [column_names]\n    # For each categorical column, find all possible values across the DFs\n\n    category_unions = {\n        column_name: union_categoricals(\n            [df[column_name] for df in dataframes if column_name in df.columns]\n        )\n        for column_name in column_names\n    }\n\n    # Make a shallow copy of all DFs and modify the categorical columns\n    # such that they can encode the union of all possible categories for each.\n\n    refactored_dfs = []\n\n    for df in dataframes:\n        df = df.copy(deep=False)\n\n        for column_name, categorical in category_unions.items():\n            if column_name in df.columns:\n                df[column_name] = pd.Categorical(\n                    df[column_name], categories=categorical.categories\n                )\n\n        refactored_dfs.append(df)\n\n    return refactored_dfs\n</code></pre>"},{"location":"api/io/","title":"Input/Output (io)","text":""},{"location":"api/io/#janitor.io.read_commandline","title":"<code>read_commandline(cmd, engine='pandas', **kwargs)</code>","text":"<p>Read a CSV file based on a command-line command.</p> <p>For example, you may wish to run the following command on <code>sep-quarter.csv</code> before reading it into a pandas DataFrame:</p> <pre><code>cat sep-quarter.csv | grep .SEA1AA\n</code></pre> <p>In this case, you can use the following Python code to load the dataframe:</p> <pre><code>import janitor as jn\ndf = jn.read_commandline(\"cat data.csv | grep .SEA1AA\")\n</code></pre> <p>This function assumes that your command line command will return an output that is parsable using the relevant engine and StringIO. This function defaults to using <code>pd.read_csv</code> underneath the hood. Keyword arguments are passed through as-is.</p> <p>Parameters:</p> Name Type Description Default <code>cmd</code> <code>str</code> <p>Shell command to preprocess a file on disk.</p> required <code>engine</code> <code>str</code> <p>DataFrame engine to process the output of the shell command. Currently supports both pandas and polars.</p> <code>'pandas'</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments that are passed through to the engine's csv reader.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Mapping</code> <p>A DataFrame parsed from the stdout of the underlying shell.</p> Source code in <code>janitor/io.py</code> <pre><code>def read_commandline(\n    cmd: str, engine: str = \"pandas\", **kwargs: Any\n) -&gt; Mapping:\n    \"\"\"Read a CSV file based on a command-line command.\n\n    For example, you may wish to run the following command on `sep-quarter.csv`\n    before reading it into a pandas DataFrame:\n\n    ```bash\n    cat sep-quarter.csv | grep .SEA1AA\n    ```\n\n    In this case, you can use the following Python code to load the dataframe:\n\n    ```python\n    import janitor as jn\n    df = jn.read_commandline(\"cat data.csv | grep .SEA1AA\")\n    ```\n\n    This function assumes that your command line command will return\n    an output that is parsable using the relevant engine and StringIO.\n    This function defaults to using `pd.read_csv` underneath the hood.\n    Keyword arguments are passed through as-is.\n\n    Args:\n        cmd: Shell command to preprocess a file on disk.\n        engine: DataFrame engine to process the output of the shell command.\n            Currently supports both pandas and polars.\n        **kwargs: Keyword arguments that are passed through to\n            the engine's csv reader.\n\n\n    Returns:\n        A DataFrame parsed from the stdout of the underlying\n            shell.\n    \"\"\"\n\n    check(\"cmd\", cmd, [str])\n    if engine not in {\"pandas\", \"polars\"}:\n        raise ValueError(\"engine should be either pandas or polars.\")\n    # adding check=True ensures that an explicit, clear error\n    # is raised, so that the user can see the reason for the failure\n    outcome = subprocess.run(\n        cmd, shell=True, capture_output=True, text=True, check=True\n    )\n    if engine == \"polars\":\n        try:\n            import polars as pl\n        except ImportError:\n            import_message(\n                submodule=\"polars\",\n                package=\"polars\",\n                conda_channel=\"conda-forge\",\n                pip_install=True,\n            )\n        return pl.read_csv(StringIO(outcome.stdout), **kwargs)\n    return pd.read_csv(StringIO(outcome.stdout), **kwargs)\n</code></pre>"},{"location":"api/io/#janitor.io.read_csvs","title":"<code>read_csvs(files_path, separate_df=False, **kwargs)</code>","text":"<p>Read multiple CSV files and return a dictionary of DataFrames, or one concatenated DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>files_path</code> <code>Union[str, Iterable[str]]</code> <p>The filepath pattern matching the CSV files. Accepts regular expressions, with or without <code>.csv</code> extension. Also accepts iterable of file paths.</p> required <code>separate_df</code> <code>bool</code> <p>If <code>False</code> (default), returns a single Dataframe with the concatenation of the csv files. If <code>True</code>, returns a dictionary of separate DataFrames for each CSV file.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments to pass into the original pandas <code>read_csv</code>.</p> <code>{}</code> <p>Raises:</p> Type Description <code>JanitorError</code> <p>If <code>None</code> provided for <code>files_path</code>.</p> <code>JanitorError</code> <p>If length of <code>files_path</code> is <code>0</code>.</p> <code>ValueError</code> <p>If no CSV files exist in <code>files_path</code>.</p> <code>ValueError</code> <p>If columns in input CSV files do not match.</p> <p>Returns:</p> Type Description <code>Union[DataFrame, dict]</code> <p>DataFrame of concatenated DataFrames or dictionary of DataFrames.</p> Source code in <code>janitor/io.py</code> <pre><code>@deprecated_alias(seperate_df=\"separate_df\", filespath=\"files_path\")\ndef read_csvs(\n    files_path: Union[str, Iterable[str]],\n    separate_df: bool = False,\n    **kwargs: Any,\n) -&gt; Union[pd.DataFrame, dict]:\n    \"\"\"Read multiple CSV files and return a dictionary of DataFrames, or\n    one concatenated DataFrame.\n\n    Args:\n        files_path: The filepath pattern matching the CSV files.\n            Accepts regular expressions, with or without `.csv` extension.\n            Also accepts iterable of file paths.\n        separate_df: If `False` (default), returns a single Dataframe\n            with the concatenation of the csv files.\n            If `True`, returns a dictionary of separate DataFrames\n            for each CSV file.\n        **kwargs: Keyword arguments to pass into the\n            original pandas `read_csv`.\n\n    Raises:\n        JanitorError: If `None` provided for `files_path`.\n        JanitorError: If length of `files_path` is `0`.\n        ValueError: If no CSV files exist in `files_path`.\n        ValueError: If columns in input CSV files do not match.\n\n    Returns:\n        DataFrame of concatenated DataFrames or dictionary of DataFrames.\n    \"\"\"\n    # Sanitize input\n    if files_path is None:\n        raise JanitorError(\"`None` provided for `files_path`\")\n    if not files_path:\n        raise JanitorError(\"0 length `files_path` provided\")\n\n    # Read the csv files\n    # String to file/folder or file pattern provided\n    if isinstance(files_path, str):\n        dfs_dict = {\n            os.path.basename(f): pd.read_csv(f, **kwargs)\n            for f in glob(files_path)\n        }\n    # Iterable of file paths provided\n    else:\n        dfs_dict = {\n            os.path.basename(f): pd.read_csv(f, **kwargs) for f in files_path\n        }\n    # Check if dataframes have been read\n    if not dfs_dict:\n        raise ValueError(\"No CSV files to read with the given `files_path`\")\n    # Concatenate the dataframes if requested (default)\n    col_names = list(dfs_dict.values())[0].columns  # noqa: PD011\n    if not separate_df:\n        # If columns do not match raise an error\n        for df in dfs_dict.values():  # noqa: PD011\n            if not all(df.columns == col_names):\n                raise ValueError(\n                    \"Columns in input CSV files do not match.\"\n                    \"Files cannot be concatenated.\"\n                )\n        return pd.concat(\n            list(dfs_dict.values()),\n            ignore_index=True,\n            sort=False,  # noqa: PD011\n            copy=False,\n        )\n    return dfs_dict\n</code></pre>"},{"location":"api/io/#janitor.io.xlsx_cells","title":"<code>xlsx_cells(path, sheetnames=None, start_point=None, end_point=None, read_only=True, include_blank_cells=True, fill=False, font=False, alignment=False, border=False, protection=False, comment=False, engine='pandas', **kwargs)</code>","text":"<p>Imports data from spreadsheet without coercing it into a rectangle.</p> <p>Each cell is represented by a row in a dataframe, and includes the cell's coordinates, the value, row and column position. The cell formatting (fill, font, border, etc) can also be accessed; usually this is returned as a dictionary in the cell, and the specific cell format attribute can be accessed using <code>pd.Series.str.get</code> or <code>pl.struct.field</code> if it is a polars DataFrame.</p> <p>Inspiration for this comes from R's tidyxl package.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from janitor import xlsx_cells\n&gt;&gt;&gt; pd.set_option(\"display.max_columns\", None)\n&gt;&gt;&gt; pd.set_option(\"display.expand_frame_repr\", False)\n&gt;&gt;&gt; pd.set_option(\"max_colwidth\", None)\n&gt;&gt;&gt; filename = \"../pyjanitor/tests/test_data/worked-examples.xlsx\"\n</code></pre> <p>Each cell is returned as a row:</p> <pre><code>&gt;&gt;&gt; xlsx_cells(filename, sheetnames=\"highlights\")\n    value internal_value coordinate  row  column data_type  is_date number_format\n0     Age            Age         A1    1       1         s    False       General\n1  Height         Height         B1    1       2         s    False       General\n2       1              1         A2    2       1         n    False       General\n3       2              2         B2    2       2         n    False       General\n4       3              3         A3    3       1         n    False       General\n5       4              4         B3    3       2         n    False       General\n6       5              5         A4    4       1         n    False       General\n7       6              6         B4    4       2         n    False       General\n</code></pre> <p>Access cell formatting such as fill:</p> <pre><code>&gt;&gt;&gt; out=xlsx_cells(filename, sheetnames=\"highlights\", fill=True).select(\"value\", \"fill\", axis='columns')\n&gt;&gt;&gt; out\n    value                                                                                                                                              fill\n0     Age     {'patternType': None, 'fgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}}\n1  Height     {'patternType': None, 'fgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}}\n2       1     {'patternType': None, 'fgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}}\n3       2     {'patternType': None, 'fgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}}\n4       3  {'patternType': 'solid', 'fgColor': {'rgb': 'FFFFFF00', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': 'FFFFFF00', 'type': 'rgb', 'tint': 0.0}}\n5       4  {'patternType': 'solid', 'fgColor': {'rgb': 'FFFFFF00', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': 'FFFFFF00', 'type': 'rgb', 'tint': 0.0}}\n6       5     {'patternType': None, 'fgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}}\n7       6     {'patternType': None, 'fgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}}\n</code></pre> <p>Specific cell attributes can be accessed by using Pandas' <code>series.str.get</code>:</p> <pre><code>&gt;&gt;&gt; out.fill.str.get(\"fgColor\").str.get(\"rgb\")\n0    00000000\n1    00000000\n2    00000000\n3    00000000\n4    FFFFFF00\n5    FFFFFF00\n6    00000000\n7    00000000\nName: fill, dtype: object\n</code></pre> <p>Access cell formatting in a polars DataFrame:</p> <pre><code>&gt;&gt;&gt; out = xlsx_cells(filename, sheetnames=\"highlights\", engine='polars', fill=True).get_column('fill')\n&gt;&gt;&gt; out\nshape: (8,)\nSeries: 'fill' [struct[3]]\n[\n   {null,{\"00000000\",\"rgb\",0.0},{\"00000000\",\"rgb\",0.0}}\n   {null,{\"00000000\",\"rgb\",0.0},{\"00000000\",\"rgb\",0.0}}\n   {null,{\"00000000\",\"rgb\",0.0},{\"00000000\",\"rgb\",0.0}}\n   {null,{\"00000000\",\"rgb\",0.0},{\"00000000\",\"rgb\",0.0}}\n   {\"solid\",{\"FFFFFF00\",\"rgb\",0.0},{\"FFFFFF00\",\"rgb\",0.0}}\n   {\"solid\",{\"FFFFFF00\",\"rgb\",0.0},{\"FFFFFF00\",\"rgb\",0.0}}\n   {null,{\"00000000\",\"rgb\",0.0},{\"00000000\",\"rgb\",0.0}}\n   {null,{\"00000000\",\"rgb\",0.0},{\"00000000\",\"rgb\",0.0}}\n]\n</code></pre> <p>Specific cell attributes can be acessed via Polars' struct:</p> <pre><code>&gt;&gt;&gt; out.struct.field('fgColor').struct.field('rgb')\nshape: (8,)\nSeries: 'rgb' [str]\n[\n   \"00000000\"\n   \"00000000\"\n   \"00000000\"\n   \"00000000\"\n   \"FFFFFF00\"\n   \"FFFFFF00\"\n   \"00000000\"\n   \"00000000\"\n]\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Workbook]</code> <p>Path to the Excel File. It can also be an openpyxl Workbook.</p> required <code>sheetnames</code> <code>Union[str, list, tuple]</code> <p>Names of the sheets from which the cells are to be extracted. If <code>None</code>, all the sheets in the file are extracted; if it is a string, or list or tuple, only the specified sheets are extracted.</p> <code>None</code> <code>start_point</code> <code>Union[str, int]</code> <p>Start coordinates of the Excel sheet. This is useful if the user is only interested in a subsection of the sheet. If <code>start_point</code> is provided, <code>end_point</code> must be provided as well.</p> <code>None</code> <code>end_point</code> <code>Union[str, int]</code> <p>End coordinates of the Excel sheet. This is useful if the user is only interested in a subsection of the sheet. If <code>end_point</code> is provided, <code>start_point</code> must be provided as well.</p> <code>None</code> <code>read_only</code> <code>bool</code> <p>Determines if the entire file is loaded in memory, or streamed. For memory efficiency, read_only should be set to <code>True</code>. Some cell properties like <code>comment</code>, can only be accessed by setting <code>read_only</code> to <code>False</code>.</p> <code>True</code> <code>include_blank_cells</code> <code>bool</code> <p>Determines if cells without a value should be included.</p> <code>True</code> <code>fill</code> <code>bool</code> <p>If <code>True</code>, return fill properties of the cell. It is usually returned as a dictionary.</p> <code>False</code> <code>font</code> <code>bool</code> <p>If <code>True</code>, return font properties of the cell. It is usually returned as a dictionary.</p> <code>False</code> <code>alignment</code> <code>bool</code> <p>If <code>True</code>, return alignment properties of the cell. It is usually returned as a dictionary.</p> <code>False</code> <code>border</code> <code>bool</code> <p>If <code>True</code>, return border properties of the cell. It is usually returned as a dictionary.</p> <code>False</code> <code>protection</code> <code>bool</code> <p>If <code>True</code>, return protection properties of the cell. It is usually returned as a dictionary.</p> <code>False</code> <code>comment</code> <code>bool</code> <p>If <code>True</code>, return comment properties of the cell. It is usually returned as a dictionary.</p> <code>False</code> <code>engine</code> <code>str</code> <p>DataFrame engine. Should be either pandas or polars.</p> <code>'pandas'</code> <code>**kwargs</code> <code>Any</code> <p>Any other attributes of the cell, that can be accessed from openpyxl.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If kwargs is provided, and one of the keys is a default column.</p> <code>AttributeError</code> <p>If kwargs is provided and any of the keys is not a openpyxl cell attribute.</p> <p>Returns:</p> Type Description <code>Mapping</code> <p>A DataFrame, or a dictionary of DataFrames.</p> Source code in <code>janitor/io.py</code> <pre><code>def xlsx_cells(\n    path: Union[str, Workbook],\n    sheetnames: Union[str, list, tuple] = None,\n    start_point: Union[str, int] = None,\n    end_point: Union[str, int] = None,\n    read_only: bool = True,\n    include_blank_cells: bool = True,\n    fill: bool = False,\n    font: bool = False,\n    alignment: bool = False,\n    border: bool = False,\n    protection: bool = False,\n    comment: bool = False,\n    engine: str = \"pandas\",\n    **kwargs: Any,\n) -&gt; Mapping:\n    \"\"\"Imports data from spreadsheet without coercing it into a rectangle.\n\n    Each cell is represented by a row in a dataframe, and includes the\n    cell's coordinates, the value, row and column position.\n    The cell formatting (fill, font, border, etc) can also be accessed;\n    usually this is returned as a dictionary in the cell, and the specific\n    cell format attribute can be accessed using `pd.Series.str.get`\n    or `pl.struct.field` if it is a polars DataFrame.\n\n    Inspiration for this comes from R's [tidyxl][link] package.\n    [link]: https://nacnudus.github.io/tidyxl/reference/tidyxl.html\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import polars as pl\n        &gt;&gt;&gt; from janitor import xlsx_cells\n        &gt;&gt;&gt; pd.set_option(\"display.max_columns\", None)\n        &gt;&gt;&gt; pd.set_option(\"display.expand_frame_repr\", False)\n        &gt;&gt;&gt; pd.set_option(\"max_colwidth\", None)\n        &gt;&gt;&gt; filename = \"../pyjanitor/tests/test_data/worked-examples.xlsx\"\n\n        Each cell is returned as a row:\n\n        &gt;&gt;&gt; xlsx_cells(filename, sheetnames=\"highlights\")\n            value internal_value coordinate  row  column data_type  is_date number_format\n        0     Age            Age         A1    1       1         s    False       General\n        1  Height         Height         B1    1       2         s    False       General\n        2       1              1         A2    2       1         n    False       General\n        3       2              2         B2    2       2         n    False       General\n        4       3              3         A3    3       1         n    False       General\n        5       4              4         B3    3       2         n    False       General\n        6       5              5         A4    4       1         n    False       General\n        7       6              6         B4    4       2         n    False       General\n\n        Access cell formatting such as fill:\n\n        &gt;&gt;&gt; out=xlsx_cells(filename, sheetnames=\"highlights\", fill=True).select(\"value\", \"fill\", axis='columns')\n        &gt;&gt;&gt; out\n            value                                                                                                                                              fill\n        0     Age     {'patternType': None, 'fgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}}\n        1  Height     {'patternType': None, 'fgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}}\n        2       1     {'patternType': None, 'fgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}}\n        3       2     {'patternType': None, 'fgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}}\n        4       3  {'patternType': 'solid', 'fgColor': {'rgb': 'FFFFFF00', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': 'FFFFFF00', 'type': 'rgb', 'tint': 0.0}}\n        5       4  {'patternType': 'solid', 'fgColor': {'rgb': 'FFFFFF00', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': 'FFFFFF00', 'type': 'rgb', 'tint': 0.0}}\n        6       5     {'patternType': None, 'fgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}}\n        7       6     {'patternType': None, 'fgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}, 'bgColor': {'rgb': '00000000', 'type': 'rgb', 'tint': 0.0}}\n\n        Specific cell attributes can be accessed by using Pandas' `series.str.get`:\n\n        &gt;&gt;&gt; out.fill.str.get(\"fgColor\").str.get(\"rgb\")\n        0    00000000\n        1    00000000\n        2    00000000\n        3    00000000\n        4    FFFFFF00\n        5    FFFFFF00\n        6    00000000\n        7    00000000\n        Name: fill, dtype: object\n\n        Access cell formatting in a polars DataFrame:\n\n        &gt;&gt;&gt; out = xlsx_cells(filename, sheetnames=\"highlights\", engine='polars', fill=True).get_column('fill')\n        &gt;&gt;&gt; out\n        shape: (8,)\n        Series: 'fill' [struct[3]]\n        [\n           {null,{\"00000000\",\"rgb\",0.0},{\"00000000\",\"rgb\",0.0}}\n           {null,{\"00000000\",\"rgb\",0.0},{\"00000000\",\"rgb\",0.0}}\n           {null,{\"00000000\",\"rgb\",0.0},{\"00000000\",\"rgb\",0.0}}\n           {null,{\"00000000\",\"rgb\",0.0},{\"00000000\",\"rgb\",0.0}}\n           {\"solid\",{\"FFFFFF00\",\"rgb\",0.0},{\"FFFFFF00\",\"rgb\",0.0}}\n           {\"solid\",{\"FFFFFF00\",\"rgb\",0.0},{\"FFFFFF00\",\"rgb\",0.0}}\n           {null,{\"00000000\",\"rgb\",0.0},{\"00000000\",\"rgb\",0.0}}\n           {null,{\"00000000\",\"rgb\",0.0},{\"00000000\",\"rgb\",0.0}}\n        ]\n\n        Specific cell attributes can be acessed via Polars' struct:\n\n        &gt;&gt;&gt; out.struct.field('fgColor').struct.field('rgb')\n        shape: (8,)\n        Series: 'rgb' [str]\n        [\n           \"00000000\"\n           \"00000000\"\n           \"00000000\"\n           \"00000000\"\n           \"FFFFFF00\"\n           \"FFFFFF00\"\n           \"00000000\"\n           \"00000000\"\n        ]\n\n\n    Args:\n        path: Path to the Excel File. It can also be an openpyxl Workbook.\n        sheetnames: Names of the sheets from which the cells are to be extracted.\n            If `None`, all the sheets in the file are extracted;\n            if it is a string, or list or tuple, only the specified sheets are extracted.\n        start_point: Start coordinates of the Excel sheet. This is useful\n            if the user is only interested in a subsection of the sheet.\n            If `start_point` is provided, `end_point` must be provided as well.\n        end_point: End coordinates of the Excel sheet. This is useful\n            if the user is only interested in a subsection of the sheet.\n            If `end_point` is provided, `start_point` must be provided as well.\n        read_only: Determines if the entire file is loaded in memory,\n            or streamed. For memory efficiency, read_only should be set to `True`.\n            Some cell properties like `comment`, can only be accessed by\n            setting `read_only` to `False`.\n        include_blank_cells: Determines if cells without a value should be included.\n        fill: If `True`, return fill properties of the cell.\n            It is usually returned as a dictionary.\n        font: If `True`, return font properties of the cell.\n            It is usually returned as a dictionary.\n        alignment: If `True`, return alignment properties of the cell.\n            It is usually returned as a dictionary.\n        border: If `True`, return border properties of the cell.\n            It is usually returned as a dictionary.\n        protection: If `True`, return protection properties of the cell.\n            It is usually returned as a dictionary.\n        comment: If `True`, return comment properties of the cell.\n            It is usually returned as a dictionary.\n        engine: DataFrame engine. Should be either pandas or polars.\n        **kwargs: Any other attributes of the cell, that can be accessed from openpyxl.\n\n    Raises:\n        ValueError: If kwargs is provided, and one of the keys is a default column.\n        AttributeError: If kwargs is provided and any of the keys\n            is not a openpyxl cell attribute.\n\n    Returns:\n        A DataFrame, or a dictionary of DataFrames.\n    \"\"\"  # noqa : E501\n\n    try:\n        from openpyxl import load_workbook\n        from openpyxl.cell.cell import Cell\n        from openpyxl.cell.read_only import ReadOnlyCell\n        from openpyxl.workbook.workbook import Workbook\n    except ImportError:\n        import_message(\n            submodule=\"io\",\n            package=\"openpyxl\",\n            conda_channel=\"conda-forge\",\n            pip_install=True,\n        )\n\n    path_is_workbook = isinstance(path, Workbook)\n    if not path_is_workbook:\n        # for memory efficiency, read_only is set to True\n        # if comments is True, read_only has to be False,\n        # as lazy loading is not enabled for comments\n        if comment and read_only:\n            raise ValueError(\n                \"To access comments, kindly set 'read_only' to False.\"\n            )\n        path = load_workbook(\n            filename=path, read_only=read_only, keep_links=False\n        )\n    if engine not in {\"pandas\", \"polars\"}:\n        raise ValueError(\"engine should be one of pandas or polars.\")\n    base_engine = pd\n    if engine == \"polars\":\n        try:\n            import polars as pl\n\n            base_engine = pl\n        except ImportError:\n            import_message(\n                submodule=\"polars\",\n                package=\"polars\",\n                conda_channel=\"conda-forge\",\n                pip_install=True,\n            )\n    # start_point and end_point applies if the user is interested in\n    # only a subset of the Excel File and knows the coordinates\n    if start_point or end_point:\n        check(\"start_point\", start_point, [str, int])\n        check(\"end_point\", end_point, [str, int])\n\n    defaults = (\n        \"value\",\n        \"internal_value\",\n        \"coordinate\",\n        \"row\",\n        \"column\",\n        \"data_type\",\n        \"is_date\",\n        \"number_format\",\n    )\n\n    parameters = {\n        \"fill\": fill,\n        \"font\": font,\n        \"alignment\": alignment,\n        \"border\": border,\n        \"protection\": protection,\n        \"comment\": comment,\n    }\n\n    if kwargs:\n        if path_is_workbook:\n            if path.read_only:\n                _cell = ReadOnlyCell\n            else:\n                _cell = Cell\n        else:\n            if read_only:\n                _cell = ReadOnlyCell\n            else:\n                _cell = Cell\n\n        attrs = {\n            attr\n            for attr, _ in inspect.getmembers(_cell, not (inspect.isroutine))\n            if not attr.startswith(\"_\")\n        }\n\n        for key in kwargs:\n            if key in defaults:\n                raise ValueError(\n                    f\"{key} is part of the default attributes \"\n                    \"returned as a column.\"\n                )\n            elif key not in attrs:\n                raise AttributeError(\n                    f\"{key} is not a recognized attribute of {_cell}.\"\n                )\n        parameters.update(kwargs)\n\n    if not sheetnames:\n        sheetnames = path.sheetnames\n    elif isinstance(sheetnames, str):\n        sheetnames = [sheetnames]\n    else:\n        check(\"sheetnames\", sheetnames, [str, list, tuple])\n\n    out = {\n        sheetname: _xlsx_cells(\n            path[sheetname],\n            defaults,\n            parameters,\n            start_point,\n            end_point,\n            include_blank_cells,\n            base_engine=base_engine,\n        )\n        for sheetname in sheetnames\n    }\n    if len(out) == 1:\n        _, out = out.popitem()\n\n    if (not path_is_workbook) and path.read_only:\n        path.close()\n\n    return out\n</code></pre>"},{"location":"api/io/#janitor.io.xlsx_table","title":"<code>xlsx_table(path, sheetname=None, table=None, engine='pandas')</code>","text":"<p>Returns a DataFrame of values in a table in the Excel file.</p> <p>This applies to an Excel file, where the data range is explicitly specified as a Microsoft Excel table.</p> <p>If there is a single table in the sheet, or a string is provided as an argument to the <code>table</code> parameter, a DataFrame is returned; if there is more than one table in the sheet, and the <code>table</code> argument is <code>None</code>, or a list/tuple of names, a dictionary of DataFrames is returned, where the keys of the dictionary are the table names.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; from janitor import xlsx_table\n&gt;&gt;&gt; filename=\"../pyjanitor/tests/test_data/016-MSPTDA-Excel.xlsx\"\n</code></pre> <p>Single table:</p> <pre><code>&gt;&gt;&gt; xlsx_table(filename, table='dCategory')\n   CategoryID       Category\n0           1       Beginner\n1           2       Advanced\n2           3      Freestyle\n3           4    Competition\n4           5  Long Distance\n</code></pre> <pre><code>&gt;&gt;&gt; xlsx_table(filename, table='dCategory', engine='polars')\nshape: (5, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 CategoryID \u2506 Category      \u2502\n\u2502 ---        \u2506 ---           \u2502\n\u2502 i64        \u2506 str           \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1          \u2506 Beginner      \u2502\n\u2502 2          \u2506 Advanced      \u2502\n\u2502 3          \u2506 Freestyle     \u2502\n\u2502 4          \u2506 Competition   \u2502\n\u2502 5          \u2506 Long Distance \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Multiple tables:</p> <pre><code>&gt;&gt;&gt; out=xlsx_table(filename, table=[\"dCategory\", \"dSalesReps\"])\n&gt;&gt;&gt; out[\"dCategory\"]\n   CategoryID       Category\n0           1       Beginner\n1           2       Advanced\n2           3      Freestyle\n3           4    Competition\n4           5  Long Distance\n&gt;&gt;&gt; out[\"dSalesReps\"].head(3)\n   SalesRepID             SalesRep Region\n0           1  Sioux Radcoolinator     NW\n1           2        Tyrone Smithe     NE\n2           3         Chantel Zoya     SW\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, IO, Workbook]</code> <p>Path to the Excel File. It can also be an openpyxl Workbook.</p> required <code>table</code> <code>Union[str, list, tuple]</code> <p>Name of a table, or list of tables in the sheet.</p> <code>None</code> <code>engine</code> <code>str</code> <p>DataFrame engine. Should be either pandas or polars. Defaults to pandas</p> <code>'pandas'</code> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If a workbook is provided, and is a ReadOnlyWorksheet.</p> <code>ValueError</code> <p>If there are no tables in the sheet.</p> <code>KeyError</code> <p>If the provided table does not exist in the sheet.</p> <p>Returns:</p> Type Description <code>Mapping</code> <p>A DataFrame, or a dictionary of DataFrames, if there are multiple arguments for the <code>table</code> parameter, or the argument to <code>table</code> is <code>None</code>.</p> Source code in <code>janitor/io.py</code> <pre><code>def xlsx_table(\n    path: Union[str, IO, Workbook],\n    sheetname: str = None,\n    table: Union[str, list, tuple] = None,\n    engine: str = \"pandas\",\n) -&gt; Mapping:\n    \"\"\"Returns a DataFrame of values in a table in the Excel file.\n\n    This applies to an Excel file, where the data range is explicitly\n    specified as a Microsoft Excel table.\n\n    If there is a single table in the sheet, or a string is provided\n    as an argument to the `table` parameter, a DataFrame is returned;\n    if there is more than one table in the sheet,\n    and the `table` argument is `None`, or a list/tuple of names,\n    a dictionary of DataFrames is returned, where the keys of the dictionary\n    are the table names.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import polars as pl\n        &gt;&gt;&gt; from janitor import xlsx_table\n        &gt;&gt;&gt; filename=\"../pyjanitor/tests/test_data/016-MSPTDA-Excel.xlsx\"\n\n        Single table:\n\n        &gt;&gt;&gt; xlsx_table(filename, table='dCategory')\n           CategoryID       Category\n        0           1       Beginner\n        1           2       Advanced\n        2           3      Freestyle\n        3           4    Competition\n        4           5  Long Distance\n\n        &gt;&gt;&gt; xlsx_table(filename, table='dCategory', engine='polars')\n        shape: (5, 2)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 CategoryID \u2506 Category      \u2502\n        \u2502 ---        \u2506 ---           \u2502\n        \u2502 i64        \u2506 str           \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 1          \u2506 Beginner      \u2502\n        \u2502 2          \u2506 Advanced      \u2502\n        \u2502 3          \u2506 Freestyle     \u2502\n        \u2502 4          \u2506 Competition   \u2502\n        \u2502 5          \u2506 Long Distance \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n        Multiple tables:\n\n        &gt;&gt;&gt; out=xlsx_table(filename, table=[\"dCategory\", \"dSalesReps\"])\n        &gt;&gt;&gt; out[\"dCategory\"]\n           CategoryID       Category\n        0           1       Beginner\n        1           2       Advanced\n        2           3      Freestyle\n        3           4    Competition\n        4           5  Long Distance\n        &gt;&gt;&gt; out[\"dSalesReps\"].head(3)\n           SalesRepID             SalesRep Region\n        0           1  Sioux Radcoolinator     NW\n        1           2        Tyrone Smithe     NE\n        2           3         Chantel Zoya     SW\n\n    Args:\n        path: Path to the Excel File. It can also be an openpyxl Workbook.\n        table: Name of a table, or list of tables in the sheet.\n        engine: DataFrame engine. Should be either pandas or polars.\n            Defaults to pandas\n\n    Raises:\n        AttributeError: If a workbook is provided, and is a ReadOnlyWorksheet.\n        ValueError: If there are no tables in the sheet.\n        KeyError: If the provided table does not exist in the sheet.\n\n    Returns:\n        A DataFrame, or a dictionary of DataFrames,\n            if there are multiple arguments for the `table` parameter,\n            or the argument to `table` is `None`.\n    \"\"\"  # noqa : E501\n\n    try:\n        from openpyxl import load_workbook\n        from openpyxl.workbook.workbook import Workbook\n    except ImportError:\n        import_message(\n            submodule=\"io\",\n            package=\"openpyxl\",\n            conda_channel=\"conda-forge\",\n            pip_install=True,\n        )\n    # TODO: remove in version 1.0\n    if sheetname:\n        warnings.warn(\n            \"The keyword argument \"\n            \"'sheetname' of 'xlsx_tables' is deprecated.\",\n            DeprecationWarning,\n            stacklevel=find_stack_level(),\n        )\n    if engine not in {\"pandas\", \"polars\"}:\n        raise ValueError(\"engine should be one of pandas or polars.\")\n    base_engine = pd\n    if engine == \"polars\":\n        try:\n            import polars as pl\n\n            base_engine = pl\n        except ImportError:\n            import_message(\n                submodule=\"polars\",\n                package=\"polars\",\n                conda_channel=\"conda-forge\",\n                pip_install=True,\n            )\n\n    if table is not None:\n        check(\"table\", table, [str, list, tuple])\n        if isinstance(table, (list, tuple)):\n            for num, entry in enumerate(table):\n                check(f\"entry{num} in the table argument\", entry, [str])\n    if isinstance(path, Workbook):\n        ws = path\n    else:\n        ws = load_workbook(\n            filename=path, read_only=False, keep_links=False, data_only=True\n        )\n    if ws.read_only:\n        raise ValueError(\"xlsx_table does not work in read only mode.\")\n\n    def _create_dataframe_or_dictionary_from_table(\n        table_name_and_worksheet: tuple,\n    ):\n        \"\"\"\n        Create DataFrame/dictionary if table exists in Workbook\n        \"\"\"\n        dictionary = {}\n        for table_name, worksheet in table_name_and_worksheet:\n            contents = worksheet.tables[table_name]\n            header_exist = contents.headerRowCount\n            coordinates = contents.ref\n            data = worksheet[coordinates]\n            if header_exist:\n                header, *data = data\n                header = [cell.value for cell in header]\n            else:\n                header = [f\"C{num}\" for num in range(len(data[0]))]\n            data = zip(*data)\n            data = ([entry.value for entry in cell] for cell in data)\n            data = dict(zip(header, data))\n            dictionary[table_name] = base_engine.DataFrame(data)\n        return dictionary\n\n    worksheets = [worksheet for worksheet in ws if worksheet.tables.items()]\n    if not any(worksheets):\n        raise ValueError(\"There are no tables in the Workbook.\")\n    table_is_a_string = False\n    if table:\n        if isinstance(table, str):\n            table_is_a_string = True\n            table = [table]\n        table_names = (\n            entry for worksheet in worksheets for entry in worksheet.tables\n        )\n        missing = set(table).difference(table_names)\n        if missing:\n            raise KeyError(f\"Tables {*missing,} do not exist in the Workbook.\")\n        tables = [\n            (entry, worksheet)\n            for worksheet in worksheets\n            for entry in worksheet.tables\n            if entry in table\n        ]\n    else:\n        tables = [\n            (entry, worksheet)\n            for worksheet in worksheets\n            for entry in worksheet.tables\n        ]\n    data = _create_dataframe_or_dictionary_from_table(\n        table_name_and_worksheet=tables\n    )\n    if table_is_a_string:\n        return data[table[0]]\n    return data\n</code></pre>"},{"location":"api/math/","title":"Math","text":"<p>Miscellaneous mathematical operators.</p>"},{"location":"api/math/#janitor.math.ecdf","title":"<code>ecdf(s)</code>","text":"<p>Return cumulative distribution of values in a series.</p> <p>Null values must be dropped from the series, otherwise a <code>ValueError</code> is raised.</p> <p>Also, if the <code>dtype</code> of the series is not numeric, a <code>TypeError</code> is raised.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; s = pd.Series([0, 4, 0, 1, 2, 1, 1, 3])\n&gt;&gt;&gt; x, y = s.ecdf()\n&gt;&gt;&gt; x\narray([0, 0, 1, 1, 1, 2, 3, 4])\n&gt;&gt;&gt; y\narray([0.125, 0.25 , 0.375, 0.5  , 0.625, 0.75 , 0.875, 1.   ])\n</code></pre> <p>You can then plot the ECDF values, for example:</p> <pre><code>&gt;&gt;&gt; from matplotlib import pyplot as plt\n&gt;&gt;&gt; plt.scatter(x, y)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>Series</code> <p>A pandas series. <code>dtype</code> should be numeric.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If series is not numeric.</p> <code>ValueError</code> <p>If series contains nulls.</p> <p>Returns:</p> Name Type Description <code>x</code> <code>ndarray</code> <p>Sorted array of values.</p> <code>y</code> <code>ndarray</code> <p>Cumulative fraction of data points with value <code>x</code> or lower.</p> Source code in <code>janitor/math.py</code> <pre><code>@pf.register_series_method\ndef ecdf(s: \"Series\") -&gt; Tuple[\"ndarray\", \"ndarray\"]:\n    \"\"\"Return cumulative distribution of values in a series.\n\n    Null values must be dropped from the series,\n    otherwise a `ValueError` is raised.\n\n    Also, if the `dtype` of the series is not numeric,\n    a `TypeError` is raised.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; s = pd.Series([0, 4, 0, 1, 2, 1, 1, 3])\n        &gt;&gt;&gt; x, y = s.ecdf()\n        &gt;&gt;&gt; x  # doctest: +SKIP\n        array([0, 0, 1, 1, 1, 2, 3, 4])\n        &gt;&gt;&gt; y  # doctest: +SKIP\n        array([0.125, 0.25 , 0.375, 0.5  , 0.625, 0.75 , 0.875, 1.   ])\n\n        You can then plot the ECDF values, for example:\n\n        &gt;&gt;&gt; from matplotlib import pyplot as plt\n        &gt;&gt;&gt; plt.scatter(x, y)  # doctest: +SKIP\n\n    Args:\n        s: A pandas series. `dtype` should be numeric.\n\n    Raises:\n        TypeError: If series is not numeric.\n        ValueError: If series contains nulls.\n\n    Returns:\n        x: Sorted array of values.\n        y: Cumulative fraction of data points with value `x` or lower.\n    \"\"\"\n    import numpy as np\n    import pandas.api.types as pdtypes\n\n    if not pdtypes.is_numeric_dtype(s):\n        raise TypeError(f\"series {s.name} must be numeric!\")\n    if not s.isna().sum() == 0:\n        raise ValueError(f\"series {s.name} contains nulls. Please drop them.\")\n\n    n = len(s)\n    x = np.sort(s)\n    y = np.arange(1, n + 1) / n\n\n    return x, y\n</code></pre>"},{"location":"api/math/#janitor.math.exp","title":"<code>exp(s)</code>","text":"<p>Take the exponential transform of the series.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; s = pd.Series([0, 1, 3], name=\"numbers\")\n&gt;&gt;&gt; s.exp()\n0     1.000000\n1     2.718282\n2    20.085537\nName: numbers, dtype: float64\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>Series</code> <p>Input Series.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Transformed Series.</p> Source code in <code>janitor/math.py</code> <pre><code>@pf.register_series_method\ndef exp(s: \"Series\") -&gt; \"Series\":\n    \"\"\"Take the exponential transform of the series.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; s = pd.Series([0, 1, 3], name=\"numbers\")\n        &gt;&gt;&gt; s.exp()\n        0     1.000000\n        1     2.718282\n        2    20.085537\n        Name: numbers, dtype: float64\n\n    Args:\n        s: Input Series.\n\n    Returns:\n        Transformed Series.\n    \"\"\"\n    import numpy as np\n\n    return np.exp(s)\n</code></pre>"},{"location":"api/math/#janitor.math.log","title":"<code>log(s, error='warn')</code>","text":"<p>Take natural logarithm of the Series.</p> <p>Each value in the series should be positive. Use <code>error</code> to control the behavior if there are nonpositive entries in the series.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; s = pd.Series([0, 1, 3], name=\"numbers\")\n&gt;&gt;&gt; s.log(error=\"ignore\")\n0         NaN\n1    0.000000\n2    1.098612\nName: numbers, dtype: float64\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>Series</code> <p>Input Series.</p> required <code>error</code> <code>str</code> <p>Determines behavior when taking the log of nonpositive entries. If <code>'warn'</code> then a <code>RuntimeWarning</code> is thrown. If <code>'raise'</code>, then a <code>RuntimeError</code> is thrown. Otherwise, nothing is thrown and log of nonpositive values is <code>np.nan</code>.</p> <code>'warn'</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>Raised when there are nonpositive values in the Series and <code>error='raise'</code>.</p> <p>Returns:</p> Type Description <code>Series</code> <p>Transformed Series.</p> Source code in <code>janitor/math.py</code> <pre><code>@pf.register_series_method\ndef log(s: \"Series\", error: str = \"warn\") -&gt; \"Series\":\n    \"\"\"\n    Take natural logarithm of the Series.\n\n    Each value in the series should be positive. Use `error` to control the\n    behavior if there are nonpositive entries in the series.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; s = pd.Series([0, 1, 3], name=\"numbers\")\n        &gt;&gt;&gt; s.log(error=\"ignore\")\n        0         NaN\n        1    0.000000\n        2    1.098612\n        Name: numbers, dtype: float64\n\n    Args:\n        s: Input Series.\n        error: Determines behavior when taking the log of nonpositive\n            entries. If `'warn'` then a `RuntimeWarning` is thrown. If\n            `'raise'`, then a `RuntimeError` is thrown. Otherwise, nothing\n            is thrown and log of nonpositive values is `np.nan`.\n\n    Raises:\n        RuntimeError: Raised when there are nonpositive values in the\n            Series and `error='raise'`.\n\n    Returns:\n        Transformed Series.\n    \"\"\"\n    import numpy as np\n\n    s = s.copy()\n    nonpositive = s &lt;= 0\n    if (nonpositive).any():\n        msg = f\"Log taken on {nonpositive.sum()} nonpositive value(s)\"\n        if error.lower() == \"warn\":\n            warnings.warn(msg, RuntimeWarning)\n        if error.lower() == \"raise\":\n            raise RuntimeError(msg)\n        else:\n            pass\n    s[nonpositive] = np.nan\n    return np.log(s)\n</code></pre>"},{"location":"api/math/#janitor.math.logit","title":"<code>logit(s, error='warn')</code>","text":"<p>Take logit transform of the Series.</p> <p>The logit transform is defined:</p> <pre><code>logit(p) = log(p/(1-p))\n</code></pre> <p>Each value in the series should be between 0 and 1. Use <code>error</code> to control the behavior if any series entries are outside of (0, 1).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; s = pd.Series([0.1, 0.5, 0.9], name=\"numbers\")\n&gt;&gt;&gt; s.logit()\n0   -2.197225\n1    0.000000\n2    2.197225\nName: numbers, dtype: float64\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>Series</code> <p>Input Series.</p> required <code>error</code> <code>str</code> <p>Determines behavior when <code>s</code> is outside of <code>(0, 1)</code>. If <code>'warn'</code> then a <code>RuntimeWarning</code> is thrown. If <code>'raise'</code>, then a <code>RuntimeError</code> is thrown. Otherwise, nothing is thrown and <code>np.nan</code> is returned for the problematic entries; defaults to <code>'warn'</code>.</p> <code>'warn'</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If <code>error</code> is set to <code>'raise'</code>.</p> <p>Returns:</p> Type Description <code>Series</code> <p>Transformed Series.</p> Source code in <code>janitor/math.py</code> <pre><code>@pf.register_series_method\ndef logit(s: \"Series\", error: str = \"warn\") -&gt; \"Series\":\n    \"\"\"Take logit transform of the Series.\n\n    The logit transform is defined:\n\n    ```python\n    logit(p) = log(p/(1-p))\n    ```\n\n    Each value in the series should be between 0 and 1. Use `error` to\n    control the behavior if any series entries are outside of (0, 1).\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; s = pd.Series([0.1, 0.5, 0.9], name=\"numbers\")\n        &gt;&gt;&gt; s.logit()\n        0   -2.197225\n        1    0.000000\n        2    2.197225\n        Name: numbers, dtype: float64\n\n    Args:\n        s: Input Series.\n        error: Determines behavior when `s` is outside of `(0, 1)`.\n            If `'warn'` then a `RuntimeWarning` is thrown. If `'raise'`, then a\n            `RuntimeError` is thrown. Otherwise, nothing is thrown and `np.nan`\n            is returned for the problematic entries; defaults to `'warn'`.\n\n    Raises:\n        RuntimeError: If `error` is set to `'raise'`.\n\n    Returns:\n        Transformed Series.\n    \"\"\"\n    import numpy as np\n    import scipy\n\n    s = s.copy()\n    outside_support = (s &lt;= 0) | (s &gt;= 1)\n    if (outside_support).any():\n        msg = f\"{outside_support.sum()} value(s) are outside of (0, 1)\"\n        if error.lower() == \"warn\":\n            warnings.warn(msg, RuntimeWarning)\n        if error.lower() == \"raise\":\n            raise RuntimeError(msg)\n        else:\n            pass\n    s[outside_support] = np.nan\n    return scipy.special.logit(s)\n</code></pre>"},{"location":"api/math/#janitor.math.normal_cdf","title":"<code>normal_cdf(s)</code>","text":"<p>Transforms the Series via the CDF of the Normal distribution.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; s = pd.Series([-1, 0, 3], name=\"numbers\")\n&gt;&gt;&gt; s.normal_cdf()\n0    0.158655\n1    0.500000\n2    0.998650\ndtype: float64\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>Series</code> <p>Input Series.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Transformed Series.</p> Source code in <code>janitor/math.py</code> <pre><code>@pf.register_series_method\ndef normal_cdf(s: \"Series\") -&gt; \"Series\":\n    \"\"\"Transforms the Series via the CDF of the Normal distribution.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; s = pd.Series([-1, 0, 3], name=\"numbers\")\n        &gt;&gt;&gt; s.normal_cdf()\n        0    0.158655\n        1    0.500000\n        2    0.998650\n        dtype: float64\n\n    Args:\n        s: Input Series.\n\n    Returns:\n        Transformed Series.\n    \"\"\"\n    import pandas as pd\n    import scipy\n\n    return pd.Series(scipy.stats.norm.cdf(s), index=s.index)\n</code></pre>"},{"location":"api/math/#janitor.math.probit","title":"<code>probit(s, error='warn')</code>","text":"<p>Transforms the Series via the inverse CDF of the Normal distribution.</p> <p>Each value in the series should be between 0 and 1. Use <code>error</code> to control the behavior if any series entries are outside of (0, 1).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; s = pd.Series([0.1, 0.5, 0.8], name=\"numbers\")\n&gt;&gt;&gt; s.probit()\n0   -1.281552\n1    0.000000\n2    0.841621\ndtype: float64\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>Series</code> <p>Input Series.</p> required <code>error</code> <code>str</code> <p>Determines behavior when <code>s</code> is outside of <code>(0, 1)</code>. If <code>'warn'</code> then a <code>RuntimeWarning</code> is thrown. If <code>'raise'</code>, then a <code>RuntimeError</code> is thrown. Otherwise, nothing is thrown and <code>np.nan</code> is returned for the problematic entries.</p> <code>'warn'</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>When there are problematic values in the Series and <code>error='raise'</code>.</p> <p>Returns:</p> Type Description <code>Series</code> <p>Transformed Series</p> Source code in <code>janitor/math.py</code> <pre><code>@pf.register_series_method\ndef probit(s: \"Series\", error: str = \"warn\") -&gt; \"Series\":\n    \"\"\"Transforms the Series via the inverse CDF of the Normal distribution.\n\n    Each value in the series should be between 0 and 1. Use `error` to\n    control the behavior if any series entries are outside of (0, 1).\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; s = pd.Series([0.1, 0.5, 0.8], name=\"numbers\")\n        &gt;&gt;&gt; s.probit()\n        0   -1.281552\n        1    0.000000\n        2    0.841621\n        dtype: float64\n\n    Args:\n        s: Input Series.\n        error: Determines behavior when `s` is outside of `(0, 1)`.\n            If `'warn'` then a `RuntimeWarning` is thrown. If `'raise'`, then\n            a `RuntimeError` is thrown. Otherwise, nothing is thrown and\n            `np.nan` is returned for the problematic entries.\n\n    Raises:\n        RuntimeError: When there are problematic values\n            in the Series and `error='raise'`.\n\n    Returns:\n        Transformed Series\n    \"\"\"\n    import numpy as np\n    import pandas as pd\n    import scipy\n\n    s = s.copy()\n    outside_support = (s &lt;= 0) | (s &gt;= 1)\n    if (outside_support).any():\n        msg = f\"{outside_support.sum()} value(s) are outside of (0, 1)\"\n        if error.lower() == \"warn\":\n            warnings.warn(msg, RuntimeWarning)\n        if error.lower() == \"raise\":\n            raise RuntimeError(msg)\n        else:\n            pass\n    s[outside_support] = np.nan\n    with np.errstate(all=\"ignore\"):\n        out = pd.Series(scipy.stats.norm.ppf(s), index=s.index)\n    return out\n</code></pre>"},{"location":"api/math/#janitor.math.sigmoid","title":"<code>sigmoid(s)</code>","text":"<p>Take the sigmoid transform of the series.</p> <p>The sigmoid function is defined:</p> <pre><code>sigmoid(x) = 1 / (1 + exp(-x))\n</code></pre> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; s = pd.Series([-1, 0, 4], name=\"numbers\")\n&gt;&gt;&gt; s.sigmoid()\n0    0.268941\n1    0.500000\n2    0.982014\nName: numbers, dtype: float64\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>Series</code> <p>Input Series.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Transformed Series.</p> Source code in <code>janitor/math.py</code> <pre><code>@pf.register_series_method\ndef sigmoid(s: \"Series\") -&gt; \"Series\":\n    \"\"\"Take the sigmoid transform of the series.\n\n    The sigmoid function is defined:\n\n    ```python\n    sigmoid(x) = 1 / (1 + exp(-x))\n    ```\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; s = pd.Series([-1, 0, 4], name=\"numbers\")\n        &gt;&gt;&gt; s.sigmoid()\n        0    0.268941\n        1    0.500000\n        2    0.982014\n        Name: numbers, dtype: float64\n\n    Args:\n        s: Input Series.\n\n    Returns:\n        Transformed Series.\n    \"\"\"\n    import scipy\n\n    return scipy.special.expit(s)\n</code></pre>"},{"location":"api/math/#janitor.math.softmax","title":"<code>softmax(s)</code>","text":"<p>Take the softmax transform of the series.</p> <p>The softmax function transforms each element of a collection by computing the exponential of each element divided by the sum of the exponentials of all the elements.</p> <p>That is, if x is a one-dimensional numpy array or pandas Series:</p> <pre><code>softmax(x) = exp(x)/sum(exp(x))\n</code></pre> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; s = pd.Series([0, 1, 3], name=\"numbers\")\n&gt;&gt;&gt; s.softmax()\n0    0.042010\n1    0.114195\n2    0.843795\nName: numbers, dtype: float64\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>Series</code> <p>Input Series.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Transformed Series.</p> Source code in <code>janitor/math.py</code> <pre><code>@pf.register_series_method\ndef softmax(s: \"Series\") -&gt; \"Series\":\n    \"\"\"Take the softmax transform of the series.\n\n    The softmax function transforms each element of a collection by\n    computing the exponential of each element divided by the sum of the\n    exponentials of all the elements.\n\n    That is, if x is a one-dimensional numpy array or pandas Series:\n\n    ```python\n    softmax(x) = exp(x)/sum(exp(x))\n    ```\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; s = pd.Series([0, 1, 3], name=\"numbers\")\n        &gt;&gt;&gt; s.softmax()\n        0    0.042010\n        1    0.114195\n        2    0.843795\n        Name: numbers, dtype: float64\n\n    Args:\n        s: Input Series.\n\n    Returns:\n        Transformed Series.\n    \"\"\"\n    import pandas as pd\n    import scipy\n\n    return pd.Series(scipy.special.softmax(s), index=s.index, name=s.name)\n</code></pre>"},{"location":"api/math/#janitor.math.z_score","title":"<code>z_score(s, moments_dict=None, keys=('mean', 'std'))</code>","text":"<p>Transforms the Series into z-scores.</p> <p>The z-score is defined:</p> <pre><code>z = (s - s.mean()) / s.std()\n</code></pre> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor\n&gt;&gt;&gt; s = pd.Series([0, 1, 3], name=\"numbers\")\n&gt;&gt;&gt; s.z_score()\n0   -0.872872\n1   -0.218218\n2    1.091089\nName: numbers, dtype: float64\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>Series</code> <p>Input Series.</p> required <code>moments_dict</code> <code>dict</code> <p>If not <code>None</code>, then the mean and standard deviation used to compute the z-score transformation is saved as entries in <code>moments_dict</code> with keys determined by the <code>keys</code> argument; defaults to <code>None</code>.</p> <code>None</code> <code>keys</code> <code>Tuple[str, str]</code> <p>Determines the keys saved in <code>moments_dict</code> if moments are saved; defaults to (<code>'mean'</code>, <code>'std'</code>).</p> <code>('mean', 'std')</code> <p>Returns:</p> Type Description <code>Series</code> <p>Transformed Series.</p> Source code in <code>janitor/math.py</code> <pre><code>@pf.register_series_method\ndef z_score(\n    s: \"Series\",\n    moments_dict: dict = None,\n    keys: Tuple[str, str] = (\"mean\", \"std\"),\n) -&gt; \"Series\":\n    \"\"\"Transforms the Series into z-scores.\n\n    The z-score is defined:\n\n    ```python\n    z = (s - s.mean()) / s.std()\n    ```\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor\n        &gt;&gt;&gt; s = pd.Series([0, 1, 3], name=\"numbers\")\n        &gt;&gt;&gt; s.z_score()\n        0   -0.872872\n        1   -0.218218\n        2    1.091089\n        Name: numbers, dtype: float64\n\n    Args:\n        s: Input Series.\n        moments_dict: If not `None`, then the mean and standard\n            deviation used to compute the z-score transformation is\n            saved as entries in `moments_dict` with keys determined by\n            the `keys` argument; defaults to `None`.\n        keys: Determines the keys saved in `moments_dict`\n            if moments are saved; defaults to (`'mean'`, `'std'`).\n\n    Returns:\n        Transformed Series.\n    \"\"\"\n    mean = s.mean()\n    std = s.std()\n    if std == 0:\n        return 0\n    if moments_dict is not None:\n        moments_dict[keys[0]] = mean\n        moments_dict[keys[1]] = std\n    return (s - mean) / std\n</code></pre>"},{"location":"api/ml/","title":"Machine Learning","text":"<p>Machine learning specific functions.</p>"},{"location":"api/ml/#janitor.ml.get_features_targets","title":"<code>get_features_targets(df, target_column_names, feature_column_names=None)</code>","text":"<p>Get the features and targets as separate DataFrames/Series.</p> <p>This method does not mutate the original DataFrame.</p> <p>The behaviour is as such:</p> <ul> <li><code>target_column_names</code> is mandatory.</li> <li>If <code>feature_column_names</code> is present, then we will respect the column     names inside there.</li> <li>If <code>feature_column_names</code> is not passed in, then we will assume that the rest of the columns are feature columns, and return them.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor.ml\n&gt;&gt;&gt; df = pd.DataFrame(\n...     {\"a\": [1, 2, 3], \"b\": [-2, 0, 4], \"c\": [1.23, 7.89, 4.56]}\n... )\n&gt;&gt;&gt; X, Y = df.get_features_targets(target_column_names=[\"a\", \"c\"])\n&gt;&gt;&gt; X\n   b\n0 -2\n1  0\n2  4\n&gt;&gt;&gt; Y\n   a     c\n0  1  1.23\n1  2  7.89\n2  3  4.56\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The pandas DataFrame object.</p> required <code>target_column_names</code> <code>Union[str, Union[List, Tuple], Hashable]</code> <p>Either a column name or an iterable (list or tuple) of column names that are the target(s) to be predicted.</p> required <code>feature_column_names</code> <code>Optional[Union[str, Iterable[str], Hashable]]</code> <p>The column name or iterable of column names that are the features (a.k.a. predictors) used to predict the targets.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame]</code> <p><code>(X, Y)</code> the feature matrix (<code>X</code>) and the target matrix (<code>Y</code>). Both are pandas DataFrames.</p> Source code in <code>janitor/ml.py</code> <pre><code>@pf.register_dataframe_method\n@deprecated_alias(\n    target_columns=\"target_column_names\",\n    feature_columns=\"feature_column_names\",\n)\ndef get_features_targets(\n    df: pd.DataFrame,\n    target_column_names: Union[str, Union[List, Tuple], Hashable],\n    feature_column_names: Optional[Union[str, Iterable[str], Hashable]] = None,\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Get the features and targets as separate DataFrames/Series.\n\n    This method does not mutate the original DataFrame.\n\n    The behaviour is as such:\n\n    - `target_column_names` is mandatory.\n    - If `feature_column_names` is present, then we will respect the column\n        names inside there.\n    - If `feature_column_names` is not passed in, then we will assume that\n    the rest of the columns are feature columns, and return them.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor.ml\n        &gt;&gt;&gt; df = pd.DataFrame(\n        ...     {\"a\": [1, 2, 3], \"b\": [-2, 0, 4], \"c\": [1.23, 7.89, 4.56]}\n        ... )\n        &gt;&gt;&gt; X, Y = df.get_features_targets(target_column_names=[\"a\", \"c\"])\n        &gt;&gt;&gt; X\n           b\n        0 -2\n        1  0\n        2  4\n        &gt;&gt;&gt; Y\n           a     c\n        0  1  1.23\n        1  2  7.89\n        2  3  4.56\n\n    Args:\n        df: The pandas DataFrame object.\n        target_column_names: Either a column name or an\n            iterable (list or tuple) of column names that are the target(s) to\n            be predicted.\n        feature_column_names: The column name or\n            iterable of column names that are the features (a.k.a. predictors)\n            used to predict the targets.\n\n    Returns:\n        `(X, Y)` the feature matrix (`X`) and the target matrix (`Y`).\n            Both are pandas DataFrames.\n    \"\"\"\n    Y = df[target_column_names]\n\n    if feature_column_names:\n        X = df[feature_column_names]\n    else:\n        if isinstance(target_column_names, (list, tuple)):  # noqa: W503\n            xcols = [c for c in df.columns if c not in target_column_names]\n        else:\n            xcols = [c for c in df.columns if target_column_names != c]\n\n        X = df[xcols]\n    return X, Y\n</code></pre>"},{"location":"api/polars/","title":"Polars","text":""},{"location":"api/polars/#janitor.polars.clean_names","title":"<code>clean_names</code>","text":"<p>clean_names implementation for polars.</p>"},{"location":"api/polars/#janitor.polars.clean_names.clean_names","title":"<code>clean_names(df, strip_underscores=None, case_type='lower', remove_special=False, strip_accents=False, truncate_limit=None)</code>","text":"<p>Clean the column names in a polars DataFrame.</p> <p><code>clean_names</code> can also be applied to a LazyFrame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; import janitor.polars\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"Aloha\": range(3),\n...         \"Bell Chart\": range(3),\n...         \"Animals@#$%^\": range(3)\n...     }\n... )\n&gt;&gt;&gt; df\nshape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Aloha \u2506 Bell Chart \u2506 Animals@#$%^ \u2502\n\u2502 ---   \u2506 ---        \u2506 ---          \u2502\n\u2502 i64   \u2506 i64        \u2506 i64          \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0     \u2506 0          \u2506 0            \u2502\n\u2502 1     \u2506 1          \u2506 1            \u2502\n\u2502 2     \u2506 2          \u2506 2            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n&gt;&gt;&gt; df.clean_names(remove_special=True)\nshape: (3, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 aloha \u2506 bell_chart \u2506 animals \u2502\n\u2502 ---   \u2506 ---        \u2506 ---     \u2502\n\u2502 i64   \u2506 i64        \u2506 i64     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 0     \u2506 0          \u2506 0       \u2502\n\u2502 1     \u2506 1          \u2506 1       \u2502\n\u2502 2     \u2506 2          \u2506 2       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>New in version 0.28.0</p> <p>Parameters:</p> Name Type Description Default <code>strip_underscores</code> <code>str | bool</code> <p>Removes the outer underscores from all column names. Default None keeps outer underscores. Values can be either 'left', 'right' or 'both' or the respective shorthand 'l', 'r' and True.</p> <code>None</code> <code>case_type</code> <code>str</code> <p>Whether to make the column names lower or uppercase. Current case may be preserved with 'preserve', while snake case conversion (from CamelCase or camelCase only) can be turned on using \"snake\". Default 'lower' makes all characters lowercase.</p> <code>'lower'</code> <code>remove_special</code> <code>bool</code> <p>Remove special characters from the column names. Only letters, numbers and underscores are preserved.</p> <code>False</code> <code>strip_accents</code> <code>bool</code> <p>Whether or not to remove accents from the labels.</p> <code>False</code> <code>truncate_limit</code> <code>int</code> <p>Truncates formatted column names to the specified length. Default None does not truncate.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame | LazyFrame</code> <p>A polars DataFrame/LazyFrame.</p> Source code in <code>janitor/polars/clean_names.py</code> <pre><code>@register_lazyframe_method\n@register_dataframe_method\ndef clean_names(\n    df: pl.DataFrame | pl.LazyFrame,\n    strip_underscores: str | bool = None,\n    case_type: str = \"lower\",\n    remove_special: bool = False,\n    strip_accents: bool = False,\n    truncate_limit: int = None,\n) -&gt; pl.DataFrame | pl.LazyFrame:\n    \"\"\"\n    Clean the column names in a polars DataFrame.\n\n    `clean_names` can also be applied to a LazyFrame.\n\n    Examples:\n        &gt;&gt;&gt; import polars as pl\n        &gt;&gt;&gt; import janitor.polars\n        &gt;&gt;&gt; df = pl.DataFrame(\n        ...     {\n        ...         \"Aloha\": range(3),\n        ...         \"Bell Chart\": range(3),\n        ...         \"Animals@#$%^\": range(3)\n        ...     }\n        ... )\n        &gt;&gt;&gt; df\n        shape: (3, 3)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 Aloha \u2506 Bell Chart \u2506 Animals@#$%^ \u2502\n        \u2502 ---   \u2506 ---        \u2506 ---          \u2502\n        \u2502 i64   \u2506 i64        \u2506 i64          \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 0     \u2506 0          \u2506 0            \u2502\n        \u2502 1     \u2506 1          \u2506 1            \u2502\n        \u2502 2     \u2506 2          \u2506 2            \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        &gt;&gt;&gt; df.clean_names(remove_special=True)\n        shape: (3, 3)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 aloha \u2506 bell_chart \u2506 animals \u2502\n        \u2502 ---   \u2506 ---        \u2506 ---     \u2502\n        \u2502 i64   \u2506 i64        \u2506 i64     \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 0     \u2506 0          \u2506 0       \u2502\n        \u2502 1     \u2506 1          \u2506 1       \u2502\n        \u2502 2     \u2506 2          \u2506 2       \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n    !!! info \"New in version 0.28.0\"\n\n    Args:\n        strip_underscores: Removes the outer underscores from all\n            column names. Default None keeps outer underscores. Values can be\n            either 'left', 'right' or 'both' or the respective shorthand 'l',\n            'r' and True.\n        case_type: Whether to make the column names lower or uppercase.\n            Current case may be preserved with 'preserve',\n            while snake case conversion (from CamelCase or camelCase only)\n            can be turned on using \"snake\".\n            Default 'lower' makes all characters lowercase.\n        remove_special: Remove special characters from the column names.\n            Only letters, numbers and underscores are preserved.\n        strip_accents: Whether or not to remove accents from\n            the labels.\n        truncate_limit: Truncates formatted column names to\n            the specified length. Default None does not truncate.\n\n    Returns:\n        A polars DataFrame/LazyFrame.\n    \"\"\"  # noqa: E501\n    return df.rename(\n        lambda col: _clean_column_names(\n            obj=col,\n            strip_accents=strip_accents,\n            strip_underscores=strip_underscores,\n            case_type=case_type,\n            remove_special=remove_special,\n            truncate_limit=truncate_limit,\n        )\n    )\n</code></pre>"},{"location":"api/polars/#janitor.polars.clean_names.make_clean_names","title":"<code>make_clean_names(expression, strip_underscores=None, case_type='lower', remove_special=False, strip_accents=False, enforce_string=False, truncate_limit=None)</code>","text":"<p>Clean the labels in a polars Expression.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; import janitor.polars\n&gt;&gt;&gt; df = pl.DataFrame({\"raw\": [\"Ab\u00e7d\u00ea fg\u00ed j\"]})\n&gt;&gt;&gt; df\nshape: (1, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 raw         \u2502\n\u2502 ---         \u2502\n\u2502 str         \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Ab\u00e7d\u00ea fg\u00ed j \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Clean the column values:</p> <pre><code>&gt;&gt;&gt; df.with_columns(pl.col(\"raw\").make_clean_names(strip_accents=True))\nshape: (1, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 raw         \u2502\n\u2502 ---         \u2502\n\u2502 str         \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 abcde_fgi_j \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>New in version 0.28.0</p> <p>Parameters:</p> Name Type Description Default <code>strip_underscores</code> <code>str | bool</code> <p>Removes the outer underscores from all labels in the expression. Default None keeps outer underscores. Values can be either 'left', 'right' or 'both' or the respective shorthand 'l', 'r' and True.</p> <code>None</code> <code>case_type</code> <code>str</code> <p>Whether to make the labels in the expression lower or uppercase. Current case may be preserved with 'preserve', while snake case conversion (from CamelCase or camelCase only) can be turned on using \"snake\". Default 'lower' makes all characters lowercase.</p> <code>'lower'</code> <code>remove_special</code> <code>bool</code> <p>Remove special characters from the values in the expression. Only letters, numbers and underscores are preserved.</p> <code>False</code> <code>strip_accents</code> <code>bool</code> <p>Whether or not to remove accents from the expression.</p> <code>False</code> <code>enforce_string</code> <code>bool</code> <p>Whether or not to cast the expression to a string type.</p> <code>False</code> <code>truncate_limit</code> <code>int</code> <p>Truncates formatted labels in the expression to the specified length. Default None does not truncate.</p> <code>None</code> <p>Returns:</p> Type Description <code>Expr</code> <p>A polars Expression.</p> Source code in <code>janitor/polars/clean_names.py</code> <pre><code>@register_expr_method\ndef make_clean_names(\n    expression,\n    strip_underscores: str | bool = None,\n    case_type: str = \"lower\",\n    remove_special: bool = False,\n    strip_accents: bool = False,\n    enforce_string: bool = False,\n    truncate_limit: int = None,\n) -&gt; pl.Expr:\n    \"\"\"\n    Clean the labels in a polars Expression.\n\n    Examples:\n        &gt;&gt;&gt; import polars as pl\n        &gt;&gt;&gt; import janitor.polars\n        &gt;&gt;&gt; df = pl.DataFrame({\"raw\": [\"Ab\u00e7d\u00ea fg\u00ed j\"]})\n        &gt;&gt;&gt; df\n        shape: (1, 1)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 raw         \u2502\n        \u2502 ---         \u2502\n        \u2502 str         \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 Ab\u00e7d\u00ea fg\u00ed j \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n        Clean the column values:\n        &gt;&gt;&gt; df.with_columns(pl.col(\"raw\").make_clean_names(strip_accents=True))\n        shape: (1, 1)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 raw         \u2502\n        \u2502 ---         \u2502\n        \u2502 str         \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 abcde_fgi_j \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n    !!! info \"New in version 0.28.0\"\n\n    Args:\n        strip_underscores: Removes the outer underscores\n            from all labels in the expression.\n            Default None keeps outer underscores.\n            Values can be either 'left', 'right'\n            or 'both' or the respective shorthand 'l',\n            'r' and True.\n        case_type: Whether to make the labels in the expression lower or uppercase.\n            Current case may be preserved with 'preserve',\n            while snake case conversion (from CamelCase or camelCase only)\n            can be turned on using \"snake\".\n            Default 'lower' makes all characters lowercase.\n        remove_special: Remove special characters from the values in the expression.\n            Only letters, numbers and underscores are preserved.\n        strip_accents: Whether or not to remove accents from\n            the expression.\n        enforce_string: Whether or not to cast the expression to a string type.\n        truncate_limit: Truncates formatted labels in the expression to\n            the specified length. Default None does not truncate.\n\n    Returns:\n        A polars Expression.\n    \"\"\"\n    return _clean_expr_names(\n        obj=expression,\n        strip_accents=strip_accents,\n        strip_underscores=strip_underscores,\n        case_type=case_type,\n        remove_special=remove_special,\n        enforce_string=enforce_string,\n        truncate_limit=truncate_limit,\n    )\n</code></pre>"},{"location":"api/polars/#janitor.polars.complete","title":"<code>complete</code>","text":"<p>complete implementation for polars.</p>"},{"location":"api/polars/#janitor.polars.complete.complete","title":"<code>complete(df, *columns, fill_value=None, explicit=True, sort=False, by=None)</code>","text":"<p>Turns implicit missing values into explicit missing values</p> <p>It is modeled after tidyr's <code>complete</code> function. In a way, it is the inverse of <code>pl.drop_nulls</code>, as it exposes implicitly missing rows.</p> <p>If new values need to be introduced, a polars Expression or a polars Series with the new values can be passed, as long as the polars Expression/Series has a name that already exists in the DataFrame.</p> <p><code>complete</code> can also be applied to a LazyFrame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; import janitor.polars\n&gt;&gt;&gt; df = pl.DataFrame(\n...     dict(\n...         group=(1, 2, 1, 2),\n...         item_id=(1, 2, 2, 3),\n...         item_name=(\"a\", \"a\", \"b\", \"b\"),\n...         value1=(1, None, 3, 4),\n...         value2=range(4, 8),\n...     )\n... )\n&gt;&gt;&gt; df\nshape: (4, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 group \u2506 item_id \u2506 item_name \u2506 value1 \u2506 value2 \u2502\n\u2502 ---   \u2506 ---     \u2506 ---       \u2506 ---    \u2506 ---    \u2502\n\u2502 i64   \u2506 i64     \u2506 str       \u2506 i64    \u2506 i64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1     \u2506 1       \u2506 a         \u2506 1      \u2506 4      \u2502\n\u2502 2     \u2506 2       \u2506 a         \u2506 null   \u2506 5      \u2502\n\u2502 1     \u2506 2       \u2506 b         \u2506 3      \u2506 6      \u2502\n\u2502 2     \u2506 3       \u2506 b         \u2506 4      \u2506 7      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Generate all possible combinations of <code>group</code>, <code>item_id</code>, and <code>item_name</code> (whether or not they appear in the data)</p> <pre><code>&gt;&gt;&gt; with pl.Config(tbl_rows=-1):\n...     df.complete(\"group\", \"item_id\", \"item_name\", sort=True)\nshape: (12, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 group \u2506 item_id \u2506 item_name \u2506 value1 \u2506 value2 \u2502\n\u2502 ---   \u2506 ---     \u2506 ---       \u2506 ---    \u2506 ---    \u2502\n\u2502 i64   \u2506 i64     \u2506 str       \u2506 i64    \u2506 i64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1     \u2506 1       \u2506 a         \u2506 1      \u2506 4      \u2502\n\u2502 1     \u2506 1       \u2506 b         \u2506 null   \u2506 null   \u2502\n\u2502 1     \u2506 2       \u2506 a         \u2506 null   \u2506 null   \u2502\n\u2502 1     \u2506 2       \u2506 b         \u2506 3      \u2506 6      \u2502\n\u2502 1     \u2506 3       \u2506 a         \u2506 null   \u2506 null   \u2502\n\u2502 1     \u2506 3       \u2506 b         \u2506 null   \u2506 null   \u2502\n\u2502 2     \u2506 1       \u2506 a         \u2506 null   \u2506 null   \u2502\n\u2502 2     \u2506 1       \u2506 b         \u2506 null   \u2506 null   \u2502\n\u2502 2     \u2506 2       \u2506 a         \u2506 null   \u2506 5      \u2502\n\u2502 2     \u2506 2       \u2506 b         \u2506 null   \u2506 null   \u2502\n\u2502 2     \u2506 3       \u2506 a         \u2506 null   \u2506 null   \u2502\n\u2502 2     \u2506 3       \u2506 b         \u2506 4      \u2506 7      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Cross all possible <code>group</code> values with the unique pairs of <code>(item_id, item_name)</code> that already exist in the data.</p> <pre><code>&gt;&gt;&gt; with pl.Config(tbl_rows=-1):\n...     df.select(\n...         \"group\", pl.struct(\"item_id\", \"item_name\"), \"value1\", \"value2\"\n...     ).complete(\"group\", \"item_id\", sort=True).unnest(\"item_id\")\nshape: (8, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 group \u2506 item_id \u2506 item_name \u2506 value1 \u2506 value2 \u2502\n\u2502 ---   \u2506 ---     \u2506 ---       \u2506 ---    \u2506 ---    \u2502\n\u2502 i64   \u2506 i64     \u2506 str       \u2506 i64    \u2506 i64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1     \u2506 1       \u2506 a         \u2506 1      \u2506 4      \u2502\n\u2502 1     \u2506 2       \u2506 a         \u2506 null   \u2506 null   \u2502\n\u2502 1     \u2506 2       \u2506 b         \u2506 3      \u2506 6      \u2502\n\u2502 1     \u2506 3       \u2506 b         \u2506 null   \u2506 null   \u2502\n\u2502 2     \u2506 1       \u2506 a         \u2506 null   \u2506 null   \u2502\n\u2502 2     \u2506 2       \u2506 a         \u2506 null   \u2506 5      \u2502\n\u2502 2     \u2506 2       \u2506 b         \u2506 null   \u2506 null   \u2502\n\u2502 2     \u2506 3       \u2506 b         \u2506 4      \u2506 7      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Fill in nulls:</p> <pre><code>&gt;&gt;&gt; with pl.Config(tbl_rows=-1):\n...     df.select(\n...         \"group\", pl.struct(\"item_id\", \"item_name\"), \"value1\", \"value2\"\n...     ).complete(\n...         \"group\",\n...         \"item_id\",\n...         fill_value={\"value1\": 0, \"value2\": 99},\n...         explicit=True,\n...         sort=True,\n...     ).unnest(\"item_id\")\nshape: (8, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 group \u2506 item_id \u2506 item_name \u2506 value1 \u2506 value2 \u2502\n\u2502 ---   \u2506 ---     \u2506 ---       \u2506 ---    \u2506 ---    \u2502\n\u2502 i64   \u2506 i64     \u2506 str       \u2506 i64    \u2506 i64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1     \u2506 1       \u2506 a         \u2506 1      \u2506 4      \u2502\n\u2502 1     \u2506 2       \u2506 a         \u2506 0      \u2506 99     \u2502\n\u2502 1     \u2506 2       \u2506 b         \u2506 3      \u2506 6      \u2502\n\u2502 1     \u2506 3       \u2506 b         \u2506 0      \u2506 99     \u2502\n\u2502 2     \u2506 1       \u2506 a         \u2506 0      \u2506 99     \u2502\n\u2502 2     \u2506 2       \u2506 a         \u2506 0      \u2506 5      \u2502\n\u2502 2     \u2506 2       \u2506 b         \u2506 0      \u2506 99     \u2502\n\u2502 2     \u2506 3       \u2506 b         \u2506 4      \u2506 7      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Limit the fill to only the newly created missing values with <code>explicit = FALSE</code>:</p> <pre><code>&gt;&gt;&gt; with pl.Config(tbl_rows=-1):\n...     df.select(\n...         \"group\", pl.struct(\"item_id\", \"item_name\"), \"value1\", \"value2\"\n...     ).complete(\n...         \"group\",\n...         \"item_id\",\n...         fill_value={\"value1\": 0, \"value2\": 99},\n...         explicit=False,\n...         sort=True,\n...     ).unnest(\"item_id\").sort(pl.all())\nshape: (8, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 group \u2506 item_id \u2506 item_name \u2506 value1 \u2506 value2 \u2502\n\u2502 ---   \u2506 ---     \u2506 ---       \u2506 ---    \u2506 ---    \u2502\n\u2502 i64   \u2506 i64     \u2506 str       \u2506 i64    \u2506 i64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1     \u2506 1       \u2506 a         \u2506 1      \u2506 4      \u2502\n\u2502 1     \u2506 2       \u2506 a         \u2506 0      \u2506 99     \u2502\n\u2502 1     \u2506 2       \u2506 b         \u2506 3      \u2506 6      \u2502\n\u2502 1     \u2506 3       \u2506 b         \u2506 0      \u2506 99     \u2502\n\u2502 2     \u2506 1       \u2506 a         \u2506 0      \u2506 99     \u2502\n\u2502 2     \u2506 2       \u2506 a         \u2506 null   \u2506 5      \u2502\n\u2502 2     \u2506 2       \u2506 b         \u2506 0      \u2506 99     \u2502\n\u2502 2     \u2506 3       \u2506 b         \u2506 4      \u2506 7      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <pre><code>&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"Year\": [1999, 2000, 2004, 1999, 2004],\n...         \"Taxon\": [\n...             \"Saccharina\",\n...             \"Saccharina\",\n...             \"Saccharina\",\n...             \"Agarum\",\n...             \"Agarum\",\n...         ],\n...         \"Abundance\": [4, 5, 2, 1, 8],\n...     }\n... )\n&gt;&gt;&gt; df\nshape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Year \u2506 Taxon      \u2506 Abundance \u2502\n\u2502 ---  \u2506 ---        \u2506 ---       \u2502\n\u2502 i64  \u2506 str        \u2506 i64       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1999 \u2506 Saccharina \u2506 4         \u2502\n\u2502 2000 \u2506 Saccharina \u2506 5         \u2502\n\u2502 2004 \u2506 Saccharina \u2506 2         \u2502\n\u2502 1999 \u2506 Agarum     \u2506 1         \u2502\n\u2502 2004 \u2506 Agarum     \u2506 8         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Expose missing years from 1999 to 2004 - pass a polars expression with the new dates, and ensure the expression's name already exists in the DataFrame:</p> <pre><code>&gt;&gt;&gt; expression = pl.int_range(1999,2005).alias('Year')\n&gt;&gt;&gt; with pl.Config(tbl_rows=-1):\n...     df.complete(expression,'Taxon',sort=True)\nshape: (12, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Year \u2506 Taxon      \u2506 Abundance \u2502\n\u2502 ---  \u2506 ---        \u2506 ---       \u2502\n\u2502 i64  \u2506 str        \u2506 i64       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1999 \u2506 Agarum     \u2506 1         \u2502\n\u2502 1999 \u2506 Saccharina \u2506 4         \u2502\n\u2502 2000 \u2506 Agarum     \u2506 null      \u2502\n\u2502 2000 \u2506 Saccharina \u2506 5         \u2502\n\u2502 2001 \u2506 Agarum     \u2506 null      \u2502\n\u2502 2001 \u2506 Saccharina \u2506 null      \u2502\n\u2502 2002 \u2506 Agarum     \u2506 null      \u2502\n\u2502 2002 \u2506 Saccharina \u2506 null      \u2502\n\u2502 2003 \u2506 Agarum     \u2506 null      \u2502\n\u2502 2003 \u2506 Saccharina \u2506 null      \u2502\n\u2502 2004 \u2506 Agarum     \u2506 8         \u2502\n\u2502 2004 \u2506 Saccharina \u2506 2         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Expose missing rows per group:</p> <pre><code>&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"state\": [\"CA\", \"CA\", \"HI\", \"HI\", \"HI\", \"NY\", \"NY\"],\n...         \"year\": [2010, 2013, 2010, 2012, 2016, 2009, 2013],\n...         \"value\": [1, 3, 1, 2, 3, 2, 5],\n...     }\n... )\n&gt;&gt;&gt; df\nshape: (7, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 state \u2506 year \u2506 value \u2502\n\u2502 ---   \u2506 ---  \u2506 ---   \u2502\n\u2502 str   \u2506 i64  \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 CA    \u2506 2010 \u2506 1     \u2502\n\u2502 CA    \u2506 2013 \u2506 3     \u2502\n\u2502 HI    \u2506 2010 \u2506 1     \u2502\n\u2502 HI    \u2506 2012 \u2506 2     \u2502\n\u2502 HI    \u2506 2016 \u2506 3     \u2502\n\u2502 NY    \u2506 2009 \u2506 2     \u2502\n\u2502 NY    \u2506 2013 \u2506 5     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n&gt;&gt;&gt; low = pl.col('year').min()\n&gt;&gt;&gt; high = pl.col('year').max().add(1)\n&gt;&gt;&gt; new_year_values=pl.int_range(low,high).alias('year')\n&gt;&gt;&gt; with pl.Config(tbl_rows=-1):\n...     df.complete(new_year_values,by='state',sort=True)\nshape: (16, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 state \u2506 year \u2506 value \u2502\n\u2502 ---   \u2506 ---  \u2506 ---   \u2502\n\u2502 str   \u2506 i64  \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 CA    \u2506 2010 \u2506 1     \u2502\n\u2502 CA    \u2506 2011 \u2506 null  \u2502\n\u2502 CA    \u2506 2012 \u2506 null  \u2502\n\u2502 CA    \u2506 2013 \u2506 3     \u2502\n\u2502 HI    \u2506 2010 \u2506 1     \u2502\n\u2502 HI    \u2506 2011 \u2506 null  \u2502\n\u2502 HI    \u2506 2012 \u2506 2     \u2502\n\u2502 HI    \u2506 2013 \u2506 null  \u2502\n\u2502 HI    \u2506 2014 \u2506 null  \u2502\n\u2502 HI    \u2506 2015 \u2506 null  \u2502\n\u2502 HI    \u2506 2016 \u2506 3     \u2502\n\u2502 NY    \u2506 2009 \u2506 2     \u2502\n\u2502 NY    \u2506 2010 \u2506 null  \u2502\n\u2502 NY    \u2506 2011 \u2506 null  \u2502\n\u2502 NY    \u2506 2012 \u2506 null  \u2502\n\u2502 NY    \u2506 2013 \u2506 5     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>New in version 0.28.0</p> <p>Parameters:</p> Name Type Description Default <code>*columns</code> <code>ColumnNameOrSelector</code> <p>This refers to the columns to be completed. It can be a string or a column selector or a polars expression. A polars expression can be used to introduced new values, as long as the polars expression has a name that already exists in the DataFrame.</p> <code>()</code> <code>fill_value</code> <code>dict | Any | Expr</code> <p>Scalar value or polars expression to use instead of nulls for missing combinations. A dictionary, mapping columns names to a scalar value is also accepted.</p> <code>None</code> <code>explicit</code> <code>bool</code> <p>Determines if only implicitly missing values should be filled (<code>False</code>), or all nulls existing in the LazyFrame (<code>True</code>). <code>explicit</code> is applicable only if <code>fill_value</code> is not <code>None</code>.</p> <code>True</code> <code>sort</code> <code>bool</code> <p>Sort the DataFrame based on *columns.</p> <code>False</code> <code>by</code> <code>ColumnNameOrSelector</code> <p>Column(s) to group by. The explicit missing rows are returned per group.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame | LazyFrame</code> <p>A polars DataFrame/LazyFrame.</p> Source code in <code>janitor/polars/complete.py</code> <pre><code>@register_lazyframe_method\n@register_dataframe_method\ndef complete(\n    df: pl.DataFrame | pl.LazyFrame,\n    *columns: ColumnNameOrSelector,\n    fill_value: dict | Any | pl.Expr = None,\n    explicit: bool = True,\n    sort: bool = False,\n    by: ColumnNameOrSelector = None,\n) -&gt; pl.DataFrame | pl.LazyFrame:\n    \"\"\"\n    Turns implicit missing values into explicit missing values\n\n    It is modeled after tidyr's `complete` function.\n    In a way, it is the inverse of `pl.drop_nulls`,\n    as it exposes implicitly missing rows.\n\n    If new values need to be introduced, a polars Expression\n    or a polars Series with the new values can be passed,\n    as long as the polars Expression/Series\n    has a name that already exists in the DataFrame.\n\n    `complete` can also be applied to a LazyFrame.\n\n    Examples:\n        &gt;&gt;&gt; import polars as pl\n        &gt;&gt;&gt; import janitor.polars\n        &gt;&gt;&gt; df = pl.DataFrame(\n        ...     dict(\n        ...         group=(1, 2, 1, 2),\n        ...         item_id=(1, 2, 2, 3),\n        ...         item_name=(\"a\", \"a\", \"b\", \"b\"),\n        ...         value1=(1, None, 3, 4),\n        ...         value2=range(4, 8),\n        ...     )\n        ... )\n        &gt;&gt;&gt; df\n        shape: (4, 5)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 group \u2506 item_id \u2506 item_name \u2506 value1 \u2506 value2 \u2502\n        \u2502 ---   \u2506 ---     \u2506 ---       \u2506 ---    \u2506 ---    \u2502\n        \u2502 i64   \u2506 i64     \u2506 str       \u2506 i64    \u2506 i64    \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 1     \u2506 1       \u2506 a         \u2506 1      \u2506 4      \u2502\n        \u2502 2     \u2506 2       \u2506 a         \u2506 null   \u2506 5      \u2502\n        \u2502 1     \u2506 2       \u2506 b         \u2506 3      \u2506 6      \u2502\n        \u2502 2     \u2506 3       \u2506 b         \u2506 4      \u2506 7      \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n        Generate all possible combinations of\n        `group`, `item_id`, and `item_name`\n        (whether or not they appear in the data)\n        &gt;&gt;&gt; with pl.Config(tbl_rows=-1):\n        ...     df.complete(\"group\", \"item_id\", \"item_name\", sort=True)\n        shape: (12, 5)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 group \u2506 item_id \u2506 item_name \u2506 value1 \u2506 value2 \u2502\n        \u2502 ---   \u2506 ---     \u2506 ---       \u2506 ---    \u2506 ---    \u2502\n        \u2502 i64   \u2506 i64     \u2506 str       \u2506 i64    \u2506 i64    \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 1     \u2506 1       \u2506 a         \u2506 1      \u2506 4      \u2502\n        \u2502 1     \u2506 1       \u2506 b         \u2506 null   \u2506 null   \u2502\n        \u2502 1     \u2506 2       \u2506 a         \u2506 null   \u2506 null   \u2502\n        \u2502 1     \u2506 2       \u2506 b         \u2506 3      \u2506 6      \u2502\n        \u2502 1     \u2506 3       \u2506 a         \u2506 null   \u2506 null   \u2502\n        \u2502 1     \u2506 3       \u2506 b         \u2506 null   \u2506 null   \u2502\n        \u2502 2     \u2506 1       \u2506 a         \u2506 null   \u2506 null   \u2502\n        \u2502 2     \u2506 1       \u2506 b         \u2506 null   \u2506 null   \u2502\n        \u2502 2     \u2506 2       \u2506 a         \u2506 null   \u2506 5      \u2502\n        \u2502 2     \u2506 2       \u2506 b         \u2506 null   \u2506 null   \u2502\n        \u2502 2     \u2506 3       \u2506 a         \u2506 null   \u2506 null   \u2502\n        \u2502 2     \u2506 3       \u2506 b         \u2506 4      \u2506 7      \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n        Cross all possible `group` values with the unique pairs of\n        `(item_id, item_name)` that already exist in the data.\n        &gt;&gt;&gt; with pl.Config(tbl_rows=-1):\n        ...     df.select(\n        ...         \"group\", pl.struct(\"item_id\", \"item_name\"), \"value1\", \"value2\"\n        ...     ).complete(\"group\", \"item_id\", sort=True).unnest(\"item_id\")\n        shape: (8, 5)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 group \u2506 item_id \u2506 item_name \u2506 value1 \u2506 value2 \u2502\n        \u2502 ---   \u2506 ---     \u2506 ---       \u2506 ---    \u2506 ---    \u2502\n        \u2502 i64   \u2506 i64     \u2506 str       \u2506 i64    \u2506 i64    \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 1     \u2506 1       \u2506 a         \u2506 1      \u2506 4      \u2502\n        \u2502 1     \u2506 2       \u2506 a         \u2506 null   \u2506 null   \u2502\n        \u2502 1     \u2506 2       \u2506 b         \u2506 3      \u2506 6      \u2502\n        \u2502 1     \u2506 3       \u2506 b         \u2506 null   \u2506 null   \u2502\n        \u2502 2     \u2506 1       \u2506 a         \u2506 null   \u2506 null   \u2502\n        \u2502 2     \u2506 2       \u2506 a         \u2506 null   \u2506 5      \u2502\n        \u2502 2     \u2506 2       \u2506 b         \u2506 null   \u2506 null   \u2502\n        \u2502 2     \u2506 3       \u2506 b         \u2506 4      \u2506 7      \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n        Fill in nulls:\n        &gt;&gt;&gt; with pl.Config(tbl_rows=-1):\n        ...     df.select(\n        ...         \"group\", pl.struct(\"item_id\", \"item_name\"), \"value1\", \"value2\"\n        ...     ).complete(\n        ...         \"group\",\n        ...         \"item_id\",\n        ...         fill_value={\"value1\": 0, \"value2\": 99},\n        ...         explicit=True,\n        ...         sort=True,\n        ...     ).unnest(\"item_id\")\n        shape: (8, 5)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 group \u2506 item_id \u2506 item_name \u2506 value1 \u2506 value2 \u2502\n        \u2502 ---   \u2506 ---     \u2506 ---       \u2506 ---    \u2506 ---    \u2502\n        \u2502 i64   \u2506 i64     \u2506 str       \u2506 i64    \u2506 i64    \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 1     \u2506 1       \u2506 a         \u2506 1      \u2506 4      \u2502\n        \u2502 1     \u2506 2       \u2506 a         \u2506 0      \u2506 99     \u2502\n        \u2502 1     \u2506 2       \u2506 b         \u2506 3      \u2506 6      \u2502\n        \u2502 1     \u2506 3       \u2506 b         \u2506 0      \u2506 99     \u2502\n        \u2502 2     \u2506 1       \u2506 a         \u2506 0      \u2506 99     \u2502\n        \u2502 2     \u2506 2       \u2506 a         \u2506 0      \u2506 5      \u2502\n        \u2502 2     \u2506 2       \u2506 b         \u2506 0      \u2506 99     \u2502\n        \u2502 2     \u2506 3       \u2506 b         \u2506 4      \u2506 7      \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n        Limit the fill to only the newly created\n        missing values with `explicit = FALSE`:\n        &gt;&gt;&gt; with pl.Config(tbl_rows=-1):\n        ...     df.select(\n        ...         \"group\", pl.struct(\"item_id\", \"item_name\"), \"value1\", \"value2\"\n        ...     ).complete(\n        ...         \"group\",\n        ...         \"item_id\",\n        ...         fill_value={\"value1\": 0, \"value2\": 99},\n        ...         explicit=False,\n        ...         sort=True,\n        ...     ).unnest(\"item_id\").sort(pl.all())\n        shape: (8, 5)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 group \u2506 item_id \u2506 item_name \u2506 value1 \u2506 value2 \u2502\n        \u2502 ---   \u2506 ---     \u2506 ---       \u2506 ---    \u2506 ---    \u2502\n        \u2502 i64   \u2506 i64     \u2506 str       \u2506 i64    \u2506 i64    \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 1     \u2506 1       \u2506 a         \u2506 1      \u2506 4      \u2502\n        \u2502 1     \u2506 2       \u2506 a         \u2506 0      \u2506 99     \u2502\n        \u2502 1     \u2506 2       \u2506 b         \u2506 3      \u2506 6      \u2502\n        \u2502 1     \u2506 3       \u2506 b         \u2506 0      \u2506 99     \u2502\n        \u2502 2     \u2506 1       \u2506 a         \u2506 0      \u2506 99     \u2502\n        \u2502 2     \u2506 2       \u2506 a         \u2506 null   \u2506 5      \u2502\n        \u2502 2     \u2506 2       \u2506 b         \u2506 0      \u2506 99     \u2502\n        \u2502 2     \u2506 3       \u2506 b         \u2506 4      \u2506 7      \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n        &gt;&gt;&gt; df = pl.DataFrame(\n        ...     {\n        ...         \"Year\": [1999, 2000, 2004, 1999, 2004],\n        ...         \"Taxon\": [\n        ...             \"Saccharina\",\n        ...             \"Saccharina\",\n        ...             \"Saccharina\",\n        ...             \"Agarum\",\n        ...             \"Agarum\",\n        ...         ],\n        ...         \"Abundance\": [4, 5, 2, 1, 8],\n        ...     }\n        ... )\n        &gt;&gt;&gt; df\n        shape: (5, 3)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 Year \u2506 Taxon      \u2506 Abundance \u2502\n        \u2502 ---  \u2506 ---        \u2506 ---       \u2502\n        \u2502 i64  \u2506 str        \u2506 i64       \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 1999 \u2506 Saccharina \u2506 4         \u2502\n        \u2502 2000 \u2506 Saccharina \u2506 5         \u2502\n        \u2502 2004 \u2506 Saccharina \u2506 2         \u2502\n        \u2502 1999 \u2506 Agarum     \u2506 1         \u2502\n        \u2502 2004 \u2506 Agarum     \u2506 8         \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n        Expose missing years from 1999 to 2004 -\n        pass a polars expression with the new dates,\n        and ensure the expression's name already exists\n        in the DataFrame:\n        &gt;&gt;&gt; expression = pl.int_range(1999,2005).alias('Year')\n        &gt;&gt;&gt; with pl.Config(tbl_rows=-1):\n        ...     df.complete(expression,'Taxon',sort=True)\n        shape: (12, 3)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 Year \u2506 Taxon      \u2506 Abundance \u2502\n        \u2502 ---  \u2506 ---        \u2506 ---       \u2502\n        \u2502 i64  \u2506 str        \u2506 i64       \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 1999 \u2506 Agarum     \u2506 1         \u2502\n        \u2502 1999 \u2506 Saccharina \u2506 4         \u2502\n        \u2502 2000 \u2506 Agarum     \u2506 null      \u2502\n        \u2502 2000 \u2506 Saccharina \u2506 5         \u2502\n        \u2502 2001 \u2506 Agarum     \u2506 null      \u2502\n        \u2502 2001 \u2506 Saccharina \u2506 null      \u2502\n        \u2502 2002 \u2506 Agarum     \u2506 null      \u2502\n        \u2502 2002 \u2506 Saccharina \u2506 null      \u2502\n        \u2502 2003 \u2506 Agarum     \u2506 null      \u2502\n        \u2502 2003 \u2506 Saccharina \u2506 null      \u2502\n        \u2502 2004 \u2506 Agarum     \u2506 8         \u2502\n        \u2502 2004 \u2506 Saccharina \u2506 2         \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n        Expose missing rows per group:\n        &gt;&gt;&gt; df = pl.DataFrame(\n        ...     {\n        ...         \"state\": [\"CA\", \"CA\", \"HI\", \"HI\", \"HI\", \"NY\", \"NY\"],\n        ...         \"year\": [2010, 2013, 2010, 2012, 2016, 2009, 2013],\n        ...         \"value\": [1, 3, 1, 2, 3, 2, 5],\n        ...     }\n        ... )\n        &gt;&gt;&gt; df\n        shape: (7, 3)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 state \u2506 year \u2506 value \u2502\n        \u2502 ---   \u2506 ---  \u2506 ---   \u2502\n        \u2502 str   \u2506 i64  \u2506 i64   \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 CA    \u2506 2010 \u2506 1     \u2502\n        \u2502 CA    \u2506 2013 \u2506 3     \u2502\n        \u2502 HI    \u2506 2010 \u2506 1     \u2502\n        \u2502 HI    \u2506 2012 \u2506 2     \u2502\n        \u2502 HI    \u2506 2016 \u2506 3     \u2502\n        \u2502 NY    \u2506 2009 \u2506 2     \u2502\n        \u2502 NY    \u2506 2013 \u2506 5     \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        &gt;&gt;&gt; low = pl.col('year').min()\n        &gt;&gt;&gt; high = pl.col('year').max().add(1)\n        &gt;&gt;&gt; new_year_values=pl.int_range(low,high).alias('year')\n        &gt;&gt;&gt; with pl.Config(tbl_rows=-1):\n        ...     df.complete(new_year_values,by='state',sort=True)\n        shape: (16, 3)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 state \u2506 year \u2506 value \u2502\n        \u2502 ---   \u2506 ---  \u2506 ---   \u2502\n        \u2502 str   \u2506 i64  \u2506 i64   \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 CA    \u2506 2010 \u2506 1     \u2502\n        \u2502 CA    \u2506 2011 \u2506 null  \u2502\n        \u2502 CA    \u2506 2012 \u2506 null  \u2502\n        \u2502 CA    \u2506 2013 \u2506 3     \u2502\n        \u2502 HI    \u2506 2010 \u2506 1     \u2502\n        \u2502 HI    \u2506 2011 \u2506 null  \u2502\n        \u2502 HI    \u2506 2012 \u2506 2     \u2502\n        \u2502 HI    \u2506 2013 \u2506 null  \u2502\n        \u2502 HI    \u2506 2014 \u2506 null  \u2502\n        \u2502 HI    \u2506 2015 \u2506 null  \u2502\n        \u2502 HI    \u2506 2016 \u2506 3     \u2502\n        \u2502 NY    \u2506 2009 \u2506 2     \u2502\n        \u2502 NY    \u2506 2010 \u2506 null  \u2502\n        \u2502 NY    \u2506 2011 \u2506 null  \u2502\n        \u2502 NY    \u2506 2012 \u2506 null  \u2502\n        \u2502 NY    \u2506 2013 \u2506 5     \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n    !!! info \"New in version 0.28.0\"\n\n    Args:\n        *columns: This refers to the columns to be completed.\n            It can be a string or a column selector or a polars expression.\n            A polars expression can be used to introduced new values,\n            as long as the polars expression has a name that already exists\n            in the DataFrame.\n        fill_value: Scalar value or polars expression to use instead of nulls\n            for missing combinations. A dictionary, mapping columns names\n            to a scalar value is also accepted.\n        explicit: Determines if only implicitly missing values\n            should be filled (`False`), or all nulls existing in the LazyFrame\n            (`True`). `explicit` is applicable only\n            if `fill_value` is not `None`.\n        sort: Sort the DataFrame based on *columns.\n        by: Column(s) to group by.\n            The explicit missing rows are returned per group.\n\n    Returns:\n        A polars DataFrame/LazyFrame.\n    \"\"\"  # noqa: E501\n    if not columns:\n        return df\n    return _complete(\n        df=df,\n        columns=columns,\n        fill_value=fill_value,\n        explicit=explicit,\n        sort=sort,\n        by=by,\n    )\n</code></pre>"},{"location":"api/polars/#janitor.polars.complete.expand","title":"<code>expand(df, *columns, sort=False, by=None)</code>","text":"<p>Creates a DataFrame from a cartesian combination of all inputs.</p> <p>Inspiration is from tidyr's expand() function.</p> <p>expand() is often useful with pl.DataFrame.join to convert implicit missing values to explicit missing values - similar to <code>complete</code>.</p> <p>It can also be used to figure out which combinations are missing (e.g identify gaps in your DataFrame).</p> <p>The variable <code>columns</code> parameter can be a string, a ColumnSelector, a polars expression, or a polars Series.</p> <p><code>expand</code> can also be applied to a LazyFrame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; import janitor.polars\n&gt;&gt;&gt; data = [{'type': 'apple', 'year': 2010, 'size': 'XS'},\n...         {'type': 'orange', 'year': 2010, 'size': 'S'},\n...         {'type': 'apple', 'year': 2012, 'size': 'M'},\n...         {'type': 'orange', 'year': 2010, 'size': 'S'},\n...         {'type': 'orange', 'year': 2011, 'size': 'S'},\n...         {'type': 'orange', 'year': 2012, 'size': 'M'}]\n&gt;&gt;&gt; df = pl.DataFrame(data)\n&gt;&gt;&gt; df\nshape: (6, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 type   \u2506 year \u2506 size \u2502\n\u2502 ---    \u2506 ---  \u2506 ---  \u2502\n\u2502 str    \u2506 i64  \u2506 str  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 apple  \u2506 2010 \u2506 XS   \u2502\n\u2502 orange \u2506 2010 \u2506 S    \u2502\n\u2502 apple  \u2506 2012 \u2506 M    \u2502\n\u2502 orange \u2506 2010 \u2506 S    \u2502\n\u2502 orange \u2506 2011 \u2506 S    \u2502\n\u2502 orange \u2506 2012 \u2506 M    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Get unique observations:</p> <pre><code>&gt;&gt;&gt; df.expand('type',sort=True)\nshape: (2, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 type   \u2502\n\u2502 ---    \u2502\n\u2502 str    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 apple  \u2502\n\u2502 orange \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n&gt;&gt;&gt; df.expand('size',sort=True)\nshape: (3, 1)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 size \u2502\n\u2502 ---  \u2502\n\u2502 str  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 M    \u2502\n\u2502 S    \u2502\n\u2502 XS   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n&gt;&gt;&gt; df.expand('type', 'size',sort=True)\nshape: (6, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 type   \u2506 size \u2502\n\u2502 ---    \u2506 ---  \u2502\n\u2502 str    \u2506 str  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 apple  \u2506 M    \u2502\n\u2502 apple  \u2506 S    \u2502\n\u2502 apple  \u2506 XS   \u2502\n\u2502 orange \u2506 M    \u2502\n\u2502 orange \u2506 S    \u2502\n\u2502 orange \u2506 XS   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n&gt;&gt;&gt; with pl.Config(tbl_rows=-1):\n...     df.expand('type','size','year',sort=True)\nshape: (18, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 type   \u2506 size \u2506 year \u2502\n\u2502 ---    \u2506 ---  \u2506 ---  \u2502\n\u2502 str    \u2506 str  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 apple  \u2506 M    \u2506 2010 \u2502\n\u2502 apple  \u2506 M    \u2506 2011 \u2502\n\u2502 apple  \u2506 M    \u2506 2012 \u2502\n\u2502 apple  \u2506 S    \u2506 2010 \u2502\n\u2502 apple  \u2506 S    \u2506 2011 \u2502\n\u2502 apple  \u2506 S    \u2506 2012 \u2502\n\u2502 apple  \u2506 XS   \u2506 2010 \u2502\n\u2502 apple  \u2506 XS   \u2506 2011 \u2502\n\u2502 apple  \u2506 XS   \u2506 2012 \u2502\n\u2502 orange \u2506 M    \u2506 2010 \u2502\n\u2502 orange \u2506 M    \u2506 2011 \u2502\n\u2502 orange \u2506 M    \u2506 2012 \u2502\n\u2502 orange \u2506 S    \u2506 2010 \u2502\n\u2502 orange \u2506 S    \u2506 2011 \u2502\n\u2502 orange \u2506 S    \u2506 2012 \u2502\n\u2502 orange \u2506 XS   \u2506 2010 \u2502\n\u2502 orange \u2506 XS   \u2506 2011 \u2502\n\u2502 orange \u2506 XS   \u2506 2012 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Get observations that only occur in the data:</p> <pre><code>&gt;&gt;&gt; df.expand(pl.struct('type','size'),sort=True).unnest('type')\nshape: (4, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 type   \u2506 size \u2502\n\u2502 ---    \u2506 ---  \u2502\n\u2502 str    \u2506 str  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 apple  \u2506 M    \u2502\n\u2502 apple  \u2506 XS   \u2502\n\u2502 orange \u2506 M    \u2502\n\u2502 orange \u2506 S    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n&gt;&gt;&gt; df.expand(pl.struct('type','size','year'),sort=True).unnest('type')\nshape: (5, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 type   \u2506 size \u2506 year \u2502\n\u2502 ---    \u2506 ---  \u2506 ---  \u2502\n\u2502 str    \u2506 str  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 apple  \u2506 M    \u2506 2012 \u2502\n\u2502 apple  \u2506 XS   \u2506 2010 \u2502\n\u2502 orange \u2506 M    \u2506 2012 \u2502\n\u2502 orange \u2506 S    \u2506 2010 \u2502\n\u2502 orange \u2506 S    \u2506 2011 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Expand the DataFrame to include new observations:</p> <pre><code>&gt;&gt;&gt; with pl.Config(tbl_rows=-1):\n...     df.expand('type','size',pl.int_range(2010,2014).alias('new_year'),sort=True)\nshape: (24, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 type   \u2506 size \u2506 new_year \u2502\n\u2502 ---    \u2506 ---  \u2506 ---      \u2502\n\u2502 str    \u2506 str  \u2506 i64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 apple  \u2506 M    \u2506 2010     \u2502\n\u2502 apple  \u2506 M    \u2506 2011     \u2502\n\u2502 apple  \u2506 M    \u2506 2012     \u2502\n\u2502 apple  \u2506 M    \u2506 2013     \u2502\n\u2502 apple  \u2506 S    \u2506 2010     \u2502\n\u2502 apple  \u2506 S    \u2506 2011     \u2502\n\u2502 apple  \u2506 S    \u2506 2012     \u2502\n\u2502 apple  \u2506 S    \u2506 2013     \u2502\n\u2502 apple  \u2506 XS   \u2506 2010     \u2502\n\u2502 apple  \u2506 XS   \u2506 2011     \u2502\n\u2502 apple  \u2506 XS   \u2506 2012     \u2502\n\u2502 apple  \u2506 XS   \u2506 2013     \u2502\n\u2502 orange \u2506 M    \u2506 2010     \u2502\n\u2502 orange \u2506 M    \u2506 2011     \u2502\n\u2502 orange \u2506 M    \u2506 2012     \u2502\n\u2502 orange \u2506 M    \u2506 2013     \u2502\n\u2502 orange \u2506 S    \u2506 2010     \u2502\n\u2502 orange \u2506 S    \u2506 2011     \u2502\n\u2502 orange \u2506 S    \u2506 2012     \u2502\n\u2502 orange \u2506 S    \u2506 2013     \u2502\n\u2502 orange \u2506 XS   \u2506 2010     \u2502\n\u2502 orange \u2506 XS   \u2506 2011     \u2502\n\u2502 orange \u2506 XS   \u2506 2012     \u2502\n\u2502 orange \u2506 XS   \u2506 2013     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Filter for missing observations:</p> <pre><code>&gt;&gt;&gt; columns = ('type','size','year')\n&gt;&gt;&gt; with pl.Config(tbl_rows=-1):\n...     df.expand(*columns).join(df, how='anti', on=columns).sort(by=pl.all())\nshape: (13, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 type   \u2506 size \u2506 year \u2502\n\u2502 ---    \u2506 ---  \u2506 ---  \u2502\n\u2502 str    \u2506 str  \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 apple  \u2506 M    \u2506 2010 \u2502\n\u2502 apple  \u2506 M    \u2506 2011 \u2502\n\u2502 apple  \u2506 S    \u2506 2010 \u2502\n\u2502 apple  \u2506 S    \u2506 2011 \u2502\n\u2502 apple  \u2506 S    \u2506 2012 \u2502\n\u2502 apple  \u2506 XS   \u2506 2011 \u2502\n\u2502 apple  \u2506 XS   \u2506 2012 \u2502\n\u2502 orange \u2506 M    \u2506 2010 \u2502\n\u2502 orange \u2506 M    \u2506 2011 \u2502\n\u2502 orange \u2506 S    \u2506 2012 \u2502\n\u2502 orange \u2506 XS   \u2506 2010 \u2502\n\u2502 orange \u2506 XS   \u2506 2011 \u2502\n\u2502 orange \u2506 XS   \u2506 2012 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Expand within each group, using <code>by</code>:</p> <pre><code>&gt;&gt;&gt; with pl.Config(tbl_rows=-1):\n...     df.expand('year','size',by='type',sort=True)\nshape: (10, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 type   \u2506 year \u2506 size \u2502\n\u2502 ---    \u2506 ---  \u2506 ---  \u2502\n\u2502 str    \u2506 i64  \u2506 str  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 apple  \u2506 2010 \u2506 M    \u2502\n\u2502 apple  \u2506 2010 \u2506 XS   \u2502\n\u2502 apple  \u2506 2012 \u2506 M    \u2502\n\u2502 apple  \u2506 2012 \u2506 XS   \u2502\n\u2502 orange \u2506 2010 \u2506 M    \u2502\n\u2502 orange \u2506 2010 \u2506 S    \u2502\n\u2502 orange \u2506 2011 \u2506 M    \u2502\n\u2502 orange \u2506 2011 \u2506 S    \u2502\n\u2502 orange \u2506 2012 \u2506 M    \u2502\n\u2502 orange \u2506 2012 \u2506 S    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>New in version 0.28.0</p> <p>Parameters:</p> Name Type Description Default <code>*columns</code> <code>ColumnNameOrSelector</code> <p>This refers to the columns to be completed. It can be a string or a column selector or a polars expression. A polars expression can be used to introduced new values, as long as the polars expression has a name that already exists in the DataFrame.</p> <code>()</code> <code>sort</code> <code>bool</code> <p>Sort the DataFrame based on *columns.</p> <code>False</code> <code>by</code> <code>ColumnNameOrSelector</code> <p>Column(s) to group by.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame | LazyFrame</code> <p>A polars DataFrame/LazyFrame.</p> Source code in <code>janitor/polars/complete.py</code> <pre><code>@register_lazyframe_method\n@register_dataframe_method\ndef expand(\n    df: pl.DataFrame | pl.LazyFrame,\n    *columns: ColumnNameOrSelector,\n    sort: bool = False,\n    by: ColumnNameOrSelector = None,\n) -&gt; pl.DataFrame | pl.LazyFrame:\n    \"\"\"\n    Creates a DataFrame from a cartesian combination of all inputs.\n\n    Inspiration is from tidyr's expand() function.\n\n    expand() is often useful with\n    [pl.DataFrame.join](https://docs.pola.rs/api/python/stable/reference/dataframe/api/polars.DataFrame.join.html)\n    to convert implicit\n    missing values to explicit missing values - similar to\n    [`complete`][janitor.polars.complete.complete].\n\n    It can also be used to figure out which combinations are missing\n    (e.g identify gaps in your DataFrame).\n\n    The variable `columns` parameter can be a string,\n    a ColumnSelector, a polars expression, or a polars Series.\n\n    `expand` can also be applied to a LazyFrame.\n\n    Examples:\n        &gt;&gt;&gt; import polars as pl\n        &gt;&gt;&gt; import janitor.polars\n        &gt;&gt;&gt; data = [{'type': 'apple', 'year': 2010, 'size': 'XS'},\n        ...         {'type': 'orange', 'year': 2010, 'size': 'S'},\n        ...         {'type': 'apple', 'year': 2012, 'size': 'M'},\n        ...         {'type': 'orange', 'year': 2010, 'size': 'S'},\n        ...         {'type': 'orange', 'year': 2011, 'size': 'S'},\n        ...         {'type': 'orange', 'year': 2012, 'size': 'M'}]\n        &gt;&gt;&gt; df = pl.DataFrame(data)\n        &gt;&gt;&gt; df\n        shape: (6, 3)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 type   \u2506 year \u2506 size \u2502\n        \u2502 ---    \u2506 ---  \u2506 ---  \u2502\n        \u2502 str    \u2506 i64  \u2506 str  \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 apple  \u2506 2010 \u2506 XS   \u2502\n        \u2502 orange \u2506 2010 \u2506 S    \u2502\n        \u2502 apple  \u2506 2012 \u2506 M    \u2502\n        \u2502 orange \u2506 2010 \u2506 S    \u2502\n        \u2502 orange \u2506 2011 \u2506 S    \u2502\n        \u2502 orange \u2506 2012 \u2506 M    \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n        Get unique observations:\n        &gt;&gt;&gt; df.expand('type',sort=True)\n        shape: (2, 1)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 type   \u2502\n        \u2502 ---    \u2502\n        \u2502 str    \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 apple  \u2502\n        \u2502 orange \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        &gt;&gt;&gt; df.expand('size',sort=True)\n        shape: (3, 1)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 size \u2502\n        \u2502 ---  \u2502\n        \u2502 str  \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 M    \u2502\n        \u2502 S    \u2502\n        \u2502 XS   \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        &gt;&gt;&gt; df.expand('type', 'size',sort=True)\n        shape: (6, 2)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 type   \u2506 size \u2502\n        \u2502 ---    \u2506 ---  \u2502\n        \u2502 str    \u2506 str  \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 apple  \u2506 M    \u2502\n        \u2502 apple  \u2506 S    \u2502\n        \u2502 apple  \u2506 XS   \u2502\n        \u2502 orange \u2506 M    \u2502\n        \u2502 orange \u2506 S    \u2502\n        \u2502 orange \u2506 XS   \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        &gt;&gt;&gt; with pl.Config(tbl_rows=-1):\n        ...     df.expand('type','size','year',sort=True)\n        shape: (18, 3)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 type   \u2506 size \u2506 year \u2502\n        \u2502 ---    \u2506 ---  \u2506 ---  \u2502\n        \u2502 str    \u2506 str  \u2506 i64  \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 apple  \u2506 M    \u2506 2010 \u2502\n        \u2502 apple  \u2506 M    \u2506 2011 \u2502\n        \u2502 apple  \u2506 M    \u2506 2012 \u2502\n        \u2502 apple  \u2506 S    \u2506 2010 \u2502\n        \u2502 apple  \u2506 S    \u2506 2011 \u2502\n        \u2502 apple  \u2506 S    \u2506 2012 \u2502\n        \u2502 apple  \u2506 XS   \u2506 2010 \u2502\n        \u2502 apple  \u2506 XS   \u2506 2011 \u2502\n        \u2502 apple  \u2506 XS   \u2506 2012 \u2502\n        \u2502 orange \u2506 M    \u2506 2010 \u2502\n        \u2502 orange \u2506 M    \u2506 2011 \u2502\n        \u2502 orange \u2506 M    \u2506 2012 \u2502\n        \u2502 orange \u2506 S    \u2506 2010 \u2502\n        \u2502 orange \u2506 S    \u2506 2011 \u2502\n        \u2502 orange \u2506 S    \u2506 2012 \u2502\n        \u2502 orange \u2506 XS   \u2506 2010 \u2502\n        \u2502 orange \u2506 XS   \u2506 2011 \u2502\n        \u2502 orange \u2506 XS   \u2506 2012 \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n        Get observations that only occur in the data:\n        &gt;&gt;&gt; df.expand(pl.struct('type','size'),sort=True).unnest('type')\n        shape: (4, 2)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 type   \u2506 size \u2502\n        \u2502 ---    \u2506 ---  \u2502\n        \u2502 str    \u2506 str  \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 apple  \u2506 M    \u2502\n        \u2502 apple  \u2506 XS   \u2502\n        \u2502 orange \u2506 M    \u2502\n        \u2502 orange \u2506 S    \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        &gt;&gt;&gt; df.expand(pl.struct('type','size','year'),sort=True).unnest('type')\n        shape: (5, 3)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 type   \u2506 size \u2506 year \u2502\n        \u2502 ---    \u2506 ---  \u2506 ---  \u2502\n        \u2502 str    \u2506 str  \u2506 i64  \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 apple  \u2506 M    \u2506 2012 \u2502\n        \u2502 apple  \u2506 XS   \u2506 2010 \u2502\n        \u2502 orange \u2506 M    \u2506 2012 \u2502\n        \u2502 orange \u2506 S    \u2506 2010 \u2502\n        \u2502 orange \u2506 S    \u2506 2011 \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n        Expand the DataFrame to include new observations:\n        &gt;&gt;&gt; with pl.Config(tbl_rows=-1):\n        ...     df.expand('type','size',pl.int_range(2010,2014).alias('new_year'),sort=True)\n        shape: (24, 3)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 type   \u2506 size \u2506 new_year \u2502\n        \u2502 ---    \u2506 ---  \u2506 ---      \u2502\n        \u2502 str    \u2506 str  \u2506 i64      \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 apple  \u2506 M    \u2506 2010     \u2502\n        \u2502 apple  \u2506 M    \u2506 2011     \u2502\n        \u2502 apple  \u2506 M    \u2506 2012     \u2502\n        \u2502 apple  \u2506 M    \u2506 2013     \u2502\n        \u2502 apple  \u2506 S    \u2506 2010     \u2502\n        \u2502 apple  \u2506 S    \u2506 2011     \u2502\n        \u2502 apple  \u2506 S    \u2506 2012     \u2502\n        \u2502 apple  \u2506 S    \u2506 2013     \u2502\n        \u2502 apple  \u2506 XS   \u2506 2010     \u2502\n        \u2502 apple  \u2506 XS   \u2506 2011     \u2502\n        \u2502 apple  \u2506 XS   \u2506 2012     \u2502\n        \u2502 apple  \u2506 XS   \u2506 2013     \u2502\n        \u2502 orange \u2506 M    \u2506 2010     \u2502\n        \u2502 orange \u2506 M    \u2506 2011     \u2502\n        \u2502 orange \u2506 M    \u2506 2012     \u2502\n        \u2502 orange \u2506 M    \u2506 2013     \u2502\n        \u2502 orange \u2506 S    \u2506 2010     \u2502\n        \u2502 orange \u2506 S    \u2506 2011     \u2502\n        \u2502 orange \u2506 S    \u2506 2012     \u2502\n        \u2502 orange \u2506 S    \u2506 2013     \u2502\n        \u2502 orange \u2506 XS   \u2506 2010     \u2502\n        \u2502 orange \u2506 XS   \u2506 2011     \u2502\n        \u2502 orange \u2506 XS   \u2506 2012     \u2502\n        \u2502 orange \u2506 XS   \u2506 2013     \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n        Filter for missing observations:\n        &gt;&gt;&gt; columns = ('type','size','year')\n        &gt;&gt;&gt; with pl.Config(tbl_rows=-1):\n        ...     df.expand(*columns).join(df, how='anti', on=columns).sort(by=pl.all())\n        shape: (13, 3)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 type   \u2506 size \u2506 year \u2502\n        \u2502 ---    \u2506 ---  \u2506 ---  \u2502\n        \u2502 str    \u2506 str  \u2506 i64  \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 apple  \u2506 M    \u2506 2010 \u2502\n        \u2502 apple  \u2506 M    \u2506 2011 \u2502\n        \u2502 apple  \u2506 S    \u2506 2010 \u2502\n        \u2502 apple  \u2506 S    \u2506 2011 \u2502\n        \u2502 apple  \u2506 S    \u2506 2012 \u2502\n        \u2502 apple  \u2506 XS   \u2506 2011 \u2502\n        \u2502 apple  \u2506 XS   \u2506 2012 \u2502\n        \u2502 orange \u2506 M    \u2506 2010 \u2502\n        \u2502 orange \u2506 M    \u2506 2011 \u2502\n        \u2502 orange \u2506 S    \u2506 2012 \u2502\n        \u2502 orange \u2506 XS   \u2506 2010 \u2502\n        \u2502 orange \u2506 XS   \u2506 2011 \u2502\n        \u2502 orange \u2506 XS   \u2506 2012 \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n        Expand within each group, using `by`:\n        &gt;&gt;&gt; with pl.Config(tbl_rows=-1):\n        ...     df.expand('year','size',by='type',sort=True)\n        shape: (10, 3)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 type   \u2506 year \u2506 size \u2502\n        \u2502 ---    \u2506 ---  \u2506 ---  \u2502\n        \u2502 str    \u2506 i64  \u2506 str  \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 apple  \u2506 2010 \u2506 M    \u2502\n        \u2502 apple  \u2506 2010 \u2506 XS   \u2502\n        \u2502 apple  \u2506 2012 \u2506 M    \u2502\n        \u2502 apple  \u2506 2012 \u2506 XS   \u2502\n        \u2502 orange \u2506 2010 \u2506 M    \u2502\n        \u2502 orange \u2506 2010 \u2506 S    \u2502\n        \u2502 orange \u2506 2011 \u2506 M    \u2502\n        \u2502 orange \u2506 2011 \u2506 S    \u2502\n        \u2502 orange \u2506 2012 \u2506 M    \u2502\n        \u2502 orange \u2506 2012 \u2506 S    \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n    !!! info \"New in version 0.28.0\"\n\n    Args:\n        *columns: This refers to the columns to be completed.\n            It can be a string or a column selector or a polars expression.\n            A polars expression can be used to introduced new values,\n            as long as the polars expression has a name that already exists\n            in the DataFrame.\n        sort: Sort the DataFrame based on *columns.\n        by: Column(s) to group by.\n\n    Returns:\n        A polars DataFrame/LazyFrame.\n    \"\"\"  # noqa: E501\n    if not columns:\n        return df\n    uniques, _ = _expand(df=df, columns=columns, by=by, sort=sort)\n    return uniques\n</code></pre>"},{"location":"api/polars/#janitor.polars.pivot_longer","title":"<code>pivot_longer</code>","text":"<p>pivot_longer implementation for polars.</p>"},{"location":"api/polars/#janitor.polars.pivot_longer.pivot_longer","title":"<code>pivot_longer(df, index=None, column_names=None, names_to='variable', values_to='value', names_sep=None, names_pattern=None, names_transform=None)</code>","text":"<p>Unpivots a DataFrame from wide to long format.</p> <p>It is modeled after the <code>pivot_longer</code> function in R's tidyr package, and also takes inspiration from the <code>melt</code> function in R's data.table package.</p> <p>This function is useful to massage a DataFrame into a format where one or more columns are considered measured variables, and all other columns are considered as identifier variables.</p> <p>All measured variables are unpivoted (and typically duplicated) along the row axis.</p> <p>If <code>names_pattern</code>, use a valid regular expression pattern containing at least one capture group, compatible with the regex crate.</p> <p>For more granular control on the unpivoting, have a look at <code>pivot_longer_spec</code>.</p> <p><code>pivot_longer</code> can also be applied to a LazyFrame.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; import polars.selectors as cs\n&gt;&gt;&gt; import janitor.polars\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"Sepal.Length\": [5.1, 5.9],\n...         \"Sepal.Width\": [3.5, 3.0],\n...         \"Petal.Length\": [1.4, 5.1],\n...         \"Petal.Width\": [0.2, 1.8],\n...         \"Species\": [\"setosa\", \"virginica\"],\n...     }\n... )\n&gt;&gt;&gt; df\nshape: (2, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Sepal.Length \u2506 Sepal.Width \u2506 Petal.Length \u2506 Petal.Width \u2506 Species   \u2502\n\u2502 ---          \u2506 ---         \u2506 ---          \u2506 ---         \u2506 ---       \u2502\n\u2502 f64          \u2506 f64         \u2506 f64          \u2506 f64         \u2506 str       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 5.1          \u2506 3.5         \u2506 1.4          \u2506 0.2         \u2506 setosa    \u2502\n\u2502 5.9          \u2506 3.0         \u2506 5.1          \u2506 1.8         \u2506 virginica \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Replicate polars' melt:</p> <pre><code>&gt;&gt;&gt; df.pivot_longer(index = 'Species').sort(by=pl.all())\nshape: (8, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Species   \u2506 variable     \u2506 value \u2502\n\u2502 ---       \u2506 ---          \u2506 ---   \u2502\n\u2502 str       \u2506 str          \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 setosa    \u2506 Petal.Length \u2506 1.4   \u2502\n\u2502 setosa    \u2506 Petal.Width  \u2506 0.2   \u2502\n\u2502 setosa    \u2506 Sepal.Length \u2506 5.1   \u2502\n\u2502 setosa    \u2506 Sepal.Width  \u2506 3.5   \u2502\n\u2502 virginica \u2506 Petal.Length \u2506 5.1   \u2502\n\u2502 virginica \u2506 Petal.Width  \u2506 1.8   \u2502\n\u2502 virginica \u2506 Sepal.Length \u2506 5.9   \u2502\n\u2502 virginica \u2506 Sepal.Width  \u2506 3.0   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Split the column labels into individual columns:</p> <pre><code>&gt;&gt;&gt; df.pivot_longer(\n...     index = 'Species',\n...     names_to = ('part', 'dimension'),\n...     names_sep = '.',\n... ).select('Species','part','dimension','value').sort(by=pl.all())\nshape: (8, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Species   \u2506 part  \u2506 dimension \u2506 value \u2502\n\u2502 ---       \u2506 ---   \u2506 ---       \u2506 ---   \u2502\n\u2502 str       \u2506 str   \u2506 str       \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 setosa    \u2506 Petal \u2506 Length    \u2506 1.4   \u2502\n\u2502 setosa    \u2506 Petal \u2506 Width     \u2506 0.2   \u2502\n\u2502 setosa    \u2506 Sepal \u2506 Length    \u2506 5.1   \u2502\n\u2502 setosa    \u2506 Sepal \u2506 Width     \u2506 3.5   \u2502\n\u2502 virginica \u2506 Petal \u2506 Length    \u2506 5.1   \u2502\n\u2502 virginica \u2506 Petal \u2506 Width     \u2506 1.8   \u2502\n\u2502 virginica \u2506 Sepal \u2506 Length    \u2506 5.9   \u2502\n\u2502 virginica \u2506 Sepal \u2506 Width     \u2506 3.0   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Retain parts of the column names as headers:</p> <pre><code>&gt;&gt;&gt; df.pivot_longer(\n...     index = 'Species',\n...     names_to = ('part', '.value'),\n...     names_sep = '.',\n... ).select('Species','part','Length','Width').sort(by=pl.all())\nshape: (4, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Species   \u2506 part  \u2506 Length \u2506 Width \u2502\n\u2502 ---       \u2506 ---   \u2506 ---    \u2506 ---   \u2502\n\u2502 str       \u2506 str   \u2506 f64    \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 setosa    \u2506 Petal \u2506 1.4    \u2506 0.2   \u2502\n\u2502 setosa    \u2506 Sepal \u2506 5.1    \u2506 3.5   \u2502\n\u2502 virginica \u2506 Petal \u2506 5.1    \u2506 1.8   \u2502\n\u2502 virginica \u2506 Sepal \u2506 5.9    \u2506 3.0   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Split the column labels based on regex:</p> <pre><code>&gt;&gt;&gt; df = pl.DataFrame({\"id\": [1], \"new_sp_m5564\": [2], \"newrel_f65\": [3]})\n&gt;&gt;&gt; df\nshape: (1, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 new_sp_m5564 \u2506 newrel_f65 \u2502\n\u2502 --- \u2506 ---          \u2506 ---        \u2502\n\u2502 i64 \u2506 i64          \u2506 i64        \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 2            \u2506 3          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n&gt;&gt;&gt; df.pivot_longer(\n...     index = 'id',\n...     names_to = ('diagnosis', 'gender', 'age'),\n...     names_pattern = r\"new_?(.+)_(.)([0-9]+)\",\n... ).select('id','diagnosis','gender','age','value').sort(by=pl.all())\nshape: (2, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 diagnosis \u2506 gender \u2506 age  \u2506 value \u2502\n\u2502 --- \u2506 ---       \u2506 ---    \u2506 ---  \u2506 ---   \u2502\n\u2502 i64 \u2506 str       \u2506 str    \u2506 str  \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 rel       \u2506 f      \u2506 65   \u2506 3     \u2502\n\u2502 1   \u2506 sp        \u2506 m      \u2506 5564 \u2506 2     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Convert the dtypes of specific columns with <code>names_transform</code>:</p> <pre><code>&gt;&gt;&gt; df.pivot_longer(\n...     index = \"id\",\n...     names_pattern=r\"new_?(.+)_(.)([0-9]+)\",\n...     names_to=(\"diagnosis\", \"gender\", \"age\"),\n...     names_transform=pl.col('age').cast(pl.Int32),\n... ).select(\"id\", \"diagnosis\", \"gender\", \"age\", \"value\").sort(by=pl.all())\nshape: (2, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id  \u2506 diagnosis \u2506 gender \u2506 age  \u2506 value \u2502\n\u2502 --- \u2506 ---       \u2506 ---    \u2506 ---  \u2506 ---   \u2502\n\u2502 i64 \u2506 str       \u2506 str    \u2506 i32  \u2506 i64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1   \u2506 rel       \u2506 f      \u2506 65   \u2506 3     \u2502\n\u2502 1   \u2506 sp        \u2506 m      \u2506 5564 \u2506 2     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Use multiple <code>.value</code> to reshape the dataframe:</p> <pre><code>&gt;&gt;&gt; df = pl.DataFrame(\n...     [\n...         {\n...             \"x_1_mean\": 10,\n...             \"x_2_mean\": 20,\n...             \"y_1_mean\": 30,\n...             \"y_2_mean\": 40,\n...             \"unit\": 50,\n...         }\n...     ]\n... )\n&gt;&gt;&gt; df\nshape: (1, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 x_1_mean \u2506 x_2_mean \u2506 y_1_mean \u2506 y_2_mean \u2506 unit \u2502\n\u2502 ---      \u2506 ---      \u2506 ---      \u2506 ---      \u2506 ---  \u2502\n\u2502 i64      \u2506 i64      \u2506 i64      \u2506 i64      \u2506 i64  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 10       \u2506 20       \u2506 30       \u2506 40       \u2506 50   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n&gt;&gt;&gt; df.pivot_longer(\n...     index=\"unit\",\n...     names_to=(\".value\", \"time\", \".value\"),\n...     names_pattern=r\"(x|y)_([0-9])(_mean)\",\n... ).select('unit','time','x_mean','y_mean').sort(by=pl.all())\nshape: (2, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 unit \u2506 time \u2506 x_mean \u2506 y_mean \u2502\n\u2502 ---  \u2506 ---  \u2506 ---    \u2506 ---    \u2502\n\u2502 i64  \u2506 str  \u2506 i64    \u2506 i64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 50   \u2506 1    \u2506 10     \u2506 30     \u2502\n\u2502 50   \u2506 2    \u2506 20     \u2506 40     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>New in version 0.28.0</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>ColumnNameOrSelector</code> <p>Column(s) or selector(s) to use as identifier variables.</p> <code>None</code> <code>column_names</code> <code>ColumnNameOrSelector</code> <p>Column(s) or selector(s) to unpivot.</p> <code>None</code> <code>names_to</code> <code>list | tuple | str</code> <p>Name of new column as a string that will contain what were previously the column names in <code>column_names</code>. The default is <code>variable</code> if no value is provided. It can also be a list/tuple of strings that will serve as new column names, if <code>name_sep</code> or <code>names_pattern</code> is provided. If <code>.value</code> is in <code>names_to</code>, new column names will be extracted from part of the existing column names and overrides <code>values_to</code>.</p> <code>'variable'</code> <code>values_to</code> <code>str</code> <p>Name of new column as a string that will contain what were previously the values of the columns in <code>column_names</code>.</p> <code>'value'</code> <code>names_sep</code> <code>str</code> <p>Determines how the column name is broken up, if <code>names_to</code> contains multiple values. It takes the same specification as polars' <code>str.split</code> method.</p> <code>None</code> <code>names_pattern</code> <code>str</code> <p>Determines how the column name is broken up. It can be a regular expression containing matching groups. It takes the same specification as polars' <code>str.extract_groups</code> method.</p> <code>None</code> <code>names_transform</code> <code>Expr</code> <p>Use this option to change the types of columns that have been transformed to rows. This does not applies to the values' columns. Accepts a polars expression or a list of polars expressions. Applicable only if one of names_sep or names_pattern is provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame | LazyFrame</code> <p>A polars DataFrame/LazyFrame that has been unpivoted</p> <code>DataFrame | LazyFrame</code> <p>from wide to long format.</p> Source code in <code>janitor/polars/pivot_longer.py</code> <pre><code>@register_lazyframe_method\n@register_dataframe_method\ndef pivot_longer(\n    df: pl.DataFrame | pl.LazyFrame,\n    index: ColumnNameOrSelector = None,\n    column_names: ColumnNameOrSelector = None,\n    names_to: list | tuple | str = \"variable\",\n    values_to: str = \"value\",\n    names_sep: str = None,\n    names_pattern: str = None,\n    names_transform: pl.Expr = None,\n) -&gt; pl.DataFrame | pl.LazyFrame:\n    \"\"\"\n    Unpivots a DataFrame from *wide* to *long* format.\n\n    It is modeled after the `pivot_longer` function in R's tidyr package,\n    and also takes inspiration from the `melt` function in R's data.table package.\n\n    This function is useful to massage a DataFrame into a format where\n    one or more columns are considered measured variables, and all other\n    columns are considered as identifier variables.\n\n    All measured variables are *unpivoted* (and typically duplicated) along the\n    row axis.\n\n    If `names_pattern`, use a valid regular expression pattern containing at least\n    one capture group, compatible with the [regex crate](https://docs.rs/regex/latest/regex/).\n\n    For more granular control on the unpivoting, have a look at\n    [`pivot_longer_spec`][janitor.polars.pivot_longer.pivot_longer_spec].\n\n    `pivot_longer` can also be applied to a LazyFrame.\n\n    Examples:\n        &gt;&gt;&gt; import polars as pl\n        &gt;&gt;&gt; import polars.selectors as cs\n        &gt;&gt;&gt; import janitor.polars\n        &gt;&gt;&gt; df = pl.DataFrame(\n        ...     {\n        ...         \"Sepal.Length\": [5.1, 5.9],\n        ...         \"Sepal.Width\": [3.5, 3.0],\n        ...         \"Petal.Length\": [1.4, 5.1],\n        ...         \"Petal.Width\": [0.2, 1.8],\n        ...         \"Species\": [\"setosa\", \"virginica\"],\n        ...     }\n        ... )\n        &gt;&gt;&gt; df\n        shape: (2, 5)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 Sepal.Length \u2506 Sepal.Width \u2506 Petal.Length \u2506 Petal.Width \u2506 Species   \u2502\n        \u2502 ---          \u2506 ---         \u2506 ---          \u2506 ---         \u2506 ---       \u2502\n        \u2502 f64          \u2506 f64         \u2506 f64          \u2506 f64         \u2506 str       \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 5.1          \u2506 3.5         \u2506 1.4          \u2506 0.2         \u2506 setosa    \u2502\n        \u2502 5.9          \u2506 3.0         \u2506 5.1          \u2506 1.8         \u2506 virginica \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n        Replicate polars' [melt](https://docs.pola.rs/py-polars/html/reference/dataframe/api/polars.DataFrame.unpivot.html#polars-dataframe-melt):\n        &gt;&gt;&gt; df.pivot_longer(index = 'Species').sort(by=pl.all())\n        shape: (8, 3)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 Species   \u2506 variable     \u2506 value \u2502\n        \u2502 ---       \u2506 ---          \u2506 ---   \u2502\n        \u2502 str       \u2506 str          \u2506 f64   \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 setosa    \u2506 Petal.Length \u2506 1.4   \u2502\n        \u2502 setosa    \u2506 Petal.Width  \u2506 0.2   \u2502\n        \u2502 setosa    \u2506 Sepal.Length \u2506 5.1   \u2502\n        \u2502 setosa    \u2506 Sepal.Width  \u2506 3.5   \u2502\n        \u2502 virginica \u2506 Petal.Length \u2506 5.1   \u2502\n        \u2502 virginica \u2506 Petal.Width  \u2506 1.8   \u2502\n        \u2502 virginica \u2506 Sepal.Length \u2506 5.9   \u2502\n        \u2502 virginica \u2506 Sepal.Width  \u2506 3.0   \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n        Split the column labels into individual columns:\n        &gt;&gt;&gt; df.pivot_longer(\n        ...     index = 'Species',\n        ...     names_to = ('part', 'dimension'),\n        ...     names_sep = '.',\n        ... ).select('Species','part','dimension','value').sort(by=pl.all())\n        shape: (8, 4)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 Species   \u2506 part  \u2506 dimension \u2506 value \u2502\n        \u2502 ---       \u2506 ---   \u2506 ---       \u2506 ---   \u2502\n        \u2502 str       \u2506 str   \u2506 str       \u2506 f64   \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 setosa    \u2506 Petal \u2506 Length    \u2506 1.4   \u2502\n        \u2502 setosa    \u2506 Petal \u2506 Width     \u2506 0.2   \u2502\n        \u2502 setosa    \u2506 Sepal \u2506 Length    \u2506 5.1   \u2502\n        \u2502 setosa    \u2506 Sepal \u2506 Width     \u2506 3.5   \u2502\n        \u2502 virginica \u2506 Petal \u2506 Length    \u2506 5.1   \u2502\n        \u2502 virginica \u2506 Petal \u2506 Width     \u2506 1.8   \u2502\n        \u2502 virginica \u2506 Sepal \u2506 Length    \u2506 5.9   \u2502\n        \u2502 virginica \u2506 Sepal \u2506 Width     \u2506 3.0   \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n        Retain parts of the column names as headers:\n        &gt;&gt;&gt; df.pivot_longer(\n        ...     index = 'Species',\n        ...     names_to = ('part', '.value'),\n        ...     names_sep = '.',\n        ... ).select('Species','part','Length','Width').sort(by=pl.all())\n        shape: (4, 4)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 Species   \u2506 part  \u2506 Length \u2506 Width \u2502\n        \u2502 ---       \u2506 ---   \u2506 ---    \u2506 ---   \u2502\n        \u2502 str       \u2506 str   \u2506 f64    \u2506 f64   \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 setosa    \u2506 Petal \u2506 1.4    \u2506 0.2   \u2502\n        \u2502 setosa    \u2506 Sepal \u2506 5.1    \u2506 3.5   \u2502\n        \u2502 virginica \u2506 Petal \u2506 5.1    \u2506 1.8   \u2502\n        \u2502 virginica \u2506 Sepal \u2506 5.9    \u2506 3.0   \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n        Split the column labels based on regex:\n        &gt;&gt;&gt; df = pl.DataFrame({\"id\": [1], \"new_sp_m5564\": [2], \"newrel_f65\": [3]})\n        &gt;&gt;&gt; df\n        shape: (1, 3)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 id  \u2506 new_sp_m5564 \u2506 newrel_f65 \u2502\n        \u2502 --- \u2506 ---          \u2506 ---        \u2502\n        \u2502 i64 \u2506 i64          \u2506 i64        \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 1   \u2506 2            \u2506 3          \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        &gt;&gt;&gt; df.pivot_longer(\n        ...     index = 'id',\n        ...     names_to = ('diagnosis', 'gender', 'age'),\n        ...     names_pattern = r\"new_?(.+)_(.)([0-9]+)\",\n        ... ).select('id','diagnosis','gender','age','value').sort(by=pl.all())\n        shape: (2, 5)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 id  \u2506 diagnosis \u2506 gender \u2506 age  \u2506 value \u2502\n        \u2502 --- \u2506 ---       \u2506 ---    \u2506 ---  \u2506 ---   \u2502\n        \u2502 i64 \u2506 str       \u2506 str    \u2506 str  \u2506 i64   \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 1   \u2506 rel       \u2506 f      \u2506 65   \u2506 3     \u2502\n        \u2502 1   \u2506 sp        \u2506 m      \u2506 5564 \u2506 2     \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n        Convert the dtypes of specific columns with `names_transform`:\n        &gt;&gt;&gt; df.pivot_longer(\n        ...     index = \"id\",\n        ...     names_pattern=r\"new_?(.+)_(.)([0-9]+)\",\n        ...     names_to=(\"diagnosis\", \"gender\", \"age\"),\n        ...     names_transform=pl.col('age').cast(pl.Int32),\n        ... ).select(\"id\", \"diagnosis\", \"gender\", \"age\", \"value\").sort(by=pl.all())\n        shape: (2, 5)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 id  \u2506 diagnosis \u2506 gender \u2506 age  \u2506 value \u2502\n        \u2502 --- \u2506 ---       \u2506 ---    \u2506 ---  \u2506 ---   \u2502\n        \u2502 i64 \u2506 str       \u2506 str    \u2506 i32  \u2506 i64   \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 1   \u2506 rel       \u2506 f      \u2506 65   \u2506 3     \u2502\n        \u2502 1   \u2506 sp        \u2506 m      \u2506 5564 \u2506 2     \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n        Use multiple `.value` to reshape the dataframe:\n        &gt;&gt;&gt; df = pl.DataFrame(\n        ...     [\n        ...         {\n        ...             \"x_1_mean\": 10,\n        ...             \"x_2_mean\": 20,\n        ...             \"y_1_mean\": 30,\n        ...             \"y_2_mean\": 40,\n        ...             \"unit\": 50,\n        ...         }\n        ...     ]\n        ... )\n        &gt;&gt;&gt; df\n        shape: (1, 5)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 x_1_mean \u2506 x_2_mean \u2506 y_1_mean \u2506 y_2_mean \u2506 unit \u2502\n        \u2502 ---      \u2506 ---      \u2506 ---      \u2506 ---      \u2506 ---  \u2502\n        \u2502 i64      \u2506 i64      \u2506 i64      \u2506 i64      \u2506 i64  \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 10       \u2506 20       \u2506 30       \u2506 40       \u2506 50   \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        &gt;&gt;&gt; df.pivot_longer(\n        ...     index=\"unit\",\n        ...     names_to=(\".value\", \"time\", \".value\"),\n        ...     names_pattern=r\"(x|y)_([0-9])(_mean)\",\n        ... ).select('unit','time','x_mean','y_mean').sort(by=pl.all())\n        shape: (2, 4)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 unit \u2506 time \u2506 x_mean \u2506 y_mean \u2502\n        \u2502 ---  \u2506 ---  \u2506 ---    \u2506 ---    \u2502\n        \u2502 i64  \u2506 str  \u2506 i64    \u2506 i64    \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 50   \u2506 1    \u2506 10     \u2506 30     \u2502\n        \u2502 50   \u2506 2    \u2506 20     \u2506 40     \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n    !!! info \"New in version 0.28.0\"\n\n    Args:\n        index: Column(s) or selector(s) to use as identifier variables.\n        column_names: Column(s) or selector(s) to unpivot.\n        names_to: Name of new column as a string that will contain\n            what were previously the column names in `column_names`.\n            The default is `variable` if no value is provided. It can\n            also be a list/tuple of strings that will serve as new column\n            names, if `name_sep` or `names_pattern` is provided.\n            If `.value` is in `names_to`, new column names will be extracted\n            from part of the existing column names and overrides `values_to`.\n        values_to: Name of new column as a string that will contain what\n            were previously the values of the columns in `column_names`.\n        names_sep: Determines how the column name is broken up, if\n            `names_to` contains multiple values. It takes the same\n            specification as polars' `str.split` method.\n        names_pattern: Determines how the column name is broken up.\n            It can be a regular expression containing matching groups.\n            It takes the same specification as\n            polars' `str.extract_groups` method.\n        names_transform: Use this option to change the types of columns that\n            have been transformed to rows.\n            This does not applies to the values' columns.\n            Accepts a polars expression or a list of polars expressions.\n            Applicable only if one of names_sep\n            or names_pattern is provided.\n\n    Returns:\n        A polars DataFrame/LazyFrame that has been unpivoted\n        from wide to long format.\n    \"\"\"  # noqa: E501\n    return _pivot_longer(\n        df=df,\n        index=index,\n        column_names=column_names,\n        names_pattern=names_pattern,\n        names_sep=names_sep,\n        names_to=names_to,\n        values_to=values_to,\n        names_transform=names_transform,\n    )\n</code></pre>"},{"location":"api/polars/#janitor.polars.pivot_longer.pivot_longer_spec","title":"<code>pivot_longer_spec(df, spec)</code>","text":"<p>A declarative interface to pivot a Polars Frame from wide to long form, where you describe how the data will be unpivoted, using a DataFrame.</p> <p>It is modeled after tidyr's <code>pivot_longer_spec</code>.</p> <p>This gives you, the user, more control over the transformation to long form, using a spec DataFrame that describes exactly how data stored in the column names becomes variables.</p> <p>It can come in handy for situations where <code>pivot_longer</code> seems inadequate for the transformation.</p> <p>New in version 0.28.0</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from janitor.polars import pivot_longer_spec\n&gt;&gt;&gt; df = pl.DataFrame(\n...     {\n...         \"Sepal.Length\": [5.1, 5.9],\n...         \"Sepal.Width\": [3.5, 3.0],\n...         \"Petal.Length\": [1.4, 5.1],\n...         \"Petal.Width\": [0.2, 1.8],\n...         \"Species\": [\"setosa\", \"virginica\"],\n...     }\n... )\n&gt;&gt;&gt; df\nshape: (2, 5)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Sepal.Length \u2506 Sepal.Width \u2506 Petal.Length \u2506 Petal.Width \u2506 Species   \u2502\n\u2502 ---          \u2506 ---         \u2506 ---          \u2506 ---         \u2506 ---       \u2502\n\u2502 f64          \u2506 f64         \u2506 f64          \u2506 f64         \u2506 str       \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 5.1          \u2506 3.5         \u2506 1.4          \u2506 0.2         \u2506 setosa    \u2502\n\u2502 5.9          \u2506 3.0         \u2506 5.1          \u2506 1.8         \u2506 virginica \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n&gt;&gt;&gt; spec = {'.name':['Sepal.Length','Petal.Length',\n...                  'Sepal.Width','Petal.Width'],\n...         '.value':['Length','Length','Width','Width'],\n...         'part':['Sepal','Petal','Sepal','Petal']}\n&gt;&gt;&gt; spec = pl.DataFrame(spec)\n&gt;&gt;&gt; spec\nshape: (4, 3)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 .name        \u2506 .value \u2506 part  \u2502\n\u2502 ---          \u2506 ---    \u2506 ---   \u2502\n\u2502 str          \u2506 str    \u2506 str   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Sepal.Length \u2506 Length \u2506 Sepal \u2502\n\u2502 Petal.Length \u2506 Length \u2506 Petal \u2502\n\u2502 Sepal.Width  \u2506 Width  \u2506 Sepal \u2502\n\u2502 Petal.Width  \u2506 Width  \u2506 Petal \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n&gt;&gt;&gt; df.pipe(pivot_longer_spec,spec=spec).sort(by=pl.all())\nshape: (4, 4)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Species   \u2506 part  \u2506 Length \u2506 Width \u2502\n\u2502 ---       \u2506 ---   \u2506 ---    \u2506 ---   \u2502\n\u2502 str       \u2506 str   \u2506 f64    \u2506 f64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 setosa    \u2506 Petal \u2506 1.4    \u2506 0.2   \u2502\n\u2502 setosa    \u2506 Sepal \u2506 5.1    \u2506 3.5   \u2502\n\u2502 virginica \u2506 Petal \u2506 5.1    \u2506 1.8   \u2502\n\u2502 virginica \u2506 Sepal \u2506 5.9    \u2506 3.0   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame | LazyFrame</code> <p>The source DataFrame to unpivot. It can also be a LazyFrame.</p> required <code>spec</code> <code>DataFrame</code> <p>A specification DataFrame. At a minimum, the spec DataFrame must have a <code>.name</code> column and a <code>.value</code> column. The <code>.name</code> column  should contain the columns in the source DataFrame that will be transformed to long form. The <code>.value</code> column gives the name of the column that the values in the source DataFrame will go into. Additional columns in the spec DataFrame should be named to match columns in the long format of the dataset and contain values corresponding to columns pivoted from the wide format. Note that these additional columns should not already exist in the source DataFrame. If there are additional columns, the combination of these columns and the <code>.value</code> column must be unique.</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>If <code>.name</code> or <code>.value</code> is missing from the spec's columns.</p> <code>ValueError</code> <p>If the labels in spec's <code>.name</code> column is not unique.</p> <p>Returns:</p> Type Description <code>DataFrame | LazyFrame</code> <p>A polars DataFrame/LazyFrame.</p> Source code in <code>janitor/polars/pivot_longer.py</code> <pre><code>def pivot_longer_spec(\n    df: pl.DataFrame | pl.LazyFrame,\n    spec: pl.DataFrame,\n) -&gt; pl.DataFrame | pl.LazyFrame:\n    \"\"\"\n    A declarative interface to pivot a Polars Frame\n    from wide to long form,\n    where you describe how the data will be unpivoted,\n    using a DataFrame.\n\n    It is modeled after tidyr's `pivot_longer_spec`.\n\n    This gives you, the user,\n    more control over the transformation to long form,\n    using a *spec* DataFrame that describes exactly\n    how data stored in the column names\n    becomes variables.\n\n    It can come in handy for situations where\n    [`pivot_longer`][janitor.polars.pivot_longer.pivot_longer]\n    seems inadequate for the transformation.\n\n    !!! info \"New in version 0.28.0\"\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; from janitor.polars import pivot_longer_spec\n        &gt;&gt;&gt; df = pl.DataFrame(\n        ...     {\n        ...         \"Sepal.Length\": [5.1, 5.9],\n        ...         \"Sepal.Width\": [3.5, 3.0],\n        ...         \"Petal.Length\": [1.4, 5.1],\n        ...         \"Petal.Width\": [0.2, 1.8],\n        ...         \"Species\": [\"setosa\", \"virginica\"],\n        ...     }\n        ... )\n        &gt;&gt;&gt; df\n        shape: (2, 5)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 Sepal.Length \u2506 Sepal.Width \u2506 Petal.Length \u2506 Petal.Width \u2506 Species   \u2502\n        \u2502 ---          \u2506 ---         \u2506 ---          \u2506 ---         \u2506 ---       \u2502\n        \u2502 f64          \u2506 f64         \u2506 f64          \u2506 f64         \u2506 str       \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 5.1          \u2506 3.5         \u2506 1.4          \u2506 0.2         \u2506 setosa    \u2502\n        \u2502 5.9          \u2506 3.0         \u2506 5.1          \u2506 1.8         \u2506 virginica \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        &gt;&gt;&gt; spec = {'.name':['Sepal.Length','Petal.Length',\n        ...                  'Sepal.Width','Petal.Width'],\n        ...         '.value':['Length','Length','Width','Width'],\n        ...         'part':['Sepal','Petal','Sepal','Petal']}\n        &gt;&gt;&gt; spec = pl.DataFrame(spec)\n        &gt;&gt;&gt; spec\n        shape: (4, 3)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 .name        \u2506 .value \u2506 part  \u2502\n        \u2502 ---          \u2506 ---    \u2506 ---   \u2502\n        \u2502 str          \u2506 str    \u2506 str   \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 Sepal.Length \u2506 Length \u2506 Sepal \u2502\n        \u2502 Petal.Length \u2506 Length \u2506 Petal \u2502\n        \u2502 Sepal.Width  \u2506 Width  \u2506 Sepal \u2502\n        \u2502 Petal.Width  \u2506 Width  \u2506 Petal \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        &gt;&gt;&gt; df.pipe(pivot_longer_spec,spec=spec).sort(by=pl.all())\n        shape: (4, 4)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 Species   \u2506 part  \u2506 Length \u2506 Width \u2502\n        \u2502 ---       \u2506 ---   \u2506 ---    \u2506 ---   \u2502\n        \u2502 str       \u2506 str   \u2506 f64    \u2506 f64   \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 setosa    \u2506 Petal \u2506 1.4    \u2506 0.2   \u2502\n        \u2502 setosa    \u2506 Sepal \u2506 5.1    \u2506 3.5   \u2502\n        \u2502 virginica \u2506 Petal \u2506 5.1    \u2506 1.8   \u2502\n        \u2502 virginica \u2506 Sepal \u2506 5.9    \u2506 3.0   \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n    Args:\n        df: The source DataFrame to unpivot.\n            It can also be a LazyFrame.\n        spec: A specification DataFrame.\n            At a minimum, the spec DataFrame\n            must have a `.name` column\n            and a `.value` column.\n            The `.name` column  should contain the\n            columns in the source DataFrame that will be\n            transformed to long form.\n            The `.value` column gives the name of the column\n            that the values in the source DataFrame will go into.\n            Additional columns in the spec DataFrame\n            should be named to match columns\n            in the long format of the dataset and contain values\n            corresponding to columns pivoted from the wide format.\n            Note that these additional columns should not already exist\n            in the source DataFrame.\n            If there are additional columns, the combination of these columns\n            and the `.value` column must be unique.\n\n    Raises:\n        KeyError: If `.name` or `.value` is missing from the spec's columns.\n        ValueError: If the labels in spec's `.name` column is not unique.\n\n    Returns:\n        A polars DataFrame/LazyFrame.\n    \"\"\"\n    check(\"spec\", spec, [pl.DataFrame])\n    spec_columns = spec.collect_schema().names()\n    if \".name\" not in spec_columns:\n        raise KeyError(\n            \"Kindly ensure the spec DataFrame has a `.name` column.\"\n        )\n    if \".value\" not in spec_columns:\n        raise KeyError(\n            \"Kindly ensure the spec DataFrame has a `.value` column.\"\n        )\n    if spec.get_column(\".name\").is_duplicated().any():\n        raise ValueError(\"The labels in the `.name` column should be unique.\")\n    df_columns = df.collect_schema().names()\n    exclude = set(df_columns).intersection(spec_columns)\n    if exclude:\n        raise ValueError(\n            f\"Labels {*exclude, } in the spec dataframe already exist \"\n            \"as column labels in the source dataframe. \"\n            \"Kindly ensure the spec DataFrame's columns \"\n            \"are not present in the source DataFrame.\"\n        )\n\n    index = [\n        label for label in df_columns if label not in spec.get_column(\".name\")\n    ]\n    others = [\n        label for label in spec_columns if label not in {\".name\", \".value\"}\n    ]\n    if others:\n        if (len(others) == 1) &amp; (\n            spec.get_column(others[0]).dtype == pl.String\n        ):\n            # shortcut that avoids the implode/explode approach - and is faster\n            # if the requirements are met\n            # inspired by https://github.com/pola-rs/polars/pull/18519#issue-2500860927\n            return _pivot_longer_dot_value_string(\n                df=df,\n                index=index,\n                spec=spec,\n                variable_name=others[0],\n            )\n        variable_name = \"\".join(df_columns + spec_columns)\n        variable_name = f\"{variable_name}_\"\n        dot_value_only = False\n        expression = pl.struct(others).alias(variable_name)\n        spec = spec.select(\".name\", \".value\", expression)\n    else:\n        variable_name = \"\".join(df_columns + spec_columns)\n        variable_name = f\"{variable_name}_\"\n        dot_value_only = True\n        expression = pl.cum_count(\".value\").over(\".value\").alias(variable_name)\n        spec = spec.with_columns(expression)\n    return _pivot_longer_dot_value(\n        df=df,\n        index=index,\n        spec=spec,\n        variable_name=variable_name,\n        dot_value_only=dot_value_only,\n        names_transform=None,\n    )\n</code></pre>"},{"location":"api/polars/#janitor.polars.row_to_names","title":"<code>row_to_names</code>","text":"<p>row_to_names implementation for polars.</p>"},{"location":"api/polars/#janitor.polars.row_to_names.row_to_names","title":"<code>row_to_names(df, row_numbers=0, remove_rows=False, remove_rows_above=False, separator='_')</code>","text":"<p>Elevates a row, or rows, to be the column names of a DataFrame.</p> <p>Examples:</p> <p>Replace column names with the first row.</p> <pre><code>&gt;&gt;&gt; import polars as pl\n&gt;&gt;&gt; import janitor.polars\n&gt;&gt;&gt; df = pl.DataFrame({\n...     \"a\": [\"nums\", '6', '9'],\n...     \"b\": [\"chars\", \"x\", \"y\"],\n... })\n&gt;&gt;&gt; df\nshape: (3, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a    \u2506 b     \u2502\n\u2502 ---  \u2506 ---   \u2502\n\u2502 str  \u2506 str   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 nums \u2506 chars \u2502\n\u2502 6    \u2506 x     \u2502\n\u2502 9    \u2506 y     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n&gt;&gt;&gt; df.row_to_names(0, remove_rows=True)\nshape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 nums \u2506 chars \u2502\n\u2502 ---  \u2506 ---   \u2502\n\u2502 str  \u2506 str   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 6    \u2506 x     \u2502\n\u2502 9    \u2506 y     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n&gt;&gt;&gt; df.row_to_names(row_numbers=[0,1], remove_rows=True)\nshape: (1, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 nums_6 \u2506 chars_x \u2502\n\u2502 ---    \u2506 ---     \u2502\n\u2502 str    \u2506 str     \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 9      \u2506 y       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Remove rows above the elevated row and the elevated row itself.</p> <pre><code>&gt;&gt;&gt; df = pl.DataFrame({\n...     \"a\": [\"bla1\", \"nums\", '6', '9'],\n...     \"b\": [\"bla2\", \"chars\", \"x\", \"y\"],\n... })\n&gt;&gt;&gt; df\nshape: (4, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 a    \u2506 b     \u2502\n\u2502 ---  \u2506 ---   \u2502\n\u2502 str  \u2506 str   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 bla1 \u2506 bla2  \u2502\n\u2502 nums \u2506 chars \u2502\n\u2502 6    \u2506 x     \u2502\n\u2502 9    \u2506 y     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n&gt;&gt;&gt; df.row_to_names(1, remove_rows=True, remove_rows_above=True)\nshape: (2, 2)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 nums \u2506 chars \u2502\n\u2502 ---  \u2506 ---   \u2502\n\u2502 str  \u2506 str   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 6    \u2506 x     \u2502\n\u2502 9    \u2506 y     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>New in version 0.28.0</p> <p>Parameters:</p> Name Type Description Default <code>row_numbers</code> <code>int | list | slice</code> <p>Position of the row(s) containing the variable names. It can be an integer, list or a slice.</p> <code>0</code> <code>remove_rows</code> <code>bool</code> <p>Whether the row(s) should be removed from the DataFrame.</p> <code>False</code> <code>remove_rows_above</code> <code>bool</code> <p>Whether the row(s) above the selected row should be removed from the DataFrame.</p> <code>False</code> <code>separator</code> <code>str</code> <p>Combines the labels into a single string, if row_numbers is a list of integers. Default is '_'.</p> <code>'_'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A polars DataFrame.</p> Source code in <code>janitor/polars/row_to_names.py</code> <pre><code>@register_dataframe_method\ndef row_to_names(\n    df: pl.DataFrame,\n    row_numbers: int | list | slice = 0,\n    remove_rows: bool = False,\n    remove_rows_above: bool = False,\n    separator: str = \"_\",\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Elevates a row, or rows, to be the column names of a DataFrame.\n\n    Examples:\n        Replace column names with the first row.\n\n        &gt;&gt;&gt; import polars as pl\n        &gt;&gt;&gt; import janitor.polars\n        &gt;&gt;&gt; df = pl.DataFrame({\n        ...     \"a\": [\"nums\", '6', '9'],\n        ...     \"b\": [\"chars\", \"x\", \"y\"],\n        ... })\n        &gt;&gt;&gt; df\n        shape: (3, 2)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 a    \u2506 b     \u2502\n        \u2502 ---  \u2506 ---   \u2502\n        \u2502 str  \u2506 str   \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 nums \u2506 chars \u2502\n        \u2502 6    \u2506 x     \u2502\n        \u2502 9    \u2506 y     \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        &gt;&gt;&gt; df.row_to_names(0, remove_rows=True)\n        shape: (2, 2)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 nums \u2506 chars \u2502\n        \u2502 ---  \u2506 ---   \u2502\n        \u2502 str  \u2506 str   \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 6    \u2506 x     \u2502\n        \u2502 9    \u2506 y     \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        &gt;&gt;&gt; df.row_to_names(row_numbers=[0,1], remove_rows=True)\n        shape: (1, 2)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 nums_6 \u2506 chars_x \u2502\n        \u2502 ---    \u2506 ---     \u2502\n        \u2502 str    \u2506 str     \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 9      \u2506 y       \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n        Remove rows above the elevated row and the elevated row itself.\n\n        &gt;&gt;&gt; df = pl.DataFrame({\n        ...     \"a\": [\"bla1\", \"nums\", '6', '9'],\n        ...     \"b\": [\"bla2\", \"chars\", \"x\", \"y\"],\n        ... })\n        &gt;&gt;&gt; df\n        shape: (4, 2)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 a    \u2506 b     \u2502\n        \u2502 ---  \u2506 ---   \u2502\n        \u2502 str  \u2506 str   \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 bla1 \u2506 bla2  \u2502\n        \u2502 nums \u2506 chars \u2502\n        \u2502 6    \u2506 x     \u2502\n        \u2502 9    \u2506 y     \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        &gt;&gt;&gt; df.row_to_names(1, remove_rows=True, remove_rows_above=True)\n        shape: (2, 2)\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502 nums \u2506 chars \u2502\n        \u2502 ---  \u2506 ---   \u2502\n        \u2502 str  \u2506 str   \u2502\n        \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n        \u2502 6    \u2506 x     \u2502\n        \u2502 9    \u2506 y     \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n    !!! info \"New in version 0.28.0\"\n\n    Args:\n        row_numbers: Position of the row(s) containing the variable names.\n            It can be an integer, list or a slice.\n        remove_rows: Whether the row(s) should be removed from the DataFrame.\n        remove_rows_above: Whether the row(s) above the selected row should\n            be removed from the DataFrame.\n        separator: Combines the labels into a single string,\n            if row_numbers is a list of integers. Default is '_'.\n\n    Returns:\n        A polars DataFrame.\n    \"\"\"  # noqa: E501\n    return _row_to_names(\n        row_numbers,\n        df=df,\n        remove_rows=remove_rows,\n        remove_rows_above=remove_rows_above,\n        separator=separator,\n    )\n</code></pre>"},{"location":"api/timeseries/","title":"Timeseries","text":"<p>Time series-specific data cleaning functions.</p>"},{"location":"api/timeseries/#janitor.timeseries.fill_missing_timestamps","title":"<code>fill_missing_timestamps(df, frequency, first_time_stamp=None, last_time_stamp=None)</code>","text":"<p>Fills a DataFrame with missing timestamps based on a defined frequency.</p> <p>If timestamps are missing, this function will re-index the DataFrame. If timestamps are not missing, then the function will return the DataFrame unmodified.</p> <p>Examples:</p> <p>Functional usage</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor.timeseries\n&gt;&gt;&gt; df = janitor.timeseries.fill_missing_timestamps(\n...     df=pd.DataFrame(...),\n...     frequency=\"1H\",\n... )\n</code></pre> <p>Method chaining example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor.timeseries\n&gt;&gt;&gt; df = (\n...     pd.DataFrame(...)\n...     .fill_missing_timestamps(frequency=\"1H\")\n... )\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame which needs to be tested for missing timestamps</p> required <code>frequency</code> <code>str</code> <p>Sampling frequency of the data. Acceptable frequency strings are available here. Check offset aliases under time series in user guide</p> required <code>first_time_stamp</code> <code>Timestamp</code> <p>Timestamp expected to start from; defaults to <code>None</code>. If no input is provided, assumes the minimum value in <code>time_series</code>.</p> <code>None</code> <code>last_time_stamp</code> <code>Timestamp</code> <p>Timestamp expected to end with; defaults to <code>None</code>. If no input is provided, assumes the maximum value in <code>time_series</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame that has a complete set of contiguous datetimes.</p> Source code in <code>janitor/timeseries.py</code> <pre><code>@pf.register_dataframe_method\ndef fill_missing_timestamps(\n    df: pd.DataFrame,\n    frequency: str,\n    first_time_stamp: pd.Timestamp = None,\n    last_time_stamp: pd.Timestamp = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Fills a DataFrame with missing timestamps based on a defined frequency.\n\n    If timestamps are missing, this function will re-index the DataFrame.\n    If timestamps are not missing, then the function will return the DataFrame\n    unmodified.\n\n    Examples:\n        Functional usage\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor.timeseries\n        &gt;&gt;&gt; df = janitor.timeseries.fill_missing_timestamps(\n        ...     df=pd.DataFrame(...),\n        ...     frequency=\"1H\",\n        ... )  # doctest: +SKIP\n\n        Method chaining example:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor.timeseries\n        &gt;&gt;&gt; df = (\n        ...     pd.DataFrame(...)\n        ...     .fill_missing_timestamps(frequency=\"1H\")\n        ... )  # doctest: +SKIP\n\n    Args:\n        df: DataFrame which needs to be tested for missing timestamps\n        frequency: Sampling frequency of the data.\n            Acceptable frequency strings are available\n            [here](https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases).\n            Check offset aliases under time series in user guide\n        first_time_stamp: Timestamp expected to start from;\n            defaults to `None`. If no input is provided, assumes the\n            minimum value in `time_series`.\n        last_time_stamp: Timestamp expected to end with; defaults to `None`.\n            If no input is provided, assumes the maximum value in\n            `time_series`.\n\n    Returns:\n        DataFrame that has a complete set of contiguous datetimes.\n    \"\"\"\n    # Check all the inputs are the correct data type\n    check(\"frequency\", frequency, [str])\n    check(\"first_time_stamp\", first_time_stamp, [pd.Timestamp, type(None)])\n    check(\"last_time_stamp\", last_time_stamp, [pd.Timestamp, type(None)])\n\n    if first_time_stamp is None:\n        first_time_stamp = df.index.min()\n    if last_time_stamp is None:\n        last_time_stamp = df.index.max()\n\n    # Generate expected timestamps\n    expected_timestamps = pd.date_range(\n        start=first_time_stamp, end=last_time_stamp, freq=frequency\n    )\n\n    return df.reindex(expected_timestamps)\n</code></pre>"},{"location":"api/timeseries/#janitor.timeseries.flag_jumps","title":"<code>flag_jumps(df, scale='percentage', direction='any', threshold=0.0, strict=False)</code>","text":"<p>Create boolean column(s) that flag whether or not the change between consecutive rows exceeds a provided threshold.</p> <p>Examples:</p> <pre><code>Applies specified criteria across all columns of the DataFrame\nand appends a flag column for each column in the DataFrame\n\n&gt;&gt;&gt; df = (\n...     pd.DataFrame(...)\n...     .flag_jumps(\n...         scale=\"absolute\",\n...         direction=\"any\",\n...         threshold=2\n...     )\n... )  # doctest: +SKIP\n\nApplies specific criteria to certain DataFrame columns,\napplies default criteria to columns *not* specifically listed and\nappends a flag column for each column in the DataFrame\n\n&gt;&gt;&gt; df = (\n...     pd.DataFrame(...)\n...     .flag_jumps(\n...         scale=dict(col1=\"absolute\", col2=\"percentage\"),\n...         direction=dict(col1=\"increasing\", col2=\"any\"),\n...         threshold=dict(col1=1, col2=0.5),\n...     )\n... )  # doctest: +SKIP\n\nApplies specific criteria to certain DataFrame columns,\napplies default criteria to columns *not* specifically listed and\nappends a flag column for only those columns found in specified\ncriteria\n\n&gt;&gt;&gt; df = (\n...     pd.DataFrame(...)\n...     .flag_jumps(\n...         scale=dict(col1=\"absolute\"),\n...         threshold=dict(col2=1),\n...         strict=True,\n...     )\n... )  # doctest: +SKIP\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame which needs to be flagged for changes between consecutive rows above a certain threshold.</p> required <code>scale</code> <code>Union[str, Dict[str, str]]</code> <p>Type of scaling approach to use. Acceptable arguments are</p> <ul> <li><code>'absolute'</code> (consider the difference between rows)</li> <li><code>'percentage'</code> (consider the percentage change between rows).</li> </ul> <code>'percentage'</code> <code>direction</code> <code>Union[str, Dict[str, str]]</code> <p>Type of method used to handle the sign change when comparing consecutive rows. Acceptable arguments are</p> <ul> <li><code>'increasing'</code> (only consider rows that are increasing in value)</li> <li><code>'decreasing'</code> (only consider rows that are decreasing in value)</li> <li><code>'any'</code> (consider rows that are either increasing or decreasing;     sign is ignored).</li> </ul> <code>'any'</code> <code>threshold</code> <code>Union[int, float, Dict[str, Union[int, float]]]</code> <p>The value to check if consecutive row comparisons exceed. Always uses a greater than comparison. Must be <code>&gt;= 0.0</code>.</p> <code>0.0</code> <code>strict</code> <code>bool</code> <p>Flag to enable/disable appending of a flag column for each column in the provided DataFrame. If set to <code>True</code>, will only append a flag column for those columns found in at least one of the input dictionaries. If set to <code>False</code>, will append a flag column for each column found in the provided DataFrame. If criteria is not specified, the defaults for each criteria is used.</p> <code>False</code> <p>Raises:</p> Type Description <code>JanitorError</code> <p>If <code>strict=True</code> and at least one of <code>scale</code>, <code>direction</code>, or <code>threshold</code> inputs is not a dictionary.</p> <code>JanitorError</code> <p>If <code>scale</code> is not one of <code>(\"absolute\", \"percentage\")</code>.</p> <code>JanitorError</code> <p>If <code>direction</code> is not one of <code>(\"increasing\", \"decreasing\", \"any\")</code>.</p> <code>JanitorError</code> <p>If <code>threshold</code> is less than <code>0.0</code>.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame that has <code>flag jump</code> columns.</p> Source code in <code>janitor/timeseries.py</code> <pre><code>@pf.register_dataframe_method\ndef flag_jumps(\n    df: pd.DataFrame,\n    scale: Union[str, Dict[str, str]] = \"percentage\",\n    direction: Union[str, Dict[str, str]] = \"any\",\n    threshold: Union[int, float, Dict[str, Union[int, float]]] = 0.0,\n    strict: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Create boolean column(s) that flag whether or not the change\n    between consecutive rows exceeds a provided threshold.\n\n    Examples:\n\n        Applies specified criteria across all columns of the DataFrame\n        and appends a flag column for each column in the DataFrame\n\n        &gt;&gt;&gt; df = (\n        ...     pd.DataFrame(...)\n        ...     .flag_jumps(\n        ...         scale=\"absolute\",\n        ...         direction=\"any\",\n        ...         threshold=2\n        ...     )\n        ... )  # doctest: +SKIP\n\n        Applies specific criteria to certain DataFrame columns,\n        applies default criteria to columns *not* specifically listed and\n        appends a flag column for each column in the DataFrame\n\n        &gt;&gt;&gt; df = (\n        ...     pd.DataFrame(...)\n        ...     .flag_jumps(\n        ...         scale=dict(col1=\"absolute\", col2=\"percentage\"),\n        ...         direction=dict(col1=\"increasing\", col2=\"any\"),\n        ...         threshold=dict(col1=1, col2=0.5),\n        ...     )\n        ... )  # doctest: +SKIP\n\n        Applies specific criteria to certain DataFrame columns,\n        applies default criteria to columns *not* specifically listed and\n        appends a flag column for only those columns found in specified\n        criteria\n\n        &gt;&gt;&gt; df = (\n        ...     pd.DataFrame(...)\n        ...     .flag_jumps(\n        ...         scale=dict(col1=\"absolute\"),\n        ...         threshold=dict(col2=1),\n        ...         strict=True,\n        ...     )\n        ... )  # doctest: +SKIP\n\n    Args:\n        df: DataFrame which needs to be flagged for changes between\n            consecutive rows above a certain threshold.\n        scale:\n            Type of scaling approach to use.\n            Acceptable arguments are\n\n            * `'absolute'` (consider the difference between rows)\n            * `'percentage'` (consider the percentage change between rows).\n\n        direction: Type of method used to handle the sign change when\n            comparing consecutive rows.\n            Acceptable arguments are\n\n            * `'increasing'` (only consider rows that are increasing in value)\n            * `'decreasing'` (only consider rows that are decreasing in value)\n            * `'any'` (consider rows that are either increasing or decreasing;\n                sign is ignored).\n        threshold: The value to check if consecutive row comparisons\n            exceed. Always uses a greater than comparison. Must be `&gt;= 0.0`.\n        strict: Flag to enable/disable appending of a flag column for\n            each column in the provided DataFrame. If set to `True`, will\n            only append a flag column for those columns found in at least\n            one of the input dictionaries. If set to `False`, will append\n            a flag column for each column found in the provided DataFrame.\n            If criteria is not specified, the defaults for each criteria\n            is used.\n\n    Raises:\n        JanitorError: If `strict=True` and at least one of\n            `scale`, `direction`, or `threshold` inputs is not a\n            dictionary.\n        JanitorError: If `scale` is not one of\n            `(\"absolute\", \"percentage\")`.\n        JanitorError: If `direction` is not one of\n            `(\"increasing\", \"decreasing\", \"any\")`.\n        JanitorError: If `threshold` is less than `0.0`.\n\n    Returns:\n        DataFrame that has `flag jump` columns.\n\n    &lt;!--\n    # noqa: DAR101\n    --&gt;\n    \"\"\"\n    df = df.copy()\n\n    if strict:\n        if (\n            any(isinstance(arg, dict) for arg in (scale, direction, threshold))\n            is False\n        ):\n            raise JanitorError(\n                \"When enacting 'strict=True', 'scale', 'direction', or \"\n                + \"'threshold' must be a dictionary.\"\n            )\n\n        # Only append a flag col for the cols that appear\n        # in at least one of the input dicts\n        arg_keys = [\n            arg.keys()\n            for arg in (scale, direction, threshold)\n            if isinstance(arg, dict)\n        ]\n        cols = set(itertools.chain.from_iterable(arg_keys))\n\n    else:\n        # Append a flag col for each col in the DataFrame\n        cols = df.columns\n\n    columns_to_add = {}\n    for col in sorted(cols):\n        # Allow arguments to be a mix of dict and single instances\n        s = scale.get(col, \"percentage\") if isinstance(scale, dict) else scale\n        d = (\n            direction.get(col, \"any\")\n            if isinstance(direction, dict)\n            else direction\n        )\n        t = (\n            threshold.get(col, 0.0)\n            if isinstance(threshold, dict)\n            else threshold\n        )\n\n        columns_to_add[f\"{col}_jump_flag\"] = _flag_jumps_single_col(\n            df, col, scale=s, direction=d, threshold=t\n        )\n\n    df = df.assign(**columns_to_add)\n\n    return df\n</code></pre>"},{"location":"api/timeseries/#janitor.timeseries.sort_timestamps_monotonically","title":"<code>sort_timestamps_monotonically(df, direction='increasing', strict=False)</code>","text":"<p>Sort DataFrame such that index is monotonic.</p> <p>If timestamps are monotonic, this function will return the DataFrame unmodified. If timestamps are not monotonic, then the function will sort the DataFrame.</p> <p>Examples:</p> <p>Functional usage</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor.timeseries\n&gt;&gt;&gt; df = janitor.timeseries.sort_timestamps_monotonically(\n...     df=pd.DataFrame(...),\n...     direction=\"increasing\",\n... )\n</code></pre> <p>Method chaining example:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import janitor.timeseries\n&gt;&gt;&gt; df = (\n...     pd.DataFrame(...)\n...     .sort_timestamps_monotonically(direction=\"increasing\")\n... )\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame which needs to be tested for monotonicity.</p> required <code>direction</code> <code>str</code> <p>Type of monotonicity desired. Acceptable arguments are <code>'increasing'</code> or <code>'decreasing'</code>.</p> <code>'increasing'</code> <code>strict</code> <code>bool</code> <p>Flag to enable/disable strict monotonicity. If set to <code>True</code>, will remove duplicates in the index by retaining first occurrence of value in index. If set to <code>False</code>, will not test for duplicates in the index.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame that has monotonically increasing (or decreasing) timestamps.</p> Source code in <code>janitor/timeseries.py</code> <pre><code>@pf.register_dataframe_method\ndef sort_timestamps_monotonically(\n    df: pd.DataFrame, direction: str = \"increasing\", strict: bool = False\n) -&gt; pd.DataFrame:\n    \"\"\"Sort DataFrame such that index is monotonic.\n\n    If timestamps are monotonic, this function will return\n    the DataFrame unmodified. If timestamps are not monotonic,\n    then the function will sort the DataFrame.\n\n    Examples:\n        Functional usage\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor.timeseries\n        &gt;&gt;&gt; df = janitor.timeseries.sort_timestamps_monotonically(\n        ...     df=pd.DataFrame(...),\n        ...     direction=\"increasing\",\n        ... )  # doctest: +SKIP\n\n        Method chaining example:\n\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; import janitor.timeseries\n        &gt;&gt;&gt; df = (\n        ...     pd.DataFrame(...)\n        ...     .sort_timestamps_monotonically(direction=\"increasing\")\n        ... )  # doctest: +SKIP\n\n    Args:\n        df: DataFrame which needs to be tested for monotonicity.\n        direction: Type of monotonicity desired.\n            Acceptable arguments are `'increasing'` or `'decreasing'`.\n        strict: Flag to enable/disable strict monotonicity.\n            If set to `True`, will remove duplicates in the index\n            by retaining first occurrence of value in index.\n            If set to `False`, will not test for duplicates in the index.\n\n    Returns:\n        DataFrame that has monotonically increasing (or decreasing)\n            timestamps.\n    \"\"\"\n    # Check all the inputs are the correct data type\n    check(\"df\", df, [pd.DataFrame])\n    check(\"direction\", direction, [str])\n    check(\"strict\", strict, [bool])\n\n    # Remove duplicates if requested\n    if strict:\n        df = df[~df.index.duplicated(keep=\"first\")]\n\n    # Sort timestamps\n    if direction == \"increasing\":\n        df = df.sort_index()\n    else:\n        df = df.sort_index(ascending=False)\n\n    # Return the DataFrame\n    return df\n</code></pre>"},{"location":"api/xarray/","title":"XArray","text":"<p>Functions to augment XArray DataArrays and Datasets with additional functionality.</p>"},{"location":"api/xarray/#janitor.xarray.functions.clone_using","title":"<code>clone_using(da, np_arr, use_coords=True, use_attrs=False, new_name=None)</code>","text":"<p>Given a NumPy array, return an XArray <code>DataArray</code> which contains the same dimension names and (optionally) coordinates and other properties as the supplied <code>DataArray</code>.</p> <p>This is similar to <code>xr.DataArray.copy()</code> with more specificity for the type of cloning you would like to perform - the different properties that you desire to mirror in the new <code>DataArray</code>.</p> <p>If the coordinates from the source <code>DataArray</code> are not desired, the shape of the source and new NumPy arrays don't need to match. The number of dimensions do, however.</p> <p>Examples:</p> <p>Making a new <code>DataArray</code> from a previous one, keeping the dimension names but dropping the coordinates (the input NumPy array is of a different size):</p> <pre><code>&gt;&gt;&gt; import xarray as xr\n&gt;&gt;&gt; import janitor.xarray\n&gt;&gt;&gt; da = xr.DataArray(\n...     np.zeros((512, 1024)), dims=[\"ax_1\", \"ax_2\"],\n...     coords=dict(ax_1=np.linspace(0, 1, 512),\n...                 ax_2=np.logspace(-2, 2, 1024)),\n...     name=\"original\",\n... )\n&gt;&gt;&gt; new_da = da.clone_using(\n...     np.ones((4, 6)), new_name='new_and_improved', use_coords=False,\n... )\n&gt;&gt;&gt; new_da\n&lt;xarray.DataArray 'new_and_improved' (ax_1: 4, ax_2: 6)&gt; Size: 192B\narray([[1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1.],\n       [1., 1., 1., 1., 1., 1.]])\nDimensions without coordinates: ax_1, ax_2\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>da</code> <code>DataArray</code> <p>The <code>DataArray</code> supplied by the method itself.</p> required <code>np_arr</code> <code>array</code> <p>The NumPy array which will be wrapped in a new <code>DataArray</code> given the properties copied over from the source <code>DataArray</code>.</p> required <code>use_coords</code> <code>bool</code> <p>If <code>True</code>, use the coordinates of the source <code>DataArray</code> for the coordinates of the newly-generated array. Shapes must match in this case. If <code>False</code>, only the number of dimensions must match.</p> <code>True</code> <code>use_attrs</code> <code>bool</code> <p>If <code>True</code>, copy over the <code>attrs</code> from the source <code>DataArray</code>. The data inside <code>attrs</code> itself is not copied, only the mapping. Otherwise, use the supplied attrs.</p> <code>False</code> <code>new_name</code> <code>str</code> <p>If set, use as the new name of the returned <code>DataArray</code>. Otherwise, use the name of <code>da</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If number of dimensions in <code>NumPy</code> array and <code>DataArray</code> do not match.</p> <code>ValueError</code> <p>If shape of <code>NumPy</code> array and <code>DataArray</code> do not match.</p> <p>Returns:</p> Type Description <code>DataArray</code> <p>A <code>DataArray</code> styled like the input <code>DataArray</code> containing the NumPy array data.</p> Source code in <code>janitor/xarray/functions.py</code> <pre><code>@pf.register_xarray_dataarray_method\ndef clone_using(\n    da: xr.DataArray,\n    np_arr: np.array,\n    use_coords: bool = True,\n    use_attrs: bool = False,\n    new_name: str = None,\n) -&gt; xr.DataArray:\n    \"\"\"\n    Given a NumPy array, return an XArray `DataArray` which contains the same\n    dimension names and (optionally) coordinates and other properties as the\n    supplied `DataArray`.\n\n    This is similar to `xr.DataArray.copy()` with more specificity for\n    the type of cloning you would like to perform - the different properties\n    that you desire to mirror in the new `DataArray`.\n\n    If the coordinates from the source `DataArray` are not desired, the shape\n    of the source and new NumPy arrays don't need to match.\n    The number of dimensions do, however.\n\n    Examples:\n        Making a new `DataArray` from a previous one, keeping the\n        dimension names but dropping the coordinates (the input NumPy array\n        is of a different size):\n\n        &gt;&gt;&gt; import xarray as xr\n        &gt;&gt;&gt; import janitor.xarray\n        &gt;&gt;&gt; da = xr.DataArray(\n        ...     np.zeros((512, 1024)), dims=[\"ax_1\", \"ax_2\"],\n        ...     coords=dict(ax_1=np.linspace(0, 1, 512),\n        ...                 ax_2=np.logspace(-2, 2, 1024)),\n        ...     name=\"original\",\n        ... )\n        &gt;&gt;&gt; new_da = da.clone_using(\n        ...     np.ones((4, 6)), new_name='new_and_improved', use_coords=False,\n        ... )\n        &gt;&gt;&gt; new_da\n        &lt;xarray.DataArray 'new_and_improved' (ax_1: 4, ax_2: 6)&gt; Size: 192B\n        array([[1., 1., 1., 1., 1., 1.],\n               [1., 1., 1., 1., 1., 1.],\n               [1., 1., 1., 1., 1., 1.],\n               [1., 1., 1., 1., 1., 1.]])\n        Dimensions without coordinates: ax_1, ax_2\n\n    Args:\n        da: The `DataArray` supplied by the method itself.\n        np_arr: The NumPy array which will be wrapped in a new `DataArray`\n            given the properties copied over from the source `DataArray`.\n        use_coords: If `True`, use the coordinates of the source\n            `DataArray` for the coordinates of the newly-generated array.\n            Shapes must match in this case. If `False`, only the number of\n            dimensions must match.\n        use_attrs: If `True`, copy over the `attrs` from the source\n            `DataArray`.\n            The data inside `attrs` itself is not copied, only the mapping.\n            Otherwise, use the supplied attrs.\n        new_name: If set, use as the new name of the returned `DataArray`.\n            Otherwise, use the name of `da`.\n\n    Raises:\n        ValueError: If number of dimensions in `NumPy` array and\n            `DataArray` do not match.\n        ValueError: If shape of `NumPy` array and `DataArray`\n            do not match.\n\n    Returns:\n        A `DataArray` styled like the input `DataArray` containing the\n            NumPy array data.\n    \"\"\"\n\n    if np_arr.ndim != da.ndim:\n        raise ValueError(\n            \"Number of dims in the NumPy array and the DataArray \"\n            \"must match.\"\n        )\n\n    if use_coords and not all(\n        np_ax_len == da_ax_len\n        for np_ax_len, da_ax_len in zip(np_arr.shape, da.shape)\n    ):\n        raise ValueError(\n            \"Input NumPy array and DataArray must have the same \"\n            \"shape if copying over coordinates.\"\n        )\n\n    return xr.DataArray(\n        np_arr,\n        dims=da.dims,\n        coords=da.coords if use_coords else None,\n        attrs=da.attrs.copy() if use_attrs else None,\n        name=new_name if new_name is not None else da.name,\n    )\n</code></pre>"},{"location":"api/xarray/#janitor.xarray.functions.convert_datetime_to_number","title":"<code>convert_datetime_to_number(da_or_ds, time_units, dim='time')</code>","text":"<p>Convert the coordinates of a datetime axis to a human-readable float representation.</p> <p>Examples:</p> <p>Convert a <code>DataArray</code>'s time dimension coordinates from minutes to seconds:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import xarray as xr\n&gt;&gt;&gt; import janitor.xarray\n&gt;&gt;&gt; timepoints = 5\n&gt;&gt;&gt; da = xr.DataArray(\n...     np.array([2, 8, 0, 1, 7, 7]),\n...     dims=\"time\",\n...     coords=dict(time=np.arange(6) * np.timedelta64(1, \"m\"))\n... )\n&gt;&gt;&gt; da_minutes = da.convert_datetime_to_number(\"s\", dim=\"time\")\n&gt;&gt;&gt; da_minutes\n&lt;xarray.DataArray (time: 6)&gt; Size: 48B\narray([2, 8, 0, 1, 7, 7])\nCoordinates:\n  * time     (time) float64 48B 0.0 60.0 120.0 180.0 240.0 300.0\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>da_or_ds</code> <code>Union[DataArray, Dataset]</code> <p>XArray object.</p> required <code>time_units</code> <code>str</code> <p>Numpy timedelta string specification for the unit you would like to convert the coordinates to.</p> required <code>dim</code> <code>str</code> <p>The time dimension whose coordinates are datetime objects.</p> <code>'time'</code> <p>Returns:</p> Type Description <code>Union[DataArray, Dataset]</code> <p>The original XArray object with the time dimension reassigned.</p> Source code in <code>janitor/xarray/functions.py</code> <pre><code>@pf.register_xarray_dataset_method\n@pf.register_xarray_dataarray_method\ndef convert_datetime_to_number(\n    da_or_ds: Union[xr.DataArray, xr.Dataset],\n    time_units: str,\n    dim: str = \"time\",\n) -&gt; Union[xr.DataArray, xr.Dataset]:\n    \"\"\"Convert the coordinates of a datetime axis to a human-readable float\n    representation.\n\n    Examples:\n        Convert a `DataArray`'s time dimension coordinates from\n        minutes to seconds:\n\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; import xarray as xr\n        &gt;&gt;&gt; import janitor.xarray\n        &gt;&gt;&gt; timepoints = 5\n        &gt;&gt;&gt; da = xr.DataArray(\n        ...     np.array([2, 8, 0, 1, 7, 7]),\n        ...     dims=\"time\",\n        ...     coords=dict(time=np.arange(6) * np.timedelta64(1, \"m\"))\n        ... )\n        &gt;&gt;&gt; da_minutes = da.convert_datetime_to_number(\"s\", dim=\"time\")\n        &gt;&gt;&gt; da_minutes\n        &lt;xarray.DataArray (time: 6)&gt; Size: 48B\n        array([2, 8, 0, 1, 7, 7])\n        Coordinates:\n          * time     (time) float64 48B 0.0 60.0 120.0 180.0 240.0 300.0\n\n    Args:\n        da_or_ds: XArray object.\n        time_units: Numpy timedelta string specification for the unit you\n            would like to convert the coordinates to.\n        dim: The time dimension whose coordinates are datetime objects.\n\n    Returns:\n        The original XArray object with the time dimension reassigned.\n    \"\"\"\n\n    times = da_or_ds.coords[dim].data / np.timedelta64(1, time_units)\n\n    return da_or_ds.assign_coords({dim: times})\n</code></pre>"},{"location":"development/lazy_imports/","title":"Lazy Imports","text":"<p>In <code>pyjanitor</code>, we use lazy imports to speed up <code>import janitor</code>. Prior to using lazy imports, <code>import janitor</code> would take about 1-2 seconds to complete, thereby causing significant delays for downstream consumers of <code>pyjanitor</code>. Slow importing be undesirable as it would slow down programs that demand low latency.</p>"},{"location":"development/lazy_imports/#a-brief-history-of-the-decision","title":"A brief history of the decision","text":"<p>The original issue was raised by @ericmjl in issue (#1059). The basis there is that the scientific Python community was hurting with imports that took a long time, especially the ones that depended on SciPy and Pandas. As <code>pyjanitor</code> is a package that depends on <code>pandas</code>, it was important for us to see if we could improve the speed at which imports happened.</p>"},{"location":"development/lazy_imports/#current-speed-benchmark","title":"Current Speed Benchmark","text":"<p>As of 5 April 2022, imports take about ~0.5 seconds (give or take) to complete on a GitHub Codespaces workspace. This is much more desirable than the original 1-2 seconds, also measured on a GitHub Codespaces workspace.</p>"},{"location":"development/lazy_imports/#how-to-benchmark","title":"How to benchmark","text":"<p>To benchmark, we run the following line:</p> <pre><code>python -X importtime -c \"import janitor\" 2&gt; timing.log\n</code></pre> <p>Then, using the <code>tuna</code> CLI tool, we can view the timing log:</p> <pre><code>tuna timing.log\n</code></pre> <p>Note: You may need to install tuna using <code>pip install -U tuna</code>. <code>tuna</code>'s development repository is on GitHub</p> <p>You'll be redirected to your browser, where the web UI will allow you to see which imports are causing time delays.</p> <p></p>"},{"location":"development/lazy_imports/#which-imports-to-lazily-load","title":"Which imports to lazily load","text":"<p>Generally speaking, the external imports are the ones that when lazily loaded, will give the maximal gain in speed. You can also opt to lazily load <code>pyjanitor</code> submodules, but we doubt they will give much advantage in speed.</p>"}]}